{"meta":{"title":"信平的小屋","subtitle":"念念不忘,必有回响,有一口气,点一盏灯。","description":null,"author":"王信平","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-04-06T06:34:22.591Z","updated":"2017-04-06T06:34:22.591Z","comments":true,"path":"uploads/prettify/atelier-dune-light.css","permalink":"http://yoursite.com/uploads/prettify/atelier-dune-light.css","excerpt":"","text":"/*! Color themes for Google Code Prettify | MIT License | github.com/jmblog/color-themes-for-google-code-prettify */ .prettyprint { background: #fefbec; font-family: Menlo, \"Bitstream Vera Sans Mono\", \"DejaVu Sans Mono\", Monaco, Consolas, monospace; border: 0 !important; } .pln { color: #20201d; } /* Specify class=linenums on a pre to get line numbering */ ol.linenums { margin-top: 0; margin-bottom: 0; color: #999580; } li.L0, li.L1, li.L2, li.L3, li.L4, li.L5, li.L6, li.L7, li.L8, li.L9 { padding-left: 1em; background-color: #fefbec; list-style-type: decimal; } @media screen { /* string content */ .str { color: #60ac39; } /* keyword */ .kwd { color: #b854d4; } /* comment */ .com { color: #999580; } /* type name */ .typ { color: #6684e1; } /* literal value */ .lit { color: #b65611; } /* punctuation */ .pun { color: #20201d; } /* lisp open bracket */ .opn { color: #20201d; } /* lisp close bracket */ .clo { color: #20201d; } /* markup tag name */ .tag { color: #d73737; } /* markup attribute name */ .atn { color: #b65611; } /* markup attribute value */ .atv { color: #1fad83; } /* declaration */ .dec { color: #b65611; } /* variable name */ .var { color: #d73737; } /* function name */ .fun { color: #6684e1; } }"},{"title":"","date":"2017-04-06T06:35:22.301Z","updated":"2017-01-13T09:58:19.000Z","comments":true,"path":"uploads/prettify/prettify.js","permalink":"http://yoursite.com/uploads/prettify/prettify.js","excerpt":"","text":"!function(){/* Copyright (C) 2006 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ window.PR_SHOULD_USE_CONTINUATION=!0; (function(){function T(a){function d(e){var b=e.charCodeAt(0);if(92!==b)return b;var a=e.charAt(1);return(b=w[a])?b:\"0\"=a?parseInt(e.substring(1),8):\"u\"===a||\"x\"===a?parseInt(e.substring(2),16):e.charCodeAt(1)}function f(e){if(32>e)return(16>e?\"\\\\x0\":\"\\\\x\")+e.toString(16);e=String.fromCharCode(e);return\"\\\\\"===e||\"-\"===e||\"]\"===e||\"^\"===e?\"\\\\\"+e:e}function b(e){var b=e.substring(1,e.length-1).match(/\\\\u[0-9A-Fa-f]{4}|\\\\x[0-9A-Fa-f]{2}|\\\\[0-3][0-7]{0,2}|\\\\[0-7]{1,2}|\\\\[\\s\\S]|-|[^-\\\\]/g);e= [];var a=\"^\"===b[0],c=[\"[\"];a&&c.push(\"^\");for(var a=a?1:0,g=b.length;ak||122k||90k||122h[0]&&(h[1]+1>h[0]&&c.push(\"-\"),c.push(f(h[1])));c.push(\"]\");return c.join(\"\")}function v(e){for(var a=e.source.match(/(?:\\[(?:[^\\x5C\\x5D]|\\\\[\\s\\S])*\\]|\\\\u[A-Fa-f0-9]{4}|\\\\x[A-Fa-f0-9]{2}|\\\\[0-9]+|\\\\[^ux0-9]|\\(\\?[:!=]|[\\(\\)\\^]|[^\\x5B\\x5C\\(\\)\\^]+)/g),c=a.length,d=[],g=0,h=0;g"},{"title":"","date":"2017-04-06T06:39:34.407Z","updated":"2017-04-06T06:39:34.407Z","comments":true,"path":"uploads/prettify/prettify.css","permalink":"http://yoursite.com/uploads/prettify/prettify.css","excerpt":"","text":".pln{color:#000}@media screen{.str{color:#080}.kwd{color:#008}.com{color:#800}.typ{color:#606}.lit{color:#066}.pun,.opn,.clo{color:#660}.tag{color:#008}.atn{color:#606}.atv{color:#080}.dec,.var{color:#606}.fun{color:red}}@media print,projection{.str{color:#060}.kwd{color:#006;font-weight:bold}.com{color:#600;font-style:italic}.typ{color:#404;font-weight:bold}.lit{color:#044}.pun,.opn,.clo{color:#440}.tag{color:#006;font-weight:bold}.atn{color:#404}.atv{color:#060}}pre.prettyprint{padding:2px;border:1px solid #888}ol.linenums{margin-top:0;margin-bottom:0}li.L0,li.L1,li.L2,li.L3,li.L5,li.L6,li.L7,li.L8{list-style-type:none}li.L1,li.L3,li.L5,li.L7,li.L9{background:#eee pre{ word-break: break-all; word-wrap: break-word; }"}],"posts":[{"title":"Pandas学习笔记","slug":"Pandas学习笔记","date":"2017-04-06T10:38:20.000Z","updated":"2017-04-06T10:38:37.460Z","comments":true,"path":"2017/04/06/Pandas学习笔记/","link":"","permalink":"http://yoursite.com/2017/04/06/Pandas学习笔记/","excerpt":"","text":"1《pandas函数手册·zw汉化标注版》（pandas v0.17）zw量化开源·系列课件zw开源量化团队QQ群：533233771。作者：zw=智王+字王2016.01.18 字王 Git 项目总览：github.com/ziwang-com/,包括：字王 4k 云字库，zwPython、zwpy_lst Zw 量化 QQ 群：124134140 （AI 量化，足彩大数据、云字库、zwPython） zw 开源量化团队 QQ 群：533233771。 技术 Blog：blog.sina.com.cn/zbrow （AI 量化、足彩大数据、字库） www.cnblogs.com/ziwang/ （机器视觉） 网盘下载：http://pan.baidu.com/s/1bnSqTxd zw 网站：http://www.ziwang.comgithub，全球最大的极客创意平台，欢迎大家参与2前言………………………………………………………………………………………………………………………………………………………………………6ZW 量化开源团队简介…………………………………………………………………………………………………………………………………………. 7Categorical………………………………………………………………………………………………………………………………………………………….. 9CategoricalIndex……………………………………………………………………………………………………………………………………………….. 24DataFrame………………………………………………………………………………………………………………………………………………………… 46DateOffset……………………………………………………………………………………………………………………………………………………….. 145DatetimeIndex…………………………………………………………………………………………………………………………………………………. 148ExcelFile………………………………………………………………………………………………………………………………………………………….. 170ExcelWriter……………………………………………………………………………………………………………………………………………………….171Expr…………………………………………………………………………………………………………………………………………………………………. 173Float64Index……………………………………………………………………………………………………………………………………………………. 174Grouper…………………………………………………………………………………………………………………………………………………………….192HDFStore………………………………………………………………………………………………………………………………………………………… 194Index…………………………………………………………………………………………………………………………………………………………………200IndexSlice…………………………………………………………………………………………………………………………………………………………218Int64Index…………………………………………………………………………………………………………………………………………………………219LooseVersion……………………………………………………………………………………………………………………………………………………237MultiIndex…………………………………………………………………………………………………………………………………………………………239NaT…………………………………………………………………………………………………………………………………………………………………..262Panel……………………………………………………………………………………………………………………………………………………………….. 266Panel4D……………………………………………………………………………………………………………………………………………………………319Period……………………………………………………………………………………………………………………………………………………………….370PeriodIndex………………………………………………………………………………………………………………………………………………………375Series……………………………………………………………………………………………………………………………………………………………….397SparseArray……………………………………………………………………………………………………………………………………………………..470SparseDataFrame…………………………………………………………………………………………………………………………………………… 505SparseList……………………………………………………………………………………………………………………………………………………….. 603SparsePanel……………………………………………………………………………………………………………………………………………………. 605SparseSeries…………………………………………………………………………………………………………………………………………………… 655SparseTimeSeries…………………………………………………………………………………………………………………………………………… 728Term………………………………………………………………………………………………………………………………………………………………… 803TimeGrouper…………………………………………………………………………………………………………………………………………………….805TimeSeries………………………………………………………………………………………………………………………………………………………. 806Timedelta………………………………………………………………………………………………………………………………………………………….882TimedeltaIndex…………………………………………………………………………………………………………………………………………………886Timestamp………………………………………………………………………………………………………………………………………………………..904WidePanel………………………………………………………………………………………………………………………………………………………..910builtins………………………………………………………………………………………………………………………………………………………966cached…………………………………………………………………………………………………………………………………………………….. 968doc……………………………………………………………………………………………………………………………………………………………968docformat………………………………………………………………………………………………………………………………………………… 968file……………………………………………………………………………………………………………………………………………………………. 969loader………………………………………………………………………………………………………………………………………………………. 969name……………………………………………………………………………………………………………………………………………………….. 970package…………………………………………………………………………………………………………………………………………………… 972path…………………………………………………………………………………………………………………………………………………………..973spec………………………………………………………………………………………………………………………………………………………….976version…………………………………………………………………………………………………………………………………………………….. 977warningregistry………………………………………………………………………………………………………………………………………… 977_np_version…………………………………………………………………………………………………………………………………………………….. 979_np_version_under1p8……………………………………………………………………………………………………………………………………. 979_np_version_under1p9……………………………………………………………………………………………………………………………………. 984_period……………………………………………………………………………………………………………………………………………………………..988_sparse……………………………………………………………………………………………………………………………………………………………. 994_testing………………………………………………………………………………………………………………………………………………………….. 1000_version………………………………………………………………………………………………………………………………………………………….1001algos……………………………………………………………………………………………………………………………………………………………… 10013bdate_range………………………………………………………………………………………………………………………………………………….. 1015compat……………………………………………………………………………………………………………………………………………………………1015computation…………………………………………………………………………………………………………………………………………………… 1020concat……………………………………………………………………………………………………………………………………………………………. 1021core……………………………………………………………………………………………………………………………………………………………….. 1022crosstab………………………………………………………………………………………………………………………………………………………….1023cut…………………………………………………………………………………………………………………………………………………………………..1024date_range……………………………………………………………………………………………………………………………………………………..1025datetime………………………………………………………………………………………………………………………………………………………….1026datetools…………………………………………………………………………………………………………………………………………………………1029describe_option………………………………………………………………………………………………………………………………………………1031eval…………………………………………………………………………………………………………………………………………………………………1031ewma…………………………………………………………………………………………………………………………………………………………….. 1033ewmcorr………………………………………………………………………………………………………………………………………………………….1034ewmcov…………………………………………………………………………………………………………………………………………………………. 1035ewmstd………………………………………………………………………………………………………………………………………………………….. 1037ewmvar………………………………………………………………………………………………………………………………………………………….. 1038ewmvol………………………………………………………………………………………………………………………………………………………….. 1040expanding_apply…………………………………………………………………………………………………………………………………………….1041expanding_corr……………………………………………………………………………………………………………………………………………… 1042expanding_corr_pairwise………………………………………………………………………………………………………………………………. 1042expanding_count…………………………………………………………………………………………………………………………………………… 1043expanding_cov………………………………………………………………………………………………………………………………………………. 1043expanding_kurt……………………………………………………………………………………………………………………………………………….1044expanding_max………………………………………………………………………………………………………………………………………………1045expanding_mean…………………………………………………………………………………………………………………………………………… 1045expanding_median………………………………………………………………………………………………………………………………………… 1046expanding_min……………………………………………………………………………………………………………………………………………….1046expanding_quantile……………………………………………………………………………………………………………………………………….. 1047expanding_skew……………………………………………………………………………………………………………………………………………. 1047expanding_std………………………………………………………………………………………………………………………………………………..1048expanding_sum………………………………………………………………………………………………………………………………………………1048expanding_var………………………………………………………………………………………………………………………………………………..1049factorize………………………………………………………………………………………………………………………………………………………….1049fama_macbeth………………………………………………………………………………………………………………………………………………. 1050get_dummies………………………………………………………………………………………………………………………………………………….1050get_option……………………………………………………………………………………………………………………………………………………… 1052get_store……………………………………………………………………………………………………………………………………………………….. 1052groupby…………………………………………………………………………………………………………………………………………………………..1052hashtable………………………………………………………………………………………………………………………………………………………..1054index……………………………………………………………………………………………………………………………………………………………… 1059infer_freq……………………………………………………………………………………………………………………………………………………….. 1066info………………………………………………………………………………………………………………………………………………………………….1067io……………………………………………………………………………………………………………………………………………………………………. 1067isnull……………………………………………………………………………………………………………………………………………………………….1068json…………………………………………………………………………………………………………………………………………………………………1068lib…………………………………………………………………………………………………………………………………………………………………… 1069lreshape………………………………………………………………………………………………………………………………………………………….1081match…………………………………………………………………………………………………………………………………………………………….. 1082melt…………………………………………………………………………………………………………………………………………………………………1083merge……………………………………………………………………………………………………………………………………………………………..1084msgpack…………………………………………………………………………………………………………………………………………………………1086notnull……………………………………………………………………………………………………………………………………………………………. 1090offsets……………………………………………………………………………………………………………………………………………………………. 1090ols………………………………………………………………………………………………………………………………………………………………….. 1160option_context……………………………………………………………………………………………………………………………………………….. 1162options…………………………………………………………………………………………………………………………………………………………… 11624ordered_merge……………………………………………………………………………………………………………………………………………….1162pandas…………………………………………………………………………………………………………………………………………………………… 1164parser…………………………………………………………………………………………………………………………………………………………….. 1165period_range…………………………………………………………………………………………………………………………………………………..1170pivot………………………………………………………………………………………………………………………………………………………………..1170pivot_table………………………………………………………………………………………………………………………………………………………1171plot_params…………………………………………………………………………………………………………………………………………………… 1172pnow……………………………………………………………………………………………………………………………………………………………….1174qcut…………………………………………………………………………………………………………………………………………………………………1175read_clipboard………………………………………………………………………………………………………………………………………………..1176read_csv………………………………………………………………………………………………………………………………………………………… 1176read_excel………………………………………………………………………………………………………………………………………………………1179read_fwf…………………………………………………………………………………………………………………………………………………………. 1181read_gbq……………………………………………………………………………………………………………………………………………………….. 1184read_hdf………………………………………………………………………………………………………………………………………………………….1185read_html………………………………………………………………………………………………………………………………………………………. 1185read_json………………………………………………………………………………………………………………………………………………………..1188read_msgpack………………………………………………………………………………………………………………………………………………..1189read_pickle……………………………………………………………………………………………………………………………………………………..1190read_sas…………………………………………………………………………………………………………………………………………………………1190read_sql…………………………………………………………………………………………………………………………………………………………. 1191read_sql_query……………………………………………………………………………………………………………………………………………….1192read_sql_table………………………………………………………………………………………………………………………………………………..1193read_stata……………………………………………………………………………………………………………………………………………………… 1194read_table……………………………………………………………………………………………………………………………………………………… 1195reset_option…………………………………………………………………………………………………………………………………………………… 1199rolling_apply……………………………………………………………………………………………………………………………………………………1199rolling_corr…………………………………………………………………………………………………………………………………………………….. 1200rolling_corr_pairwise……………………………………………………………………………………………………………………………………… 1201rolling_count………………………………………………………………………………………………………………………………………………….. 1202rolling_cov………………………………………………………………………………………………………………………………………………………1202rolling_kurt…………………………………………………………………………………………………………………………………………………….. 1203rolling_max……………………………………………………………………………………………………………………………………………………..1204rolling_mean………………………………………………………………………………………………………………………………………………….. 1205rolling_median………………………………………………………………………………………………………………………………………………..1206rolling_min………………………………………………………………………………………………………………………………………………………1207rolling_quantile………………………………………………………………………………………………………………………………………………. 1207rolling_skew……………………………………………………………………………………………………………………………………………………1208rolling_std……………………………………………………………………………………………………………………………………………………….1209rolling_sum……………………………………………………………………………………………………………………………………………………..1210rolling_var……………………………………………………………………………………………………………………………………………………….1211rolling_window………………………………………………………………………………………………………………………………………………..1211scatter_matrix…………………………………………………………………………………………………………………………………………………1213set_eng_float_format…………………………………………………………………………………………………………………………………….. 1213set_option……………………………………………………………………………………………………………………………………………………….1214show_versions………………………………………………………………………………………………………………………………………………. 1214sparse……………………………………………………………………………………………………………………………………………………………. 1215stats………………………………………………………………………………………………………………………………………………………………..1215timedelta_range…………………………………………………………………………………………………………………………………………….. 1216to_datetime……………………………………………………………………………………………………………………………………………………. 1216to_msgpack…………………………………………………………………………………………………………………………………………………… 1218to_numeric…………………………………………………………………………………………………………………………………………………….. 1218to_pickle………………………………………………………………………………………………………………………………………………………… 1219to_timedelta…………………………………………………………………………………………………………………………………………………… 1219tools………………………………………………………………………………………………………………………………………………………………..1220tseries……………………………………………………………………………………………………………………………………………………………. 1220tslib…………………………………………………………………………………………………………………………………………………………………12215unique……………………………………………………………………………………………………………………………………………………………. 1221util…………………………………………………………………………………………………………………………………………………………………..1222value_counts…………………………………………………………………………………………………………………………………………………. 1222wide_to_long…………………………………………………………………………………………………………………………………………………. 12236前言自 2014 年，美国银行、美林证券的“石英”计划，以及摩根大通的“雅典娜”项目后。Python 量化，已经势不可挡，连老牌的 matlab、和专业的统计语言 R，都一一惜败。Python 量化， 除了原本深厚的科学计算底蕴： numPy， sciKit， 以及 AI 人工智能、 机器学习： scikit、 theano、 pyMC、NLTK 等强大的生态系统外。Pandas 潘达思（熊猫）数据分析模块，无疑是 Python 量化的刀锋所在。pandas 问世不久，这两年 python 量化，发展十分火爆，相关资料非常缺乏，不光是中文，连英文文档，都不多见。作为开发必备的 pandas 函数手册，更是其中的，重中之重。原本以为，不过是 174 个函数，zw 量化开源团队，每人十个函数，几天就 ok 了。通过 zw 自行开发的百度电脑全文翻译软件，一个晚上，机器翻译版本，全部到位。可一整理，洋洋洒洒，六千多页，即使配合“机译”版本辅助，也不是短期可以完成的。浏览了一下，函数手册，大部分是标准的 API 接口，把其中的 5%关键词，人工翻译下，基本上能看懂，可以勉强用于实际编程参考。毕竟，函数手册，API 接口，无法就是参数、类型和返回值这些。于是，便有了，这个《pandas 函数手册·zw 汉化标注版》更多请浏览 zw 网站: http://ziwang.com或技术 blog:http://blog.sina.com.cn/zbrow7ZW量化开源团队简介zw 开源量化团队 QQ 群号： 533233771zw 开源量化团队，英文名称暂定：zwQTT：ziwang.com Quant Tearm，zw 开源量化团队，是个免费的开源公益组织，专业从事国内外最新的金融、量化资料、软件、教程等各种相关资源的引进、翻译、宣传、托管。同时，在能力所及的范围内，进行相关的量化开源软件开发。有关团队介绍，可参见：《zw 开源量化团队·成立纪念》 http://ziwang.com/?p=214《zw 开源量化团队·约法三章》 http://ziwang.com/?p=212新人申请，请先浏览，以上文档，填写表格，再联系团队管理员。更多请浏览 zw 网站:http://ziwang.com或技术 blog:http://blog.sina.com.cn/zbrow附图：是 zw 开源量化团队，QQ 群首批成员截图纪念89CategoricalCategorical 模块所属：pandas.core.categorical:类定义：Categorical(pandas.core.base.PandasObject)| Represents a categorical variable in classic R / S-plus fashion|| Categoricals can only take on only a limited, and usually fixed, number| of possible values (categories). In contrast to statistical categorical| variables, a Categorical might have an order, but numerical operations| (additions, divisions, …) are not possible.|| All values of the Categorical are either in categories or np.nan.| Assigning values outside of categories will raise a ValueError. Order is| defined by the order of the categories, not lexical order of the values.|| 【参数】| ———-| values : list-like| The values of the categorical. If categories are given, values not in categories will| be replaced with NaN.| categories : Index-like (unique), optional| The unique categories for this categorical. If not given, the categories are assumed| to be the unique values of values.| ordered : boolean, (default False)| Whether or not this categorical is treated as a ordered categorical. If not given,| the resulting categorical will not be ordered.|| 【属性】| ———-| categories : Index| The categories of this categorical| codes : ndarray| The codes (integer positions, which point to the categories) of this categorical, read only.| ordered : boolean| Whether or not this Categorical is ordered.|| 【Raises 引发错误】| ——| ValueError| If the categories do not validate.| TypeError| If an explicit ordered=True is given but no categories and the values are| not sortable.||| 【示例】| ——–| &gt;&gt;&gt; from pandas import Categorical| &gt;&gt;&gt; Categorical([1, 2, 3, 1, 2, 3])| [1, 2, 3, 1, 2, 3]10| Categories (3, int64): [1 &lt; 2 &lt; 3]|| &gt;&gt;&gt; Categorical([‘a’, ‘b’, ‘c’, ‘a’, ‘b’, ‘c’])| [a, b, c, a, b, c]| Categories (3, object): [a &lt; b &lt; c]|| &gt;&gt;&gt; a = Categorical([‘a’,’b’,’c’,’a’,’b’,’c’], [‘c’, ‘b’, ‘a’], ordered=True)| &gt;&gt;&gt; a.min()| ‘c’|| 【方法排序】| Categorical| pandas.core.base.PandasObject| pandas.core.base.StringMixin| 【内置对象】|| 【方法定义】|| array(self, dtype=None)| The numpy array interface.|| 【返回值】| ——-| values : numpy array| A numpy array of either the specified dtype or, if dtype==None (default), the same| dtype as categorical.categories.dtype|| eq(self, other)|| ge(self, other)|| getitem(self, key)| Return an item.|| gt(self, other)|| init(self, values, categories=None, ordered=False, name=None, fastpath=False, levels=None)| Initialize self. See help(type(self)) for accurate signature.|| iter(self)| Returns an Iterator over the values of this Categorical.|| le(self, other)|| len(self)| The length of this Categorical.|| lt(self, other)|| ne(self, other)|| setitem(self, key, value)| Item assignment.||| 【Raises 引发错误】| ——11| ValueError| If (one or more) Value is not in categories or if a assigned Categorical has not the| same categories|| setstate(self, state)| Necessary for making this object picklable|| unicode(self)| Unicode representation.|| add_categories(self, new_categories, inplace=False)| Add new categories.|| new_categories will be included at the last/highest place in the categories and will be| unused directly after this call.|| 【Raises 引发错误】| ——| ValueError| If the new categories include old categories or do not validate as categories|| 【参数】| ———-| new_categories : category or list-like of category| The new categories to be included.| inplace : boolean (default: False)| Whether or not to add the categories inplace or return a copy of this categorical| with added categories.|| 【返回值】| ——-| cat : Categorical with new categories added or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| remove_categories| remove_unused_categories| set_categories|| argsort(self, ascending=True, kwargs)| Implements ndarray.argsort.|| For internal compatibility with numpy arrays.|| Only ordered Categoricals can be argsorted!|| 【返回值】| ——-| argsorted : numpy array|| as_ordered(self, inplace=False)| Sets the Categorical to be ordered|| 【参数】| ———-12| inplace : boolean (default: False)| Whether or not to set the ordered attribute inplace or return a copy of this categorical| with ordered set to True|| as_unordered(self, inplace=False)| Sets the Categorical to be unordered|| 【参数】| ———-| inplace : boolean (default: False)| Whether or not to set the ordered attribute inplace or return a copy of this categorical| with ordered set to False|| astype(self, dtype)| coerce this type to another dtype|| check_for_ordered(self, op)| assert that we are ordered|| copy(self)| Copy constructor.|| describe(self)| Describes this Categorical|| 【返回值】| ——-| description: DataFrame| A dataframe with frequency and counts by category.|| dropna(self)| Return the Categorical without null values.|| Both missing values (-1 in .codes) and NA as a category are detected.| NA is removed from the categories if present.|| 【返回值】| ——-| valid : Categorical|| equals(self, other)| Returns True if categorical arrays are equal.|| 【参数】| ———-| other : Categorical|| 【返回值】| ——-| are_equal : boolean|| fillna(self, value=None, method=None, limit=None)| Fill NA/NaN values using the specified method.|| 【参数】| ———-| method : {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None13| Method to use for filling holes in reindexed Series| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use NEXT valid observation to fill gap| value : scalar| Value to use to fill holes (e.g. 0)| limit : int, default None| (Not implemented yet for Categorical!)| If method is specified, this is the maximum number of consecutive| NaN values to forward/backward fill. In other words, if there is| a gap with more than this number of consecutive NaNs, it will only| be partially filled. If method is not specified, this is the| maximum number of entries along the entire axis where NaNs will be| filled.|| 【返回值】| ——-| filled : Categorical with NA/NaN filled|| get_values(self)| Return the values.|| For internal compatibility with pandas formatting.|| 【返回值】| ——-| values : numpy array| A numpy array of the same dtype as categorical.categories.dtype or| Index if datetime / periods|| is_dtype_equal(self, other)| Returns True if categoricals are the same dtype| same categories, and same ordered|| 【参数】| ———-| other : Categorical|| 【返回值】| ——-| are_equal : boolean|| isnull(self)| Detect missing values|| Both missing values (-1 in .codes) and NA as a category are detected.|| 【返回值】| ——-| a boolean array of whether my values are null|| 【参见】| ——–| pandas.isnull : pandas version| Categorical.notnull : boolean inverse of Categorical.isnull|| max(self, numeric_only=None, kwargs)| The maximum value of the object.14|| Only ordered Categoricals have a maximum!|| 【Raises 引发错误】| ——| TypeError| If the Categorical is not ordered.|| 【返回值】| ——-| max : the maximum of this Categorical|| memory_usage(self, deep=False)| Memory usage of my values|| 【参数】| ———-| deep : bool| Introspect the data deeply, interrogate| object dtypes for system-level memory consumption|| 【返回值】| ——-| bytes used|| 【注意】| —–| Memory usage does not include memory consumed by elements that| are not components of the array if deep=False|| 【参见】| ——–| numpy.ndarray.nbytes|| min(self, numeric_only=None, kwargs)| The minimum value of the object.|| Only ordered Categoricals have a minimum!|| 【Raises 引发错误】| ——| TypeError| If the Categorical is not ordered.|| 【返回值】| ——-| min : the minimum of this Categorical|| mode(self)| Returns the mode(s) of the Categorical.|| Empty if nothing occurs at least 2 times. Always returns Categorical even| if only one value.|| 【返回值】| ——-15| modes : Categorical (sorted)|| notnull(self)| Reverse of isnull|| Both missing values (-1 in .codes) and NA as a category are detected as null.|| 【返回值】| ——-| a boolean array of whether my values are not null|| 【参见】| ——–| pandas.notnull : pandas version| Categorical.isnull : boolean inverse of Categorical.notnull|| order(self, inplace=False, ascending=True, na_position=’last’)| DEPRECATED: use :meth:Categorical.sort_values|| Sorts the Category by category value returning a new Categorical by default.|| Only ordered Categoricals can be sorted!|| Categorical.sort is the equivalent but sorts the Categorical inplace.|| 【参数】| ———-| inplace : boolean, default False| Do operation in place.| ascending : boolean, default True| Sort ascending. Passing False sorts descending| na_position : {‘first’, ‘last’} (optional, default=’last’)| ‘first’ puts NaNs at the beginning| ‘last’ puts NaNs at the end|| 【返回值】| ——-| y : Category or None|| 【参见】| ——–| Category.sort|| ravel(self, order=’C’)| Return a flattened (numpy) array.|| For internal compatibility with numpy arrays.|| 【返回值】| ——-| raveled : numpy array|| remove_categories(self, removals, inplace=False)| Removes the specified categories.|| removals must be included in the old categories. Values which were in the removed| categories will be set to NaN16|| 【Raises 引发错误】| ——| ValueError| If the removals are not contained in the categories|| 【参数】| ———-| removals : category or list of categories| The categories which should be removed.| inplace : boolean (default: False)| Whether or not to remove the categories inplace or return a copy of this categorical| with removed categories.|| 【返回值】| ——-| cat : Categorical with removed categories or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| add_categories| remove_unused_categories| set_categories|| remove_unused_categories(self, inplace=False)| Removes categories which are not used.|| 【参数】| ———-| inplace : boolean (default: False)| Whether or not to drop unused categories inplace or return a copy of this categorical| with unused categories dropped.|| 【返回值】| ——-| cat : Categorical with unused categories dropped or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| add_categories| remove_categories| set_categories|| rename_categories(self, new_categories, inplace=False)| Renames categories.|| The new categories has to be a list-like object. All items must be unique and the number of| items in the new categories must be the same as the number of items in the old categories.|| 【Raises 引发错误】| ——| ValueError17| If the new categories do not have the same number of items than the current categories| or do not validate as categories|| 【参数】| ———-| new_categories : Index-like| The renamed categories.| inplace : boolean (default: False)| Whether or not to rename the categories inplace or return a copy of this categorical| with renamed categories.|| 【返回值】| ——-| cat : Categorical with renamed categories added or None if inplace.|| 【参见】| ——–| reorder_categories| add_categories| remove_categories| remove_unused_categories| set_categories|| reorder_categories(self, new_categories, ordered=None, inplace=False)| Reorders categories as specified in new_categories.|| new_categories need to include all old categories and no new category items.|| 【Raises 引发错误】| ——| ValueError| If the new categories do not contain all old category items or any new ones|| 【参数】| ———-| new_categories : Index-like| The categories in new order.| ordered : boolean, optional| Whether or not the categorical is treated as a ordered categorical. If not given,| do not change the ordered information.| inplace : boolean (default: False)| Whether or not to reorder the categories inplace or return a copy of this categorical| with reordered categories.|| 【返回值】| ——-| cat : Categorical with reordered categories or None if inplace.|| 【参见】| ——–| rename_categories| add_categories| remove_categories| remove_unused_categories| set_categories|| repeat(self, repeats)18| Repeat elements of a Categorical.|| 【参见】| ——–| numpy.ndarray.repeat|| reshape(self, new_shape, kwargs)| compat with .reshape|| searchsorted(self, v, side=’left’, sorter=None)| Find indices where elements should be inserted to maintain order.|| Find the indices into a sorted Categorical self such that, if the| corresponding elements in v were inserted before the indices, the| order of self would be preserved.|| 【参数】| ———-| v : array_like| Array-like values or a scalar value, to insert/search for in self.| side : {‘left’, ‘right’}, optional| If ‘left’, the index of the first suitable location found is given.| If ‘right’, return the last such index. If there is no suitable| index, return either 0 or N (where N is the length of a).| sorter : 1-D array_like, optional| Optional array of integer indices that sort self into ascending| order. They are typically the result of np.argsort.|| 【返回值】| ——-| indices : array of ints| Array of insertion points with the same shape as v.|| 【参见】| ——–| Series.searchsorted| numpy.searchsorted|| 【注意】| —–| Binary search is used to find the required insertion points.|| 【示例】| ——–| &gt;&gt;&gt; x = pd.Categorical([‘apple’, ‘bread’, ‘bread’, ‘cheese’, ‘milk’ ])| [apple, bread, bread, cheese, milk]| Categories (4, object): [apple &lt; bread &lt; cheese &lt; milk]| &gt;&gt;&gt; x.searchsorted(‘bread’)| array([1]) # Note: an array, not a scalar| &gt;&gt;&gt; x.searchsorted([‘bread’])| array([1])| &gt;&gt;&gt; x.searchsorted([‘bread’, ‘eggs’])| array([1, 4])| &gt;&gt;&gt; x.searchsorted([‘bread’, ‘eggs’], side=’right’)| array([3, 4]) # eggs before milk| &gt;&gt;&gt; x = pd.Categorical([‘apple’, ‘bread’, ‘bread’, ‘cheese’, ‘milk’, ‘donuts’ ])| &gt;&gt;&gt; x.searchsorted([‘bread’, ‘eggs’], side=’right’, sorter=[0, 1, 2, 3, 5, 4])19| array([3, 5]) # eggs after donuts, after switching milk and donuts|| set_categories(self, new_categories, ordered=None, rename=False, inplace=False)| Sets the categories to the specified new_categories.|| new_categories can include new categories (which will result in unused categories) or| or remove old categories (which results in values set to NaN). If rename==True,| the categories will simple be renamed (less or more items than in old categories will| result in values set to NaN or in unused categories respectively).|| This method can be used to perform more than one action of adding, removing,| and reordering simultaneously and is therefore faster than performing the individual steps| via the more specialised methods.|| On the other hand this methods does not do checks (e.g., whether the old categories are| included in the new categories on a reorder), which can result in surprising changes, for| example when using special string dtypes on python3, which does not considers a S1 string| equal to a single char python string.|| 【Raises 引发错误】| ——| ValueError| If new_categories does not validate as categories|| 【参数】| ———-| new_categories : Index-like| The categories in new order.| ordered : boolean, (default: False)| Whether or not the categorical is treated as a ordered categorical. If not given,| do not change the ordered information.| rename : boolean (default: False)| Whether or not the new_categories should be considered as a rename of the old| categories or as reordered categories.| inplace : boolean (default: False)| Whether or not to reorder the categories inplace or return a copy of this categorical| with reordered categories.|| 【返回值】| ——-| cat : Categorical with reordered categories or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| add_categories| remove_categories| remove_unused_categories|| set_ordered(self, value, inplace=False)| Sets the ordered attribute to the boolean value|| 【参数】| ———-| value : boolean to set whether this categorical is ordered (True) or not (False)| inplace : boolean (default: False)20| Whether or not to set the ordered attribute inplace or return a copy of this categorical| with ordered set to the value|| shift(self, periods)| Shift Categorical by desired number of periods.|| 【参数】| ———-| periods : int| Number of periods to move, can be positive or negative|| 【返回值】| ——-| shifted : Categorical|| sort(self, inplace=True, ascending=True, na_position=’last’)| Sorts the Category inplace by category value.|| Only ordered Categoricals can be sorted!|| Catgorical.order is the equivalent but returns a new Categorical.|| 【参数】| ———-| ascending : boolean, default True| Sort ascending. Passing False sorts descending| inplace : boolean, default False| Do operation in place.| na_position : {‘first’, ‘last’} (optional, default=’last’)| ‘first’ puts NaNs at the beginning| ‘last’ puts NaNs at the end|| 【返回值】| ——-| y : Category or None|| 【参见】| ——–| Category.sort_values|| sort_values(self, inplace=False, ascending=True, na_position=’last’)| Sorts the Category by category value returning a new Categorical by default.|| Only ordered Categoricals can be sorted!|| Categorical.sort is the equivalent but sorts the Categorical inplace.|| 【参数】| ———-| inplace : boolean, default False| Do operation in place.| ascending : boolean, default True| Sort ascending. Passing False sorts descending| na_position : {‘first’, ‘last’} (optional, default=’last’)| ‘first’ puts NaNs at the beginning| ‘last’ puts NaNs at the end|21| 【返回值】| ——-| y : Category or None|| 【参见】| ——–| Category.sort|| take = take_nd(self, indexer, allow_fill=True, fill_value=None)|| take_nd(self, indexer, allow_fill=True, fill_value=None)| Take the codes by the indexer, fill with the fill_value.|| For internal compatibility with numpy arrays.|| to_dense(self)| Return my ‘dense’ representation|| For internal compatibility with numpy arrays.|| 【返回值】| ——-| dense : array|| unique(self)| Return the Categorical which categories and codes are unique.| Unused categories are NOT returned.|| - unordered category: values and categories are sorted by appearance| order.| - ordered category: values are sorted by appearance order, categories| keeps existing order.|| 【返回值】| ——-| unique values : Categorical|| value_counts(self, dropna=True)| Returns a Series containing counts of each category.|| Every category will have an entry, even those with a count of 0.|| 【参数】| ———-| dropna : boolean, default True| Don’t include counts of NaN, even if NaN is a category.|| 【返回值】| ——-| counts : Series|| view(self)| Return a view of myself.|| For internal compatibility with numpy arrays.|| 【返回值】22| ——-| view : Categorical| Returns self!|| ———————————————————————-| Class methods defined here:|| from_array(data, kwargs) from builtins.type| Make a Categorical type from a single array-like object.|| For internal compatibility with numpy arrays.|| 【参数】| ———-| data : array-like| Can be an Index or array-like. The categories are assumed to be| the unique values of data.|| from_codes(codes, categories, ordered=False, name=None) from builtins.type| Make a Categorical type from codes and categories arrays.|| This constructor is useful if you already have codes and categories and so do not need the| (computation intensive) factorization step, which is usually done on the constructor.|| If your data does not follow this convention, please use the normal constructor.|| 【参数】| ———-| codes : array-like, integers| An integer array, where each integer points to a category in categories or -1 for NaN| categories : index-like| The categories for the categorical. Items need to be unique.| ordered : boolean, (default False)| Whether or not this categorical is treated as a ordered categorical. If not given,| the resulting categorical will be unordered.|| ———————————————————————-| Data descriptors defined here:|| T|| base| compat, we are always our own object|| categories| The categories of this categorical.|| Setting assigns new values to each category (effectively a rename of| each individual category).|| The assigned value has to be a list-like object. All items must be unique and the number of items| in the new categories must be the same as the number of items in the old categories.|| Assigning to categories is a inplace operation!|| 【Raises 引发错误】| ——23| ValueError| If the new categories do not validate as categories or if the number of new categories is| unequal the number of old categories|| 【参见】| ——–| rename_categories| reorder_categories| add_categories| remove_categories| remove_unused_categories| set_categories|| codes| The category codes of this categorical.|| Level codes are an array if integer which are the positions of the real| values in the categories array.|| There is not setter, use the other categorical methods and the normal item setter to change| values in the categorical.|| itemsize|| labels| Get the category labels (deprecated).|| Deprecated, use .codes!|| levels| Gets the levels (deprecated, use “categories”)|| nbytes|| ndim|| ordered| Gets the ordered attribute|| shape| Shape of the Categorical.|| For internal compatibility with numpy arrays.|| 【返回值】| ——-| shape : tuple|| size|| ———————————————————————-| 其他数据、属性定义：|| array_priority = 1000|| hash = None|24| dtype = category|| ———————————————————————-| Methods inherited from pandas.core.base.PandasObject:|| dir(self)| Provide method name lookup and completion| Only provide ‘public’ methods|| ———————————————————————-| Methods inherited from pandas.core.base.StringMixin:|| bytes(self)| Return a string representation for a particular object.|| Invoked by bytes(obj) in py3 only.| Yields a bytestring in both py2/py3.|| repr(self)| Return a string representation for a particular object.|| Yields Bytestring in Py2, Unicode String in py3.|| str(self)| Return a string representation for a particular Object|| Invoked by str(df) in both py2/py3.| Yields Bytestring in Py2, Unicode String in py3.|| ———————————————————————-| Data descriptors inherited from pandas.core.base.StringMixin:|| dict| dictionary for instance variables (if defined)|| weakref| list of weak references to the object (if defined)CategoricalIndexCategoricalIndex 模块所属：pandas.core.index:类定义：CategoricalIndex(Index, pandas.core.base.PandasDelegate)| Immutable Index implementing an ordered, sliceable set. CategoricalIndex| represents a sparsely populated Index with an underlying Categorical.|| .. versionadded:: 0.16.1|| 【参数】| ———-25| data : array-like or Categorical, (1-dimensional)| categories : optional, array-like| categories for the CategoricalIndex| ordered : boolean,| designating if the categories are ordered| copy : bool| Make a copy of input ndarray| name : object| Name to be stored in the index|| 【方法排序】| CategoricalIndex| Index| pandas.core.base.IndexOpsMixin| pandas.core.strings.StringAccessorMixin| pandas.core.base.PandasDelegate| pandas.core.base.PandasObject| pandas.core.base.StringMixin| 【内置对象】|| 【方法定义】|| abs(self, other=None)|| add(self, other=None)|| array(self, dtype=None)| the array interface, return my values|| contains(self, key)|| eq = _evaluate_compare(self, other)|| floordiv(self, other=None)|| ge = _evaluate_compare(self, other)|| gt = _evaluate_compare(self, other)|| inv(self, other=None)|| le = _evaluate_compare(self, other)|| lt = _evaluate_compare(self, other)|| mul(self, other=None)|| ne = _evaluate_compare(self, other)|| neg(self, other=None)|| pos(self, other=None)|| radd = add(self, other=None)|| rfloordiv = floordiv(self, other=None)|26| rmul = mul(self, other=None)|| rtruediv = truediv(self, other=None)|| sub(self, other=None)|| truediv(self, other=None)|| add_categories(self, *args, kwargs)| Add new categories.|| new_categories will be included at the last/highest place in the categories and will be| unused directly after this call.|| 【Raises 引发错误】| ——| ValueError| If the new categories include old categories or do not validate as categories|| 【参数】| ———-| new_categories : category or list-like of category| The new categories to be included.| inplace : boolean (default: False)| Whether or not to add the categories inplace or return a copy of this categorical| with added categories.|| 【返回值】| ——-| cat : Categorical with new categories added or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| remove_categories| remove_unused_categories| set_categories|| all(self, other=None)|| any(self, other=None)|| append(self, other)| Append a collection of CategoricalIndex options together|| 【参数】| ———-| other : Index or list/tuple of indices|| 【返回值】| ——-| appended : Index|| 【Raises 引发错误】| ——| ValueError if other is not in the categories27|| argsort(self, args, **kwargs)| return an ndarray indexer of the underlying data|| 【参见】| ——–| numpy.ndarray.argsort|| as_ordered(self, args, kwargs)| Sets the Categorical to be ordered|| 【参数】| ———-| inplace : boolean (default: False)| Whether or not to set the ordered attribute inplace or return a copy of this categorical| with ordered set to True|| as_unordered(self, *args, kwargs)| Sets the Categorical to be unordered|| 【参数】| ———-| inplace : boolean (default: False)| Whether or not to set the ordered attribute inplace or return a copy of this categorical| with ordered set to False|| delete(self, loc)| Make new Index with passed location(-s) deleted|| 【返回值】| ——-| new_index : Index|| duplicated(self, keep=’first’)| Return boolean np.array denoting duplicate values|| 【参数】| ———-| keep : {‘first’, ‘last’, False}, default ‘first’| - first : Mark duplicates as True except for the first occurrence.| - last : Mark duplicates as True except for the last occurrence.| - False : Mark all duplicates as True.| take_last : deprecated|| 【返回值】| ——-| duplicated : np.array|| equals(self, other)| Determines if two CategorialIndex objects contain the same elements.|| fillna(self, value, downcast=None)| Fill NA/NaN values with the specified value|| 【参数】| ———-| value : scalar28| Scalar value to use to fill holes (e.g. 0).| This value cannot be a list-likes.| downcast : dict, default is None| a dict of item-&gt;dtype of what to downcast if possible,| or the string ‘infer’ which will try to downcast to an appropriate| equal type (e.g. float64 to int64 if possible)|| 【返回值】| ——-| filled : Index|| get_indexer(self, target, method=None, limit=None, tolerance=None)| Compute indexer and mask for new index given the current index. The| indexer should be then used as an input to ndarray.take to align the| current data to the new index. The mask determines whether labels are| found or not in the current index|| 【参数】| ———-| target : MultiIndex or Index (of tuples)| method : {‘pad’, ‘ffill’, ‘backfill’, ‘bfill’}| pad / ffill: propagate LAST valid observation forward to next valid| backfill / bfill: use NEXT valid observation to fill gap|| 【注意】| —–| This is a low-level method and probably should be used at your own risk|| 【示例】| ——–| &gt;&gt;&gt; indexer, mask = index.get_indexer(new_index)| &gt;&gt;&gt; new_values = cur_values.take(indexer)| &gt;&gt;&gt; new_values[-mask] = np.nan|| 【返回值】| ——-| (indexer, mask) : (ndarray, ndarray)|| get_indexer_non_unique(self, target)| this is the same for a CategoricalIndex for get_indexer; the API returns the missing values as well|| get_loc(self, key, method=None)| Get integer location for requested label|| 【参数】| ———-| key : label| method : {None}| default: exact matches only.|| 【返回值】| ——-| loc : int if unique index, possibly slice or mask if not|| get_values(self)| return the underlying data as an ndarray|29| insert(self, loc, item)| Make new Index inserting new item at location. Follows| Python list.append semantics for negative values|| 【参数】| ———-| loc : int| item : object|| 【返回值】| ——-| new_index : Index|| 【Raises 引发错误】| ——| ValueError if the item is not in the categories|| max(self, args, kwargs)| The maximum value of the object.|| Only ordered Categoricals have a maximum!|| 【Raises 引发错误】| ——| TypeError| If the Categorical is not ordered.|| 【返回值】| ——-| max : the maximum of this Categorical|| min(self, *args, kwargs)| The minimum value of the object.|| Only ordered Categoricals have a minimum!|| 【Raises 引发错误】| ——| TypeError| If the Categorical is not ordered.|| 【返回值】| ——-| min : the minimum of this Categorical|| reindex(self, target, method=None, level=None, limit=None, tolerance=None)| Create index with target’s values (move/add/delete values as necessary)|| 【返回值】| ——-| new_index : pd.Index| Resulting index| indexer : np.ndarray or None| Indices of output values in original index|| remove_categories(self, args, **kwargs)30| Removes the specified categories.|| removals must be included in the old categories. Values which were in the removed| categories will be set to NaN|| 【Raises 引发错误】| ——| ValueError| If the removals are not contained in the categories|| 【参数】| ———-| removals : category or list of categories| The categories which should be removed.| inplace : boolean (default: False)| Whether or not to remove the categories inplace or return a copy of this categorical| with removed categories.|| 【返回值】| ——-| cat : Categorical with removed categories or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| add_categories| remove_unused_categories| set_categories|| remove_unused_categories(self, args, kwargs)| Removes categories which are not used.|| 【参数】| ———-| inplace : boolean (default: False)| Whether or not to drop unused categories inplace or return a copy of this categorical| with unused categories dropped.|| 【返回值】| ——-| cat : Categorical with unused categories dropped or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| add_categories| remove_categories| set_categories|| rename_categories(self, *args, kwargs)| Renames categories.|| The new categories has to be a list-like object. All items must be unique and the number of| items in the new categories must be the same as the number of items in the old categories.|31| 【Raises 引发错误】| ——| ValueError| If the new categories do not have the same number of items than the current categories| or do not validate as categories|| 【参数】| ———-| new_categories : Index-like| The renamed categories.| inplace : boolean (default: False)| Whether or not to rename the categories inplace or return a copy of this categorical| with renamed categories.|| 【返回值】| ——-| cat : Categorical with renamed categories added or None if inplace.|| 【参见】| ——–| reorder_categories| add_categories| remove_categories| remove_unused_categories| set_categories|| reorder_categories(self, args, **kwargs)| Reorders categories as specified in new_categories.|| new_categories need to include all old categories and no new category items.|| 【Raises 引发错误】| ——| ValueError| If the new categories do not contain all old category items or any new ones|| 【参数】| ———-| new_categories : Index-like| The categories in new order.| ordered : boolean, optional| Whether or not the categorical is treated as a ordered categorical. If not given,| do not change the ordered information.| inplace : boolean (default: False)| Whether or not to reorder the categories inplace or return a copy of this categorical| with reordered categories.|| 【返回值】| ——-| cat : Categorical with reordered categories or None if inplace.|| 【参见】| ——–| rename_categories| add_categories| remove_categories32| remove_unused_categories| set_categories|| set_categories(self, args, kwargs)| Sets the categories to the specified new_categories.|| new_categories can include new categories (which will result in unused categories) or| or remove old categories (which results in values set to NaN). If rename==True,| the categories will simple be renamed (less or more items than in old categories will| result in values set to NaN or in unused categories respectively).|| This method can be used to perform more than one action of adding, removing,| and reordering simultaneously and is therefore faster than performing the individual steps| via the more specialised methods.|| On the other hand this methods does not do checks (e.g., whether the old categories are| included in the new categories on a reorder), which can result in surprising changes, for| example when using special string dtypes on python3, which does not considers a S1 string| equal to a single char python string.|| 【Raises 引发错误】| ——| ValueError| If new_categories does not validate as categories|| 【参数】| ———-| new_categories : Index-like| The categories in new order.| ordered : boolean, (default: False)| Whether or not the categorical is treated as a ordered categorical. If not given,| do not change the ordered information.| rename : boolean (default: False)| Whether or not the new_categories should be considered as a rename of the old| categories or as reordered categories.| inplace : boolean (default: False)| Whether or not to reorder the categories inplace or return a copy of this categorical| with reordered categories.|| 【返回值】| ——-| cat : Categorical with reordered categories or None if inplace.|| 【参见】| ——–| rename_categories| reorder_categories| add_categories| remove_categories| remove_unused_categories|| take(self, indexer, axis=0, allow_fill=True, fill_value=None)| For internal compatibility with numpy arrays.|| # filling must always be None/nan here| # but is passed thru internally| assert isnull(fill_value)33|| 【参见】| ——–| numpy.ndarray.take|| ———————————————————————-| Static methods defined here:|| new(cls, data=None, categories=None, ordered=None, dtype=None, copy=False, name=None, fastpath=False, kwargs)| Create and return a new object. See help(type) for accurate signature.|| ———————————————————————-| Data descriptors defined here:|| categories|| codes|| inferred_type|| is_unique|| ordered|| values| return the underlying data, which is a Categorical|| ———————————————————————-| Methods inherited from Index:|| and(self, other)|| array_wrap(self, result, context=None)| Gets called after a ufunc|| bool = nonzero(self)|| copy = copy(self, names=None, name=None, dtype=None, deep=False)| Make a copy of this object. Name and dtype sets those attributes on| the new object.|| 【参数】| ———-| name : string, optional| dtype : numpy dtype or pandas type|| 【返回值】| ——-| copy : Index|| 【注意】| —–| In most cases, there should be no functional difference from using| deep, but if deep is passed it will attempt to deepcopy.|| deepcopy(self, memo={})34|| getitem(self, key)| Override numpy.ndarray’s getitem method to work as desired.|| This function adds lists and Series as valid boolean indexers| (ndarrays only supports ndarray with dtype=bool).|| If resulting ndim != 1, plain ndarray is returned instead of| corresponding Index subclass.|| hash(self)| Return hash(self).|| iadd = add(self, other)|| iter(self)|| len(self)| return the length of the Index|| nonzero(self)|| or(self, other)|| reduce(self)| helper for pickle|| setitem(self, key, value)|| setstate(self, state)| Necessary for making this object picklable|| unicode(self)| Return a string representation for this object.|| Invoked by unicode(df) in py2 only. Yields a Unicode String in both| py2/py3.|| xor(self, other)|| asof(self, label)| For a sorted index, return the most recent label up to and including| the passed label. Return NaN if not found.|| 【参见】| ——–| get_loc : asof is a thin wrapper around get_loc with method=’pad’|| asof_locs(self, where, mask)| where : array of timestamps| mask : array of booleans where data is not NA|| astype(self, dtype)|| copy(self, names=None, name=None, dtype=None, deep=False)| Make a copy of this object. Name and dtype sets those attributes on| the new object.35|| 【参数】| ———-| name : string, optional| dtype : numpy dtype or pandas type|| 【返回值】| ——-| copy : Index|| 【注意】| —–| In most cases, there should be no functional difference from using| deep, but if deep is passed it will attempt to deepcopy.|| diff = wrapper(args, kwargs)|| difference(self, other)| Return a new Index with elements from the index that are not in other.|| This is the sorted set difference of two Index objects.|| 【参数】| ———-| other : Index or array-like|| 【返回值】| ——-| difference : Index|| 【示例】| ——–|| &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4])| &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6])| &gt;&gt;&gt; idx1.difference(idx2)| Int64Index([1, 2], dtype=’int64’)|| drop(self, labels, errors=’raise’)| Make new Index with passed list of labels deleted|| 【参数】| ———-| labels : array-like| errors : {‘ignore’, ‘raise’}, default ‘raise’| If ‘ignore’, suppress error and existing labels are dropped.|| 【返回值】| ——-| dropped : Index|| drop_duplicates(self, keep=’first’)| Return Index with duplicate values removed|| 【参数】| ———-36|| keep : {‘first’, ‘last’, False}, default ‘first’| - first : Drop duplicates except for the first occurrence.| - last : Drop duplicates except for the last occurrence.| - False : Drop all duplicates.| take_last : deprecated||| 【返回值】| ——-| deduplicated : Index|| format(self, name=False, formatter=None, kwargs)| Render a string representation of the Index|| get_duplicates(self)|| get_indexer_for(self, target, **kwargs)| guaranteed return of an indexer even when non-unique|| get_level_values(self, level)| Return vector of label values for requested level, equal to the length| of the index|| 【参数】| ———-| level : int|| 【返回值】| ——-| values : ndarray|| get_slice_bound(self, label, side, kind)| Calculate slice bound that corresponds to given label.|| Returns leftmost (one-past-the-rightmost if side==&#39;right&#39;) position| of given label.|| 【参数】| ———-| label : object| side : {‘left’, ‘right’}| kind : string / None, the type of indexer|| get_value(self, series, key)| Fast lookup of value from 1-dimensional ndarray. Only use this if you| know what you’re doing|| groupby(self, to_groupby)| Group the index labels by a given array of values.|| 【参数】| ———-| to_groupby : array| Values used to determine the groups.|| 【返回值】37| ——-| groups : dict| {group name -&gt; group labels}|| holdsinteger(self)|| identical(self, other)| Similar to equals, but check that other comparable attributes are| also equal|| intersection(self, other)| Form the intersection of two Index objects.|| This returns a new Index with elements common to the index and other.| Sortedness of the result is not guaranteed.|| 【参数】| ———-| other : Index or array-like|| 【返回值】| ——-| intersection : Index|| 【示例】| ——–|| &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4])| &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6])| &gt;&gt;&gt; idx1.intersection(idx2)| Int64Index([3, 4], dtype=’int64’)|| is(self, other)| More flexible, faster check like is but that works through views|| Note: this is not the same as Index.identical(), which checks| that metadata is also the same.|| 【参数】| ———-| other : object| other object to compare against.|| 【返回值】| ——-| True if both have same underlying data, False otherwise : bool|| is_boolean(self)|| is_categorical(self)|| is_floating(self)|| is_integer(self)|| is_lexsorted_for_tuple(self, tup)|38| is_mixed(self)|| is_numeric(self)|| is_object(self)|| is_type_compatible(self, kind)|| isin(self, values, level=None)| Compute boolean array of whether each index value is found in the| passed set of values.|| 【参数】| ———-| values : set or sequence of values| Sought values.| level : str or int, optional| Name or position of the index level to use (if the index is a| MultiIndex).|| 【注意】| —–| If level is specified:|| - if it is the name of one and only one index level, use that level;| - otherwise it should be a number indicating level position.|| 【返回值】| ——-| is_contained : ndarray (boolean dtype)|| join(self, other, how=’left’, level=None, return_indexers=False)| this is an internal non-public method|| Compute join_index and indexers to conform data| structures to the new index.|| 【参数】| ———-| other : Index| how : {‘left’, ‘right’, ‘inner’, ‘outer’}| level : int or level name, default None| return_indexers : boolean, default False|| 【返回值】| ——-| join_index, (left_indexer, right_indexer)|| map(self, mapper)|| order(self, return_indexer=False, ascending=True)| Return sorted copy of Index|| DEPRECATED: use :meth:Index.sort_values|| putmask(self, mask, value)| return a new Index of the values set with the mask39|| 【参见】| ——–| numpy.ndarray.putmask|| ravel(self, order=’C’)| return an ndarray of the flattened values of the underlying data|| 【参见】| ——–| numpy.ndarray.ravel|| rename(self, name, inplace=False)| Set new names on index. Defaults to returning new index.|| 【参数】| ———-| name : str or list| name to set| inplace : bool| if True, mutates in place|| 【返回值】| ——-| new index (of same type and class…etc) [if inplace, returns None]|| repeat(self, n)| return a new Index of the values repeated n times|| 【参见】| ——–| numpy.ndarray.repeat|| set_names(self, names, level=None, inplace=False)| Set new names on index. Defaults to returning new index.|| 【参数】| ———-| names : str or sequence| name(s) to set| level : int or level name, or sequence of int / level names (default None)| If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels)| Otherwise level must be None| inplace : bool| if True, mutates in place|| 【返回值】| ——-| new index (of same type and class…etc) [if inplace, returns None]|| 【示例】| ——–| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(‘foo’)| Int64Index([1, 2, 3, 4], dtype=’int64’)| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([‘foo’])| Int64Index([1, 2, 3, 4], dtype=’int64’)40| &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u’one’), (1, u’two’),| (2, u’one’), (2, u’two’)],| names=[‘foo’, ‘bar’])| &gt;&gt;&gt; idx.set_names([‘baz’, ‘quz’])| MultiIndex(levels=[[1, 2], [u’one’, u’two’]],| labels=[[0, 0, 1, 1], [0, 1, 0, 1]],| names=[u’baz’, u’quz’])| &gt;&gt;&gt; idx.set_names(‘baz’, level=0)| MultiIndex(levels=[[1, 2], [u’one’, u’two’]],| labels=[[0, 0, 1, 1], [0, 1, 0, 1]],| names=[u’baz’, u’bar’])|| set_value(self, arr, key, value)| Fast lookup of value from 1-dimensional ndarray. Only use this if you| know what you’re doing|| shift(self, periods=1, freq=None)| Shift Index containing datetime objects by input number of periods and| DateOffset|| 【返回值】| ——-| shifted : Index|| slice_indexer(self, start=None, end=None, step=None, kind=None)| For an ordered Index, compute the slice indexer for input labels and| step|| 【参数】| ———-| start : label, default None| If None, defaults to the beginning| end : label, default None| If None, defaults to the end| step : int, default None| kind : string, default None|| 【返回值】| ——-| indexer : ndarray or slice|| 【注意】| —–| This function assumes that the data is sorted, so use at your own peril|| slice_locs(self, start=None, end=None, step=None, kind=None)| Compute slice locations for input labels.|| 【参数】| ———-| start : label, default None| If None, defaults to the beginning| end : label, default None| If None, defaults to the end| step : int, defaults None| If None, defaults to 1| kind : string, defaults None41|| 【返回值】| ——-| start, end : int|| sort(self, args, kwargs)|| sort_values(self, return_indexer=False, ascending=True)| Return sorted copy of Index|| sortlevel(self, level=None, ascending=True, sort_remaining=None)| For internal compatibility with with the Index API|| Sort the Index. This is for compat with MultiIndex|| 【参数】| ———-| ascending : boolean, default True| False to sort in descending order|| level, sort_remaining are compat paramaters|| 【返回值】| ——-| sorted_index : Index|| summary(self, name=None)|| sym_diff(self, other, result_name=None)| Compute the sorted symmetric difference of two Index objects.|| 【参数】| ———-| other : Index or array-like| result_name : str|| 【返回值】| ——-| sym_diff : Index|| 【注意】| —–| sym_diff contains elements that appear in either idx1 or| idx2 but not both. Equivalent to the Index created by| (idx1 - idx2) + (idx2 - idx1) with duplicates dropped.|| The sorting of a result containing NaN values is not guaranteed| across Python versions. See GitHub issue #6444.|| 【示例】| ——–| &gt;&gt;&gt; idx1 = Index([1, 2, 3, 4])| &gt;&gt;&gt; idx2 = Index([2, 3, 4, 5])| &gt;&gt;&gt; idx1.sym_diff(idx2)| Int64Index([1, 5], dtype=’int64’)|| You can also use the ^ operator:42|| &gt;&gt;&gt; idx1 ^ idx2| Int64Index([1, 5], dtype=’int64’)|| to_datetime(self, dayfirst=False)| For an Index containing strings or datetime.datetime objects, attempt| conversion to DatetimeIndex|| to_native_types(self, slicer=None, kwargs)| slice and dice then format|| to_series(self, kwargs)| Create a Series with both index and values equal to the index keys| useful with map for returning an indexer based on an index|| 【返回值】| ——-| Series : dtype will be based on the type of the Index values.|| tolist(self)| return a list of the Index values|| union(self, other)| Form the union of two Index objects and sorts if possible.|| 【参数】| ———-| other : Index or array-like|| 【返回值】| ——-| union : Index|| 【示例】| ——–|| &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4])| &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6])| &gt;&gt;&gt; idx1.union(idx2)| Int64Index([1, 2, 3, 4, 5, 6], dtype=’int64’)|| view(self, cls=None)|| ———————————————————————-| Data descriptors inherited from Index:|| dtype|| dtype_str|| has_duplicates|| hasnans|| is_all_dates|| is_monotonic43| alias for is_monotonic_increasing (deprecated)|| is_monotonic_decreasing| return if the index is monotonic decreasing (only equal or| decreasing) values.|| is_monotonic_increasing| return if the index is monotonic increasing (only equal or| increasing) values.|| names|| nlevels|| ———————————————————————-| Data and other attributes inherited from Index:|| asi8 = None|| name = None|| ———————————————————————-| Methods inherited from pandas.core.base.IndexOpsMixin:|| argmax(self, axis=None)| return a ndarray of the maximum argument indexer|| 【参见】| ——–| numpy.ndarray.argmax|| argmin(self, axis=None)| return a ndarray of the minimum argument indexer|| 【参见】| ——–| numpy.ndarray.argmin|| factorize(self, sort=False, na_sentinel=-1)| Encode the object as an enumerated type or categorical variable|| 【参数】| ———-| sort : boolean, default False| Sort by values| na_sentinel: int, default -1| Value to mark “not found”|| 【返回值】| ——-| labels : the indexer to the original array| uniques : the unique Index|| item(self)| return the first element of the underlying data as a python scalar|| memory_usage(self, deep=False)44| Memory usage of my values|| 【参数】| ———-| deep : bool| Introspect the data deeply, interrogate| object dtypes for system-level memory consumption|| 【返回值】| ——-| bytes used|| 【注意】| —–| Memory usage does not include memory consumed by elements that| are not components of the array if deep=False|| 【参见】| ——–| numpy.ndarray.nbytes|| nunique(self, dropna=True)| Return number of unique elements in the object.|| Excludes NA values by default.|| 【参数】| ———-| dropna : boolean, default True| Don’t include NaN in the count.|| 【返回值】| ——-| nunique : int|| searchsorted(self, key, side=’left’)| np.ndarray searchsorted compat|| transpose(self)| return the transpose, which is by definition self|| unique(self)| Return array of unique values in the object. Significantly faster than| numpy.unique. Includes NA values.|| 【返回值】| ——-| uniques : ndarray|| value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True)| Returns object containing counts of unique values.|| The resulting object will be in descending order so that the| first element is the most frequently-occurring element.| Excludes NA values by default.|45| 【参数】| ———-| normalize : boolean, default False| If True then the object returned will contain the relative| frequencies of the unique values.| sort : boolean, default True| Sort by values| ascending : boolean, default False| Sort in ascending order| bins : integer, optional| Rather than count values, group them into half-open bins,| a convenience for pd.cut, only works with numeric data| dropna : boolean, default True| Don’t include counts of NaN.|| 【返回值】| ——-| counts : Series|| ———————————————————————-| Data descriptors inherited from pandas.core.base.IndexOpsMixin:|| T| return the transpose, which is by definition self|| dict| dictionary for instance variables (if defined)|| weakref| list of weak references to the object (if defined)|| base| return the base object if the memory of the underlying data is shared|| data| return the data pointer of the underlying data|| flags| return the ndarray.flags for the underlying data|| itemsize| return the size of the dtype of the item of the underlying data|| nbytes| return the number of bytes in the underlying data|| ndim| return the number of dimensions of the underlying data, by definition 1|| shape| return a tuple of the shape of the underlying data|| size| return the number of elements in the underlying data|| strides| return the strides of the underlying data46|| ———————————————————————-| Data and other attributes inherited from pandas.core.base.IndexOpsMixin:|| array_priority = 1000|| ———————————————————————-| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin:|| str = | Vectorized string functions for Series and Index. NAs stay NA unless| handled otherwise by a particular method. Patterned after Python’s string| methods, with some inspiration from R’s stringr package.|| 【示例】| ——–| &gt;&gt;&gt; s.str.split(‘‘)| &gt;&gt;&gt; s.str.replace(‘‘, ‘’)|| ———————————————————————-| Methods inherited from pandas.core.base.PandasObject:|| dir(self)| Provide method name lookup and completion| Only provide ‘public’ methods|| ———————————————————————-| Methods inherited from pandas.core.base.StringMixin:|| bytes(self)| Return a string representation for a particular object.|| Invoked by bytes(obj) in py3 only.| Yields a bytestring in both py2/py3.|| repr(self)| Return a string representation for a particular object.|| Yields Bytestring in Py2, Unicode String in py3.|| str(self)| Return a string representation for a particular Object|| Invoked by str(df) in both py2/py3.| Yields Bytestring in Py2, Unicode String in py3.DataFrameDataFrame 模块所属：pandas.core.frame:47类定义：DataFrame(pandas.core.generic.NDFrame)| Two-dimensional size-mutable, potentially heterogeneous tabular data| structure with labeled axes (rows and columns). Arithmetic operations| align on both row and column labels. Can be thought of as a dict-like| container for Series objects. The primary pandas data structure|| 【参数】| ———-| data : numpy ndarray (structured or homogeneous), dict, or DataFrame| Dict can contain Series, arrays, constants, or list-like objects| index : Index or array-like| Index to use for resulting frame. Will default to np.arange(n) if| no indexing information part of input data and no index provided| columns : Index or array-like| Column labels to use for resulting frame. Will default to| np.arange(n) if no column labels are provided| dtype : dtype, default None| Data type to force, otherwise infer| copy : boolean, default False| Copy data from inputs. Only affects DataFrame / 2d ndarray input|| 【示例】| ——–| &gt;&gt;&gt; d = {‘col1’: ts1, ‘col2’: ts2}| &gt;&gt;&gt; df = DataFrame(data=d, index=index)| &gt;&gt;&gt; df2 = DataFrame(np.random.randn(10, 5))| &gt;&gt;&gt; df3 = DataFrame(np.random.randn(10, 5),| … columns=[‘a’, ‘b’, ‘c’, ‘d’, ‘e’])||【参见】| ——–| DataFrame.from_records : constructor from tuples, also record arrays| DataFrame.from_dict : from dicts of Series, arrays, or dicts| DataFrame.from_items : from sequence of (key, value) pairs| pandas.read_csv, pandas.read_table, pandas.read_clipboard|| 【方法排序】| DataFrame| pandas.core.generic.NDFrame| pandas.core.base.PandasObject| pandas.core.base.StringMixin| 【内置对象】|| 【方法定义】|| add(self, other, axis=None, level=None, fill_value=None)| Binary operator add with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are48| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| and(self, other, axis=’columns’, level=None, fill_value=None)| Binary operator and with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| div = truediv(self, other, axis=None, level=None, fill_value=None)|| eq(self, other)| Wrapper for comparison method eq|| floordiv(self, other, axis=None, level=None, fill_value=None)| Binary operator floordiv with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level49|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| ge(self, other)| Wrapper for comparison method ge|| getitem(self, key)|| gt(self, other)| Wrapper for comparison method gt|| iadd = f(self, other)|| imul = f(self, other)|| init(self, data=None, index=None, columns=None, dtype=None, copy=False)| Initialize self. See help(type(self)) for accurate signature.|| ipow = f(self, other)|| isub = f(self, other)|| itruediv = f(self, other)|| le(self, other)| Wrapper for comparison method le|| len(self)| Returns length of info axis, but here we use the index|| lt(self, other)| Wrapper for comparison method lt|| mod(self, other, axis=None, level=None, fill_value=None)| Binary operator mod with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–50| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| mul(self, other, axis=None, level=None, fill_value=None)| Binary operator mul with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| ne(self, other)| Wrapper for comparison method ne|| or(self, other, axis=’columns’, level=None, fill_value=None)| Binary operator or with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|51| pow(self, other, axis=None, level=None, fill_value=None)| Binary operator pow with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| radd(self, other, axis=None, level=None, fill_value=None)| Binary operator radd with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| rand(self, other, axis=’columns’, level=None, fill_value=None)| Binary operator rand with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on52| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| rdiv = rtruediv(self, other, axis=None, level=None, fill_value=None)|| rfloordiv(self, other, axis=None, level=None, fill_value=None)| Binary operator rfloordiv with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| rmod(self, other, axis=None, level=None, fill_value=None)| Binary operator rmod with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|53| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| rmul(self, other, axis=None, level=None, fill_value=None)| Binary operator rmul with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| ror(self, other, axis=’columns’, level=None, fill_value=None)| Binary operator ror with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|54| rpow(self, other, axis=None, level=None, fill_value=None)| Binary operator rpow with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| rsub(self, other, axis=None, level=None, fill_value=None)| Binary operator rsub with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| rtruediv(self, other, axis=None, level=None, fill_value=None)| Binary operator rtruediv with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on55| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| rxor(self, other, axis=’columns’, level=None, fill_value=None)| Binary operator rxor with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| setitem(self, key, value)|| sub(self, other, axis=None, level=None, fill_value=None)| Binary operator sub with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|56| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| truediv(self, other, axis=None, level=None, fill_value=None)| Binary operator truediv with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| unicode(self)| Return a string representation for a particular DataFrame|| Invoked by unicode(df) in py2 only. Yields a Unicode String in both| py2/py3.|| xor(self, other, axis=’columns’, level=None, fill_value=None)| Binary operator xor with support to substitute a fill_value for missing data in| one of the inputs|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together57|| 【返回值】| ——-| result : DataFrame|| add(self, other, axis=’columns’, level=None, fill_value=None)| Addition of dataframe and other, element-wise (binary operator add).|| Equivalent to dataframe + other, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.radd|| align(self, other, join=’outer’, axis=None, level=None, copy=True, fill_value=None, method=None, limit=None, fill_axis=0,broadcast_axis=None)| Align two object on their axes with the| specified join method for each axis Index|| 【参数】| ———-| other : DataFrame or Series| join : {‘outer’, ‘inner’, ‘left’, ‘right’}, default ‘outer’| axis : allowed axis of the other object, default None| Align on index (0), columns (1), or both (None)| level : int or level name, default None| Broadcast across a level, matching Index values on the| passed MultiIndex level| copy : boolean, default True| Always returns new objects. If copy=False and no reindexing is| required then original objects are returned.| fill_value : scalar, default np.NaN| Value to use for missing values. Defaults to NaN, but can be any| “compatible” value| method : str, default None| limit : int, default None58| fill_axis : {0, 1, ‘index’, ‘columns’}, default 0| Filling axis, method and limit| broadcast_axis : {0, 1, ‘index’, ‘columns’}, default None| Broadcast values along this axis, if aligning two objects of| different dimensions|| .. versionadded:: 0.17.0|| 【返回值】| ——-| (left, right) : (DataFrame, type of other)| Aligned objects|| all(self, axis=None, bool_only=None, skipna=None, level=None, kwargs)| Return whether all elements are True over requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| bool_only : boolean, default None| Include only boolean data. If None, will attempt to use everything,| then use only boolean data|| 【返回值】| ——-| all : Series or DataFrame (if level specified)|| any(self, axis=None, bool_only=None, skipna=None, level=None, kwargs)| Return whether any element is True over requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| bool_only : boolean, default None| Include only boolean data. If None, will attempt to use everything,| then use only boolean data|| 【返回值】| ——-| any : Series or DataFrame (if level specified)|| append(self, other, ignore_index=False, verify_integrity=False)| Append rows of other to the end of this frame, returning a new| object. Columns not in this frame are added as new columns.|59| 【参数】| ———-| other : DataFrame or Series/dict-like object, or list of these| The data to append.| ignore_index : boolean, default False| If True, do not use the index labels.| verify_integrity : boolean, default False| If True, raise ValueError on creating index with duplicates.|| 【返回值】| ——-| appended : DataFrame|| 【注意】| —–| If a list of dict/series is passed and the keys are all contained in the| DataFrame’s index, the order of the columns in the resulting DataFrame| will be unchanged.|| 【参见】| ——–| pandas.concat : General function to concatenate DataFrame, Series| or Panel objects|| 【示例】| ——–|| &gt;&gt;&gt; df = pd.DataFrame([[1, 2], [3, 4]], columns=list(‘AB’))| &gt;&gt;&gt; df| A B| 0 1 2| 1 3 4| &gt;&gt;&gt; df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list(‘AB’))| &gt;&gt;&gt; df.append(df2)| A B| 0 1 2| 1 3 4| 0 5 6| 1 7 8|| With ignore_index set to True:|| &gt;&gt;&gt; df.append(df2, ignore_index=True)| A B| 0 1 2| 1 3 4| 2 5 6| 3 7 8|| apply(self, func, axis=0, broadcast=False, raw=False, reduce=None, args=(), kwds)| Applies function along input axis of DataFrame.|| Objects passed to functions are Series objects having index| either the DataFrame’s index (axis=0) or the columns (axis=1).| Return type depends on whether passed function aggregates, or the| reduce argument if the DataFrame is empty.|60| 【参数】| ———-| func : function| Function to apply to each column/row| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| 0 or ‘index’: apply function to each column| 1 or ‘columns’: apply function to each row| broadcast : boolean, default False| For aggregation functions, return object of same size with values| propagated| raw : boolean, default False| If False, convert each row or column into a Series. If raw=True the| passed function will receive ndarray objects instead. If you are| just applying a NumPy reduction function this will achieve much| better performance| reduce : boolean or None, default None| Try to apply reduction procedures. If the DataFrame is empty,| apply will use reduce to determine whether the result should be a| Series or a DataFrame. If reduce is None (the default), apply’s| return value will be guessed by calling func an empty Series (note:| while guessing, exceptions raised by func will be ignored). If| reduce is True a Series will always be returned, and if False a| DataFrame will always be returned.| args : tuple| Positional arguments to pass to function in addition to the| array/series| Additional keyword arguments will be passed as keywords to the function|| 【注意】| —–| In the current implementation apply calls func twice on the| first column/row to decide whether it can take a fast or slow| code path. This can lead to unexpected behavior if func has| side-effects, as they will take effect twice for the first| column/row.|| 【示例】| ——–| &gt;&gt;&gt; df.apply(numpy.sqrt) # returns DataFrame| &gt;&gt;&gt; df.apply(numpy.sum, axis=0) # equiv to df.sum(0)| &gt;&gt;&gt; df.apply(numpy.sum, axis=1) # equiv to df.sum(1)|| 【参见】| ——–| DataFrame.applymap: For elementwise operations|| 【返回值】| ——-| applied : Series or DataFrame|| applymap(self, func)| Apply a function to a DataFrame that is intended to operate| elementwise, i.e. like doing map(func, series) for each series in the| DataFrame|| 【参数】| ———-61| func : function| Python function, returns a single value from a single value|| 【返回值】| ——-| applied : DataFrame|| 【参见】| ——–| DataFrame.apply : For operations on rows/columns|| assign(self, kwargs)| Assign new columns to a DataFrame, returning a new object| (a copy) with all the original columns in addition to the new ones.|| .. versionadded:: 0.16.0|| 【参数】| ———-| kwargs : keyword, value pairs| keywords are the column names. If the values are| callable, they are computed on the DataFrame and| assigned to the new columns. If the values are| not callable, (e.g. a Series, scalar, or array),| they are simply assigned.|| 【返回值】| ——-| df : DataFrame| A new DataFrame with the new columns in addition to| all the existing columns.|| 【注意】| —–| Since kwargs is a dictionary, the order of your| arguments may not be preserved. The make things predicatable,| the columns are inserted in alphabetical order, at the end of| your DataFrame. Assigning multiple columns within the same| assign is possible, but you cannot reference other columns| created within the same assign call.|| 【示例】| ——–| &gt;&gt;&gt; df = DataFrame({‘A’: range(1, 11), ‘B’: np.random.randn(10)})|| Where the value is a callable, evaluated on df:|| &gt;&gt;&gt; df.assign(ln_A = lambda x: np.log(x.A))| A B ln_A| 0 1 0.426905 0.000000| 1 2 -0.780949 0.693147| 2 3 -0.418711 1.098612| 3 4 -0.269708 1.386294| 4 5 -0.274002 1.609438| 5 6 -0.500792 1.791759| 6 7 1.649697 1.945910| 7 8 -1.495604 2.07944262| 8 9 0.549296 2.197225| 9 10 -0.758542 2.302585|| Where the value already exists and is inserted:|| &gt;&gt;&gt; newcol = np.log(df[‘A’])| &gt;&gt;&gt; df.assign(ln_A=newcol)| A B ln_A| 0 1 0.426905 0.000000| 1 2 -0.780949 0.693147| 2 3 -0.418711 1.098612| 3 4 -0.269708 1.386294| 4 5 -0.274002 1.609438| 5 6 -0.500792 1.791759| 6 7 1.649697 1.945910| 7 8 -1.495604 2.079442| 8 9 0.549296 2.197225| 9 10 -0.758542 2.302585|| boxplot(self, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None,return_type=None, kwds)| Make a box plot from DataFrame column optionally grouped by some columns or| other inputs|| 【参数】| ———-| data : the pandas object holding the data| column : column name or list of names, or vector| Can be any valid input to groupby| by : string or sequence| Column in the DataFrame to group by| ax : Matplotlib axes object, optional| fontsize : int or string| rot : label rotation angle| figsize : A tuple (width, height) in inches| grid : Setting this to True will show the grid| layout : tuple (optional)| (rows, columns) for the layout of the plot| return_type : {‘axes’, ‘dict’, ‘both’}, default ‘dict’| The kind of object to return. ‘dict’ returns a dictionary| whose values are the matplotlib Lines of the boxplot;| ‘axes’ returns the matplotlib axes the boxplot is drawn on;| ‘both’ returns a namedtuple with the axes and dict.|| When grouping with by, a dict mapping columns to return_type| is returned.|| kwds : other plotting keyword arguments to be passed to matplotlib boxplot| function|| 【返回值】| ——-| lines : dict| ax : matplotlib Axes| (ax, lines): namedtuple|| 【注意】63| —–| Use return_type=&#39;dict&#39; when you want to tweak the appearance| of the lines after plotting. In this case a dict containing the Lines| making up the boxes, caps, fliers, medians, and whiskers is returned.|| combine(self, other, func, fill_value=None, overwrite=True)| Add two DataFrame objects and do not propagate NaN values, so if for a| (column, time) one frame is missing a value, it will default to the| other frame’s value (which might be NaN as well)|| 【参数】| ———-| other : DataFrame| func : function| fill_value : scalar value| overwrite : boolean, default True| If True then overwrite values for common keys in the calling frame|| 【返回值】| ——-| result : DataFrame|| combineAdd(self, other)| DEPRECATED. Use DataFrame.add(other, fill_value=0.) instead.|| Add two DataFrame objects and do not propagate| NaN values, so if for a (column, time) one frame is missing a| value, it will default to the other frame’s value (which might| be NaN as well)|| 【参数】| ———-| other : DataFrame|| 【返回值】| ——-| DataFrame|| 【参见】| ——–| DataFrame.add|| combineMult(self, other)| DEPRECATED. Use DataFrame.mul(other, fill_value=1.) instead.|| Multiply two DataFrame objects and do not propagate NaN values, so if| for a (column, time) one frame is missing a value, it will default to| the other frame’s value (which might be NaN as well)|| 【参数】| ———-| other : DataFrame|| 【返回值】| ——-| DataFrame|64| 【参见】| ——–| DataFrame.mul|| combine_first(self, other)| Combine two DataFrame objects and default to non-null values in frame| calling the method. Result index columns will be the union of the| respective indexes and columns|| 【参数】| ———-| other : DataFrame|| 【示例】| ——–| a’s values prioritized, use values from b to fill holes:|| &gt;&gt;&gt; a.combine_first(b)||| 【返回值】| ——-| combined : DataFrame|| compound(self, axis=None, skipna=None, level=None)| Return the compound percentage of the values for the requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| compounded : Series or DataFrame (if level specified)|| corr(self, method=’pearson’, min_periods=1)| Compute pairwise correlation of columns, excluding NA/null values|| 【参数】| ———-| method : {‘pearson’, ‘kendall’, ‘spearman’}| pearson : standard correlation coefficient| kendall : Kendall Tau correlation coefficient| spearman : Spearman rank correlation| min_periods : int, optional| Minimum number of observations required per pair of columns| to have a valid result. Currently only available for pearson| and spearman correlation65|| 【返回值】| ——-| y : DataFrame|| corrwith(self, other, axis=0, drop=False)| Compute pairwise correlation between rows or columns of two DataFrame| objects.|| 【参数】| ———-| other : DataFrame| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| 0 or ‘index’ to compute column-wise, 1 or ‘columns’ for row-wise| drop : boolean, default False| Drop missing indices from result, default returns union of all|| 【返回值】| ——-| correls : Series|| count(self, axis=0, level=None, numeric_only=False)| Return Series with number of non-NA/null observations over requested| axis. Works with non-floating point data as well (detects NaN and None)|| 【参数】| ———-| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a DataFrame| numeric_only : boolean, default False| Include only float, int, boolean data|| 【返回值】| ——-| count : Series (or DataFrame if level specified)|| cov(self, min_periods=None)| Compute pairwise covariance of columns, excluding NA/null values|| 【参数】| ———-| min_periods : int, optional| Minimum number of observations required per pair of columns| to have a valid result.|| 【返回值】| ——-| y : DataFrame|| 【注意】| —–| y contains the covariance matrix of the DataFrame’s time series.| The covariance is normalized by N-1 (unbiased estimator).66|| cummax = max(self, axis=None, dtype=None, out=None, skipna=True, kwargs)| Return cumulative max over requested axis.|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA|| 【返回值】| ——-| max : Series|| cummin = min(self, axis=None, dtype=None, out=None, skipna=True, kwargs)| Return cumulative min over requested axis.|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA|| 【返回值】| ——-| min : Series|| cumprod = prod(self, axis=None, dtype=None, out=None, skipna=True, kwargs)| Return cumulative prod over requested axis.|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA|| 【返回值】| ——-| prod : Series|| cumsum = sum(self, axis=None, dtype=None, out=None, skipna=True, kwargs)| Return cumulative sum over requested axis.|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA|| 【返回值】| ——-| sum : Series67|| diff(self, periods=1, axis=0)| 1st discrete difference of object|| 【参数】| ———-| periods : int, default 1| Periods to shift for forming difference| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| Take difference over rows (0) or columns (1).|| .. versionadded: 0.16.1|| 【返回值】| ——-| diffed : DataFrame|| div = truediv(self, other, axis=’columns’, level=None, fill_value=None)|| divide = truediv(self, other, axis=’columns’, level=None, fill_value=None)|| dot(self, other)| Matrix multiplication with DataFrame or Series objects|| 【参数】| ———-| other : DataFrame or Series|| 【返回值】| ——-| dot_product : DataFrame or Series|| drop_duplicates(self, subset=None, keep=’first’, inplace=False)| Return DataFrame with duplicate rows removed, optionally only| considering certain columns|| 【参数】| ———-| subset : column label or sequence of labels, optional| Only consider certain columns for identifying duplicates, by| default use all of the columns| keep : {‘first’, ‘last’, False}, default ‘first’| - first : Drop duplicates except for the first occurrence.| - last : Drop duplicates except for the last occurrence.| - False : Drop all duplicates.| take_last : deprecated| inplace : boolean, default False| Whether to drop duplicates in place or to return a copy| cols : kwargs only argument of subset [deprecated]|| 【返回值】| ——-| deduplicated : DataFrame|| dropna(self, axis=0, how=’any’, thresh=None, subset=None, inplace=False)| Return object with labels on given axis omitted where alternately any| or all of the data are missing68|| 【参数】| ———-| axis : {0 or ‘index’, 1 or ‘columns’}, or tuple/list thereof| Pass tuple or list to drop on multiple axes| how : {‘any’, ‘all’}| any : if any NA values are present, drop that label| all : if all values are NA, drop that label| thresh : int, default None| int value : require that many non-NA values| subset : array-like| Labels along other axis to consider, e.g. if you are dropping rows| these would be a list of columns to include| inplace : boolean, default False| If True, do operation inplace and return None.|| 【返回值】| ——-| dropped : DataFrame|| duplicated(self, subset=None, keep=’first’)| Return boolean Series denoting duplicate rows, optionally only| considering certain columns|| 【参数】| ———-| subset : column label or sequence of labels, optional| Only consider certain columns for identifying duplicates, by| default use all of the columns| keep : {‘first’, ‘last’, False}, default ‘first’| - first : Mark duplicates as True except for the| first occurrence.| - last : Mark duplicates as True except for the| last occurrence.| - False : Mark all duplicates as True.| take_last : deprecated| cols : kwargs only argument of subset [deprecated]|| 【返回值】| ——-| duplicated : Series|| eq(self, other, axis=’columns’, level=None)| Wrapper for flexible comparison methods eq|| eval(self, expr, kwargs)| Evaluate an expression in the context of the calling DataFrame| instance.|| 【参数】| ———-| expr : string| The expression string to evaluate.| kwargs : dict| See the documentation for :func:~pandas.eval for complete details| on the keyword arguments accepted by| :meth:~pandas.DataFrame.query.69|| 【返回值】| ——-| ret : ndarray, scalar, or pandas object|| 【参见】| ——–| pandas.DataFrame.query| pandas.eval|| 【注意】| —–| For more details see the API documentation for :func:~pandas.eval.| For detailed examples see :ref:enhancing performance with eval | &lt;enhancingperf.eval&gt;.|| 【示例】| ——–| &gt;&gt;&gt; from numpy.random import randn| &gt;&gt;&gt; from pandas import DataFrame| &gt;&gt;&gt; df = DataFrame(randn(10, 2), columns=list(‘ab’))| &gt;&gt;&gt; df.eval(‘a + b’)| &gt;&gt;&gt; df.eval(‘c = a + b’)|| fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, kwargs)| Fill NA/NaN values using the specified method|| 【参数】| ———-| value : scalar, dict, Series, or DataFrame| Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of| values specifying which value to use for each index (for a Series) or| column (for a DataFrame). (values not in the dict/Series/DataFrame will not be| filled). This value cannot be a list.| method : {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None| Method to use for filling holes in reindexed Series| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use NEXT valid observation to fill gap| axis : {0, 1, ‘index’, ‘columns’}| inplace : boolean, default False| If True, fill in place. Note: this will modify any| other views on this object, (e.g. a no-copy slice for a column in a| DataFrame).| limit : int, default None| If method is specified, this is the maximum number of consecutive| NaN values to forward/backward fill. In other words, if there is| a gap with more than this number of consecutive NaNs, it will only| be partially filled. If method is not specified, this is the| maximum number of entries along the entire axis where NaNs will be| filled.| downcast : dict, default is None| a dict of item-&gt;dtype of what to downcast if possible,| or the string ‘infer’ which will try to downcast to an appropriate| equal type (e.g. float64 to int64 if possible)|| 【参见】| ——–70| reindex, asfreq|| 【返回值】| ——-| filled : DataFrame|| first_valid_index(self)| Return label for first non-NA/null value|| floordiv(self, other, axis=’columns’, level=None, fill_value=None)| Integer division of dataframe and other, element-wise (binary operator floordiv).|| Equivalent to dataframe // other, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.rfloordiv|| ge(self, other, axis=’columns’, level=None)| Wrapper for flexible comparison methods ge|| get_value(self, index, col, takeable=False)| Quickly retrieve single value at passed column and index|| 【参数】| ———-| index : row label| col : column label| takeable : interpret the index/col as indexers, default False|| 【返回值】| ——-| value : scalar value|| gt(self, other, axis=’columns’, level=None)| Wrapper for flexible comparison methods gt71|| hist = hist_frame(data, column=None, by=None, grid=True, xlabelsize=None, xrot=None, ylabelsize=None, yrot=None,ax=None, sharex=False, sharey=False, figsize=None, layout=None, bins=10, kwds)| Draw histogram of the DataFrame’s series using matplotlib / pylab.|| 【参数】| ———-| data : DataFrame| column : string or sequence| If passed, will be used to limit data to a subset of columns| by : object, optional| If passed, then used to form histograms for separate groups| grid : boolean, default True| Whether to show axis grid lines| xlabelsize : int, default None| If specified changes the x-axis label size| xrot : float, default None| rotation of x axis labels| ylabelsize : int, default None| If specified changes the y-axis label size| yrot : float, default None| rotation of y axis labels| ax : matplotlib axes object, default None| sharex : boolean, default True if ax is None else False| In case subplots=True, share x axis and set some x axis labels to| invisible; defaults to True if ax is None otherwise False if an ax| is passed in; Be aware, that passing in both an ax and sharex=True| will alter all x axis labels for all subplots in a figure!| sharey : boolean, default False| In case subplots=True, share y axis and set some y axis labels to| invisible| figsize : tuple| The size of the figure to create in inches by default| layout: (optional) a tuple (rows, columns) for the layout of the histograms| bins: integer, default 10| Number of histogram bins to be used| kwds : other plotting keyword arguments| To be passed to hist function|| icol(self, i)| DEPRECATED. Use .iloc[:, i] instead|| idxmax(self, axis=0, skipna=True)| Return index of first occurrence of maximum over requested axis.| NA/null values are excluded.|| 【参数】| ———-| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be first index.|| 【返回值】| ——-| idxmax : Series72|| 【注意】| —–| This method is the DataFrame version of ndarray.argmax.|| 【参见】| ——–| Series.idxmax|| idxmin(self, axis=0, skipna=True)| Return index of first occurrence of minimum over requested axis.| NA/null values are excluded.|| 【参数】| ———-| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA|| 【返回值】| ——-| idxmin : Series|| 【注意】| —–| This method is the DataFrame version of ndarray.argmin.|| 【参见】| ——–| Series.idxmin|| iget_value(self, i, j)| DEPRECATED. Use .iat[i, j] instead|| info(self, verbose=None, buf=None, max_cols=None, memory_usage=None, null_counts=None)| Concise summary of a DataFrame.|| 【参数】| ———-| verbose : {None, True, False}, optional| Whether to print the full summary.| None follows the display.max_info_columns setting.| True or False overrides the display.max_info_columns setting.| buf : writable buffer, defaults to sys.stdout| max_cols : int, default None| Determines whether full summary or short summary is printed.| None follows the display.max_info_columns setting.| memory_usage : boolean/string, default None| Specifies whether total memory usage of the DataFrame| elements (including index) should be displayed. None follows| the display.memory_usage setting. True or False overrides| the display.memory_usage setting. A value of ‘deep’ is equivalent| of True, with deep introspection. Memory usage is shown in| human-readable units (base-2 representation).| null_counts : boolean, default None73| Whether to show the non-null counts| If None, then only show if the frame is smaller than max_info_rows and max_info_columns.| If True, always show counts.| If False, never show counts.|| insert(self, loc, column, value, allow_duplicates=False)| Insert column into DataFrame at specified location.|| If allow_duplicates is False, raises Exception if column| is already contained in the DataFrame.|| 【参数】| ———-| loc : int| Must have 0 &lt;= loc &lt;= len(columns)| column : object| value : int, Series, or array-like|| irow(self, i, copy=False)| DEPRECATED. Use .iloc[i] instead|| isin(self, values)| Return boolean DataFrame showing whether each element in the| DataFrame is contained in values.|| 【参数】| ———-| values : iterable, Series, DataFrame or dictionary| The result will only be true at a location if all the| labels match. If values is a Series, that’s the index. If| values is a dictionary, the keys must be the column names,| which must match. If values is a DataFrame,| then both the index and column labels must match.|| 【返回值】| ——-|| DataFrame of booleans|| 【示例】| ——–| When values is a list:|| &gt;&gt;&gt; df = DataFrame({‘A’: [1, 2, 3], ‘B’: [‘a’, ‘b’, ‘f’]})| &gt;&gt;&gt; df.isin([1, 3, 12, ‘a’])| A B| 0 True True| 1 False False| 2 True False|| When values is a dict:|| &gt;&gt;&gt; df = DataFrame({‘A’: [1, 2, 3], ‘B’: [1, 4, 7]})| &gt;&gt;&gt; df.isin({‘A’: [1, 3], ‘B’: [4, 7, 12]})| A B| 0 True False # Note that B didn’t match the 1 here.| 1 False True74| 2 True True|| When values is a Series or DataFrame:|| &gt;&gt;&gt; df = DataFrame({‘A’: [1, 2, 3], ‘B’: [‘a’, ‘b’, ‘f’]})| &gt;&gt;&gt; other = DataFrame({‘A’: [1, 3, 3, 2], ‘B’: [‘e’, ‘f’, ‘f’, ‘e’]})| &gt;&gt;&gt; df.isin(other)| A B| 0 True False| 1 False False # Column A in other has a 3, but not at index 1.| 2 True True|| items = iteritems(self)|| iteritems(self)| Iterator over (column name, Series) pairs.|| 【参见】| ——–| iterrows : Iterate over the rows of a DataFrame as (index, Series) pairs.| itertuples : Iterate over the rows of a DataFrame as namedtuples of the values.|| iterrows(self)| Iterate over the rows of a DataFrame as (index, Series) pairs.|| 【注意】| —–|| 1. Because iterrows returns a Series for each row,| it does not preserve dtypes across the rows (dtypes are| preserved across columns for DataFrames). For example,|| &gt;&gt;&gt; df = pd.DataFrame([[1, 1.5]], columns=[‘int’, ‘float’])| &gt;&gt;&gt; row = next(df.iterrows())[1]| &gt;&gt;&gt; row| int 1.0| float 1.5| Name: 0, dtype: float64| &gt;&gt;&gt; print(row[‘int’].dtype)| float64| &gt;&gt;&gt; print(df[‘int’].dtype)| int64|| To preserve dtypes while iterating over the rows, it is better| to use :meth:itertuples which returns namedtuples of the values| and which is generally faster as iterrows.|| 2. You should never modify** something you are iterating over.| This is not guaranteed to work in all cases. Depending on the| data types, the iterator returns a copy and not a view, and writing| to it will have no effect.|| 【返回值】| ——-| it : generator| A generator that iterates over the rows of the frame.|75| 【参见】| ——–| itertuples : Iterate over the rows of a DataFrame as namedtuples of the values.| iteritems : Iterate over (column name, Series) pairs.|| itertuples(self, index=True, name=’Pandas’)| Iterate over the rows of DataFrame as namedtuples, with index value| as first element of the tuple.|| 【参数】| ———-| index : boolean, default True| If True, return the index as the first element of the tuple.| name : string, default “Pandas”| The name of the returned namedtuples or None to return regular tuples.|| 【注意】| —–| The columns names will be renamed to positional names if they are| invalid Python identifiers, repeated, or start with an underscore.| With a large number of columns (&gt;255), regular tuples are returned.|| 【参见】| ——–| iterrows : Iterate over the rows of a DataFrame as (index, Series) pairs.| iteritems : Iterate over (column name, Series) pairs.|| 【示例】| ——–|| &gt;&gt;&gt; df = pd.DataFrame({‘col1’: [1, 2], ‘col2’: [0.1, 0.2]}, index=[‘a’, ‘b’])| &gt;&gt;&gt; df| col1 col2| a 1 0.1| b 2 0.2| &gt;&gt;&gt; for row in df.itertuples():| … print(row)| …| Pandas(Index=’a’, col1=1, col2=0.10000000000000001)| Pandas(Index=’b’, col1=2, col2=0.20000000000000001)|| join(self, other, on=None, how=’left’, lsuffix=’’, rsuffix=’’, sort=False)| Join columns with other DataFrame either on index or on a key| column. Efficiently Join multiple DataFrame objects by index at once by| passing a list.|| 【参数】| ———-| other : DataFrame, Series with name field set, or list of DataFrame| Index should be similar to one of the columns in this one. If a| Series is passed, its name attribute must be set, and that will be| used as the column name in the resulting joined DataFrame| on : column name, tuple/list of column names, or array-like| Column(s) to use for joining, otherwise join on index. If multiples| columns given, the passed DataFrame must have a MultiIndex. Can| pass an array as the join key if not already contained in the| calling DataFrame. Like an Excel VLOOKUP operation76| how : {‘left’, ‘right’, ‘outer’, ‘inner’}| How to handle indexes of the two objects. Default: ‘left’| for joining on index, None otherwise|| left: use calling frame’s index| right: use input frame’s index| outer: form union of indexes| inner: use intersection of indexes| lsuffix : string| Suffix to use from left frame’s overlapping columns| rsuffix : string| Suffix to use from right frame’s overlapping columns| sort : boolean, default False| Order result DataFrame lexicographically by the join key. If False,| preserves the index order of the calling (left) DataFrame|| 【注意】| —–| on, lsuffix, and rsuffix options are not supported when passing a list| of DataFrame objects|| 【返回值】| ——-| joined : DataFrame|| kurt(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return unbiased kurtosis over requested axis using Fishers definition of| kurtosis (kurtosis of normal == 0.0). Normalized by N-1||| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| kurt : Series or DataFrame (if level specified)|| kurtosis = kurt(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)|| last_valid_index(self)| Return label for last non-NA/null value|| le(self, other, axis=’columns’, level=None)| Wrapper for flexible comparison methods le|| lookup(self, row_labels, col_labels)| Label-based “fancy indexing” function for DataFrame.77| Given equal-length arrays of row and column labels, return an| array of the values corresponding to each (row, col) pair.|| 【参数】| ———-| row_labels : sequence| The row labels to use for lookup| col_labels : sequence| The column labels to use for lookup|| 【注意】| —–| Akin to::|| result = []| for row, col in zip(row_labels, col_labels):| result.append(df.get_value(row, col))|| 【示例】| ——–| values : ndarray| The found values|| lt(self, other, axis=’columns’, level=None)| Wrapper for flexible comparison methods lt|| mad(self, axis=None, skipna=None, level=None)| Return the mean absolute deviation of the values for the requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| mad : Series or DataFrame (if level specified)|| max(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)| This method returns the maximum of the values in the object. If you| want the index of the maximum, use idxmax. This is the| equivalent of the numpy.ndarray method argmax.|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA78| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| max : Series or DataFrame (if level specified)|| mean(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return the mean of the values for the requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| mean : Series or DataFrame (if level specified)|| median(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return the median of the values for the requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| median : Series or DataFrame (if level specified)|| memory_usage(self, index=False, deep=False)| Memory usage of DataFrame columns.|| 【参数】| ———-| index : bool79| Specifies whether to include memory usage of DataFrame’s| index in returned Series. If index=True (default is False)| the first index of the Series is Index.| deep : bool| Introspect the data deeply, interrogate| object dtypes for system-level memory consumption|| 【返回值】| ——-| sizes : Series| A series with column names as index and memory usage of| columns with units of bytes.|| 【注意】| —–| Memory usage does not include memory consumed by elements that| are not components of the array if deep=False|| 【参见】| ——–| numpy.ndarray.nbytes|| merge(self, right, how=’inner’, on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False,suffixes=(‘_x’, ‘_y’), copy=True, indicator=False)| Merge DataFrame objects by performing a database-style join operation by| columns or indexes.|| If joining columns on columns, the DataFrame indexes will be| ignored. Otherwise if joining indexes on indexes or indexes on a column or| columns, the index will be passed on.|| 【参数】| ———-| right : DataFrame| how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’| left: use only keys from left frame (SQL: left outer join)| right: use only keys from right frame (SQL: right outer join)| outer: use union of keys from both frames (SQL: full outer join)| inner: use intersection of keys from both frames (SQL: inner join)| on : label or list| Field names to join on. Must be found in both DataFrames. If on is| None and not merging on indexes, then it merges on the intersection of| the columns by default.| left_on : label or list, or array-like| Field names to join on in left DataFrame. Can be a vector or list of| vectors of the length of the DataFrame to use a particular vector as| the join key instead of columns| right_on : label or list, or array-like| Field names to join on in right DataFrame or vector/list of vectors per| left_on docs| left_index : boolean, default False| Use the index from the left DataFrame as the join key(s). If it is a| MultiIndex, the number of keys in the other DataFrame (either the index| or a number of columns) must match the number of levels| right_index : boolean, default False| Use the index from the right DataFrame as the join key. Same caveats as| left_index80| sort : boolean, default False| Sort the join keys lexicographically in the result DataFrame| suffixes : 2-length sequence (tuple, list, …)| Suffix to apply to overlapping column names in the left and right| side, respectively| copy : boolean, default True| If False, do not copy data unnecessarily| indicator : boolean or string, default False| If True, adds a column to output DataFrame called “_merge” with| information on the source of each row.| If string, column with information on source of each row will be added to| output DataFrame, and column will be named value of string.| Information column is Categorical-type and takes on a value of “left_only”| for observations whose merge key only appears in ‘left’ DataFrame,| “right_only” for observations whose merge key only appears in ‘right’| DataFrame, and “both” if the observation’s merge key is found in both.|| .. versionadded:: 0.17.0|| 【示例】| ——–|| &gt;&gt;&gt; A &gt;&gt;&gt; B| lkey value rkey value| 0 foo 1 0 foo 5| 1 bar 2 1 bar 6| 2 baz 3 2 qux 7| 3 foo 4 3 bar 8|| &gt;&gt;&gt; merge(A, B, left_on=’lkey’, right_on=’rkey’, how=’outer’)| lkey value_x rkey value_y| 0 foo 1 foo 5| 1 foo 4 foo 5| 2 bar 2 bar 6| 3 bar 2 bar 8| 4 baz 3 NaN NaN| 5 NaN NaN qux 7|| 【返回值】| ——-| merged : DataFrame| The output type will the be same as ‘left’, if it is a subclass| of DataFrame.|| min(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)| This method returns the minimum of the values in the object. If you| want the index of the minimum, use idxmin. This is the| equivalent of the numpy.ndarray method argmin.|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a81| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| min : Series or DataFrame (if level specified)|| mod(self, other, axis=’columns’, level=None, fill_value=None)| Modulo of dataframe and other, element-wise (binary operator mod).|| Equivalent to dataframe % other, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.rmod|| mode(self, axis=0, numeric_only=False)| Gets the mode(s) of each element along the axis selected. Empty if nothing| has 2+ occurrences. Adds a row for each mode per label, fills in gaps| with nan.|| Note that there could be multiple values returned for the selected| axis (when more than one item share the maximum frequency), which is the| reason why a dataframe is returned. If you want to impute missing values| with the mode in a dataframe df, you can just do this:| df.fillna(df.mode().iloc[0])|| 【参数】| ———-| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| 0 or ‘index’ : get mode of each column| 1 or ‘columns’ : get mode of each row| numeric_only : boolean, default False| if True, only apply to numeric columns82|| 【返回值】| ——-| modes : DataFrame (sorted)|| 【示例】| ——–| &gt;&gt;&gt; df = pd.DataFrame({‘A’: [1, 2, 1, 2, 1, 2, 3]})| &gt;&gt;&gt; df.mode()| A| 0 1| 1 2|| mul(self, other, axis=’columns’, level=None, fill_value=None)| Multiplication of dataframe and other, element-wise (binary operator mul).|| Equivalent to ``dataframe other, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other : Series, DataFrame, or constant | axis : {0, 1, &#39;index&#39;, &#39;columns&#39;} | For Series input, axis to match Series index on | fill_value : None or float value, default None | Fill missing (NaN) values with this value. If both DataFrame locations are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【注意】 | -----| Mismatched indices will be unioned together | | 【返回值】 | -------| result : DataFrame | | 【参见】 | --------| DataFrame.rmul | | multiply = mul(self, other, axis=&#39;columns&#39;, level=None, fill_value=None) | | ne(self, other, axis=&#39;columns&#39;, level=None) | Wrapper for flexible comparison methods ne | | nlargest(self, n, columns, keep=&#39;first&#39;) | Get the rows of a DataFrame sorted by the `n` largest | values of `columns`. | | .. versionadded:: 0.17.0 | | 【参数】 | ----------| n : int 83 | Number of items to retrieve | columns : list or str | Column name or names to order by | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | Where there are duplicate values: | -first: take the first occurrence. | -last: take the last occurrence. | | 【返回值】 | -------| DataFrame | | 【示例】 | --------| &gt;&gt;&gt; df = DataFrame({&#39;a&#39;: [1, 10, 8, 11, -1], | ... &#39;b&#39;: list(&#39;abdce&#39;), | ... &#39;c&#39;: [1.0, 2.0, np.nan, 3.0, 4.0]}) | &gt;&gt;&gt; df.nlargest(3, &#39;a&#39;) | a b c | 3 11 c 3 | 1 10 b 2 | 2 8 d NaN | | nsmallest(self, n, columns, keep=&#39;first&#39;) | Get the rows of a DataFrame sorted by the `n` smallest | values of `columns`. | | .. versionadded:: 0.17.0 | | 【参数】 | ----------| n : int | Number of items to retrieve | columns : list or str | Column name or names to order by | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | Where there are duplicate values: | -first: take the first occurrence. | -last: take the last occurrence. | | 【返回值】 | -------| DataFrame | | 【示例】 | --------| &gt;&gt;&gt; df = DataFrame({&#39;a&#39;: [1, 10, 8, 11, -1], | ... &#39;b&#39;: list(&#39;abdce&#39;), | ... &#39;c&#39;: [1.0, 2.0, np.nan, 3.0, 4.0]}) | &gt;&gt;&gt; df.nsmallest(3, &#39;a&#39;) | a b c | 4 -1 e 4 | 0 1 a 1 | 2 8 d NaN | | pivot(self, index=None, columns=None, values=None) | Reshape data (produce a &quot;pivot&quot; table) based on column values. Uses 84 | unique values from index / columns to form axes and return either | DataFrame or Panel, depending on whether you request a single value | column (DataFrame) or all columns (Panel) | | 【参数】 | ----------| index : string or object, optional | Column name to use to make new frame&#39;s index. If None, uses | existing index. | columns : string or object | Column name to use to make new frame&#39;s columns | values : string or object, optional | Column name to use for populating new frame&#39;s values | | 【注意】 | -----| For finer-tuned control, see hierarchical indexing documentation along | with the related stack/unstack methods | | 【示例】 | --------| &gt;&gt;&gt; df | foo bar baz | 0 one A 1. | 1 one B 2. | 2 one C 3. | 3 two A 4. | 4 two B 5. | 5 two C 6. | | &gt;&gt;&gt; df.pivot(&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;) | A B C | one 1 2 3 | two 4 5 6 | | &gt;&gt;&gt; df.pivot(&#39;foo&#39;, &#39;bar&#39;)[&#39;baz&#39;] | A B C | one 1 2 3 | two 4 5 6 | | 【返回值】 | -------| pivoted : DataFrame | If no values column specified, will have hierarchically indexed | columns | | pivot_table(data, values=None, index=None, columns=None, aggfunc=&#39;mean&#39;, fill_value=None, margins=False, dropna=True, margins_name=&#39;All&#39;) | Create a spreadsheet-style pivot table as a DataFrame. The levels in the | pivot table will be stored in MultiIndex objects (hierarchical indexes) on | the index and columns of the result DataFrame | | 【参数】 | ----------| data : DataFrame | values : column to aggregate, optional | index : a column, Grouper, array which has the same length as data, or list of them. 85 | Keys to group by on the pivot table index. | If an array is passed, it is being used as the same manner as column values. | columns : a column, Grouper, array which has the same length as data, or list of them. | Keys to group by on the pivot table column. | If an array is passed, it is being used as the same manner as column values. | aggfunc : function, default numpy.mean, or list of functions | If list of functions passed, the resulting pivot table will have | hierarchical columns whose top level are the function names (inferred | from the function objects themselves) | fill_value : scalar, default None | Value to replace missing values with | margins : boolean, default False | Add all row / columns (e.g. for subtotal / grand totals) | dropna : boolean, default True | Do not include columns whose entries are all NaN | margins_name : string, default &#39;All&#39; | Name of the row / column that will contain the totals | when margins is True. | | 【示例】 | --------| &gt;&gt;&gt; df | A B C D | 0 foo one small 1 | 1 foo one large 2 | 2 foo one large 2 | 3 foo two small 3 | 4 foo two small 3 | 5 bar one large 4 | 6 bar one small 5 | 7 bar two small 6 | 8 bar two large 7 | | &gt;&gt;&gt; table = pivot_table(df, values=&#39;D&#39;, index=[&#39;A&#39;, &#39;B&#39;], | ... columns=[&#39;C&#39;], aggfunc=np.sum) | &gt;&gt;&gt; table | small large | foo one 1 4 | two 6 NaN | bar one 5 4 | two 6 7 | | 【返回值】 | -------| table : DataFrame | | pow(self, other, axis=&#39;columns&#39;, level=None, fill_value=None) | Exponential power of dataframe and other, element-wise (binary operator `pow`). | | Equivalent todataframe other``, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on86| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.rpow|| prod(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return the product of the values for the requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| prod : Series or DataFrame (if level specified)|| product = prod(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)|| quantile(self, q=0.5, axis=0, numeric_only=True)| Return values at the given quantile over requested axis, a la| numpy.percentile.|| 【参数】| ———-| q : float or array-like, default 0.5 (50% quantile)| 0 &lt;= q &lt;= 1, the quantile(s) to compute| axis : {0, 1, ‘index’, ‘columns’} (default 0)| 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise||| 【返回值】| ——-| quantiles : Series or DataFrame| If q is an array, a DataFrame will be returned where the87| index is q, the columns are the columns of self, and the| values are the quantiles.| If q is a float, a Series will be returned where the| index is the columns of self and the values are the quantiles.|| 【示例】| ——–|| &gt;&gt;&gt; df = DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),| columns=[‘a’, ‘b’])| &gt;&gt;&gt; df.quantile(.1)| a 1.3| b 3.7| dtype: float64| &gt;&gt;&gt; df.quantile([.1, .5])| a b| 0.1 1.3 3.7| 0.5 2.5 55.0|| query(self, expr, kwargs)| Query the columns of a frame with a boolean expression.|| .. versionadded:: 0.13|| 【参数】| ———-| expr : string| The query string to evaluate. You can refer to variables| in the environment by prefixing them with an ‘@’ character like| @a + b.| kwargs : dict| See the documentation for :func:pandas.eval for complete details| on the keyword arguments accepted by :meth:DataFrame.query.|| 【返回值】| ——-| q : DataFrame|| 【注意】| —–| The result of the evaluation of this expression is first passed to| :attr:DataFrame.loc and if that fails because of a| multidimensional key (e.g., a DataFrame) then the result will be passed| to :meth:DataFrame.__getitem__.|| This method uses the top-level :func:pandas.eval function to| evaluate the passed query.|| The :meth:~pandas.DataFrame.query method uses a slightly| modified Python syntax by default. For example, the &amp; and || (bitwise) operators have the precedence of their boolean cousins,| :keyword:and and :keyword:or. This is syntactically valid Python,| however the semantics are different.|| You can change the semantics of the expression by passing the keyword| argument parser=&#39;python&#39;. This enforces the same semantics as| evaluation in Python space. Likewise, you can pass engine=&#39;python&#39;88| to evaluate an expression using Python itself as a backend. This is not| recommended as it is inefficient compared to using numexpr as the| engine.|| The :attr:DataFrame.index and| :attr:DataFrame.columns attributes of the| :class:~pandas.DataFrame instance are placed in the query namespace| by default, which allows you to treat both the index and columns of the| frame as a column in the frame.| The identifier index is used for the frame index; you can also| use the name of the index to identify it in a query.|| For further details and examples see the query documentation in| :ref:indexing &lt;indexing.query&gt;.|| 【参见】| ——–| pandas.eval| DataFrame.eval|| 【示例】| ——–| &gt;&gt;&gt; from numpy.random import randn| &gt;&gt;&gt; from pandas import DataFrame| &gt;&gt;&gt; df = DataFrame(randn(10, 2), columns=list(‘ab’))| &gt;&gt;&gt; df.query(‘a &gt; b’)| &gt;&gt;&gt; df[df.a &gt; df.b] # same result as the previous expression|| radd(self, other, axis=’columns’, level=None, fill_value=None)| Addition of dataframe and other, element-wise (binary operator radd).|| Equivalent to other + dataframe, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.add89|| rank(self, axis=0, numeric_only=None, method=’average’, na_option=’keep’, ascending=True, pct=False)| Compute numerical data ranks (1 through n) along axis. Equal values are| assigned a rank that is the average of the ranks of those values|| 【参数】| ———-| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| Ranks over columns (0) or rows (1)| numeric_only : boolean, default None| Include only float, int, boolean data| method : {‘average’, ‘min’, ‘max’, ‘first’, ‘dense’}| average: average rank of group| min: lowest rank in group| max: highest rank in group| first: ranks assigned in order they appear in the array| dense: like ‘min’, but rank always increases by 1 between groups| na_option : {‘keep’, ‘top’, ‘bottom’}| keep: leave NA values where they are| top: smallest rank if ascending| bottom: smallest rank if descending| ascending : boolean, default True| False for ranks by high (1) to low (N)| pct : boolean, default False| Computes percentage rank of data|| 【返回值】| ——-| ranks : DataFrame|| rdiv = rtruediv(self, other, axis=’columns’, level=None, fill_value=None)|| reindex(self, index=None, columns=None, kwargs)| Conform DataFrame to new index with optional filling logic, placing| NA/NaN in locations having no value in the previous index. A new object| is produced unless the new index is equivalent to the current one and| copy=False|| 【参数】| ———-| index, columns : array-like, optional (can be specified in order, or as| keywords)| New labels / index to conform to. Preferably an Index object to| avoid duplicating data| method : {None, ‘backfill’/‘bfill’, ‘pad’/‘ffill’, ‘nearest’}, optional| method to use for filling holes in reindexed DataFrame.| Please note: this is only applicable to DataFrames/Series with a| monotonically increasing/decreasing index.| default: don’t fill gaps| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use next valid observation to fill gap| nearest: use nearest valid observations to fill gap| copy : boolean, default True| Return a new object, even if the passed indexes are the same| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level90| fill_value : scalar, default np.NaN| Value to use for missing values. Defaults to NaN, but can be any| “compatible” value| limit : int, default None| Maximum number of consecutive elements to forward or backward fill| tolerance : optional| Maximum distance between original and new labels for inexact| matches. The values of the index at the matching locations most| satisfy the equation abs(index[indexer] - target) &lt;= tolerance.|| .. versionadded:: 0.17.0|| 【示例】| ——–|| Create a dataframe with some fictional data.|| &gt;&gt;&gt; index = [‘Firefox’, ‘Chrome’, ‘Safari’, ‘IE10’, ‘Konqueror’]| &gt;&gt;&gt; df = pd.DataFrame({| … ‘http_status’: [200,200,404,404,301],| … ‘response_time’: [0.04, 0.02, 0.07, 0.08, 1.0]},| … index=index)| &gt;&gt;&gt; df| http_status response_time| Firefox 200 0.04| Chrome 200 0.02| Safari 404 0.07| IE10 404 0.08| Konqueror 301 1.00|| Create a new index and reindex the dataframe. By default| values in the new index that do not have corresponding| records in the dataframe are assigned NaN.|| &gt;&gt;&gt; new_index= [‘Safari’, ‘Iceweasel’, ‘Comodo Dragon’, ‘IE10’,| … ‘Chrome’]| &gt;&gt;&gt; df.reindex(new_index)| http_status response_time| Safari 404 0.07| Iceweasel NaN NaN| Comodo Dragon NaN NaN| IE10 404 0.08| Chrome 200 0.02|| We can fill in the missing values by passing a value to| the keyword fill_value. Because the index is not monotonically| increasing or decreasing, we cannot use arguments to the keyword| method to fill the NaN values.|| &gt;&gt;&gt; df.reindex(new_index, fill_value=0)| http_status response_time| Safari 404 0.07| Iceweasel 0 0.00| Comodo Dragon 0 0.00| IE10 404 0.08| Chrome 200 0.02|91| &gt;&gt;&gt; df.reindex(new_index, fill_value=’missing’)| http_status response_time| Safari 404 0.07| Iceweasel missing missing| Comodo Dragon missing missing| IE10 404 0.08| Chrome 200 0.02|| To further illustrate the filling functionality in| reindex, we will create a dataframe with a| monotonically increasing index (for example, a sequence| of dates).|| &gt;&gt;&gt; date_index = pd.date_range(‘1/1/2010’, periods=6, freq=’D’)| &gt;&gt;&gt; df2 = pd.DataFrame({“prices”: [100, 101, np.nan, 100, 89, 88]},| index=date_index)| &gt;&gt;&gt; df2| prices| 2010-01-01 100| 2010-01-02 101| 2010-01-03 NaN| 2010-01-04 100| 2010-01-05 89| 2010-01-06 88|| Suppose we decide to expand the dataframe to cover a wider| date range.|| &gt;&gt;&gt; date_index2 = pd.date_range(‘12/29/2009’, periods=10, freq=’D’)| &gt;&gt;&gt; df2.reindex(date_index2)| prices| 2009-12-29 NaN| 2009-12-30 NaN| 2009-12-31 NaN| 2010-01-01 100| 2010-01-02 101| 2010-01-03 NaN| 2010-01-04 100| 2010-01-05 89| 2010-01-06 88| 2010-01-07 NaN|| The index entries that did not have a value in the original data frame| (for example, ‘2009-12-29’) are by default filled with NaN.| If desired, we can fill in the missing values using one of several| options.|| For example, to backpropagate the last valid value to fill the NaN| values, pass bfill as an argument to the method keyword.|| &gt;&gt;&gt; df2.reindex(date_index2, method=’bfill’)| prices| 2009-12-29 100| 2009-12-30 100| 2009-12-31 100| 2010-01-01 100| 2010-01-02 10192| 2010-01-03 NaN| 2010-01-04 100| 2010-01-05 89| 2010-01-06 88| 2010-01-07 NaN|| Please note that the NaN value present in the original dataframe| (at index value 2010-01-03) will not be filled by any of the| value propagation schemes. This is because filling while reindexing| does not look at dataframe values, but only compares the original and| desired indexes. If you do want to fill in the NaN values present| in the original dataframe, use the fillna() method.|| 【返回值】| ——-| reindexed : DataFrame|| reindex_axis(self, labels, axis=0, method=None, level=None, copy=True, limit=None, fill_value=nan)| Conform input object to new index with optional filling logic,| placing NA/NaN in locations having no value in the previous index. A| new object is produced unless the new index is equivalent to the| current one and copy=False|| 【参数】| ———-| labels : array-like| New labels / index to conform to. Preferably an Index object to| avoid duplicating data| axis : {0, 1, ‘index’, ‘columns’}| method : {None, ‘backfill’/‘bfill’, ‘pad’/‘ffill’, ‘nearest’}, optional| Method to use for filling holes in reindexed DataFrame:| default: don’t fill gaps| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use next valid observation to fill gap| nearest: use nearest valid observations to fill gap| copy : boolean, default True| Return a new object, even if the passed indexes are the same| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level| limit : int, default None| Maximum number of consecutive elements to forward or backward fill| tolerance : optional| Maximum distance between original and new labels for inexact| matches. The values of the index at the matching locations most| satisfy the equation abs(index[indexer] - target) &lt;= tolerance.|| .. versionadded:: 0.17.0|| 【示例】| ——–| &gt;&gt;&gt; df.reindex_axis([‘A’, ‘B’, ‘C’], axis=1)|| 【参见】| ——–| reindex, reindex_like|93| 【返回值】| ——-| reindexed : DataFrame|| rename(self, index=None, columns=None, kwargs)| Alter axes input function or functions. Function / dict values must be| unique (1-to-1). Labels not contained in a dict / Series will be left| as-is.|| 【参数】| ———-| index, columns : dict-like or function, optional| Transformation to apply to that axis values|| copy : boolean, default True| Also copy underlying data| inplace : boolean, default False| Whether to return a new DataFrame. If True then value of copy is| ignored.|| 【返回值】| ——-| renamed : DataFrame (new object)|| reorder_levels(self, order, axis=0)| Rearrange index levels using input order.| May not drop or duplicate levels|| 【参数】| ———-| order : list of int or list of str| List representing new level order. Reference level by number| (position) or by key (label).| axis : int| Where to reorder levels.|| 【返回值】| ——-| type of caller (new object)|| reset_index(self, level=None, drop=False, inplace=False, col_level=0, col_fill=’’)| For DataFrame with multi-level index, return new DataFrame with| labeling information in the columns under the index names, defaulting| to ‘level_0’, ‘level_1’, etc. if any are None. For a standard index,| the index name will be used (if set), otherwise a default ‘index’ or| ‘level_0’ (if ‘index’ is already taken) will be used.|| 【参数】| ———-| level : int, str, tuple, or list, default None| Only remove the given levels from the index. Removes all levels by| default| drop : boolean, default False| Do not try to insert index into dataframe columns. This resets| the index to the default integer index.| inplace : boolean, default False| Modify the DataFrame in place (do not create a new object)94| col_level : int or str, default 0| If the columns have multiple levels, determines which level the| labels are inserted into. By default it is inserted into the first| level.| col_fill : object, default ‘’| If the columns have multiple levels, determines how the other| levels are named. If None then the index name is repeated.|| 【返回值】| ——-| resetted : DataFrame|| rfloordiv(self, other, axis=’columns’, level=None, fill_value=None)| Integer division of dataframe and other, element-wise (binary operator rfloordiv).|| Equivalent to other // dataframe, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.floordiv|| rmod(self, other, axis=’columns’, level=None, fill_value=None)| Modulo of dataframe and other, element-wise (binary operator rmod).|| Equivalent to other % dataframe, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name95| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.mod|| rmul(self, other, axis=’columns’, level=None, fill_value=None)| Multiplication of dataframe and other, element-wise (binary operator rmul).|| Equivalent to other * dataframe, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.mul|| round(self, decimals=0, out=None)| Round a DataFrame to a variable number of decimal places.|| .. versionadded:: 0.17.0|| 【参数】| ———-| decimals : int, dict, Series| Number of decimal places to round each column to. If an int is| given, round each column to the same number of places.| Otherwise dict and Series round to variable numbers of places.96| Column names should be in the keys if decimals is a| dict-like, or in the index if decimals is a Series. Any| columns not included in decimals will be left as is. Elements| of decimals which are not columns of the input will be| ignored.|| 【示例】| ——–| &gt;&gt;&gt; df = pd.DataFrame(np.random.random([3, 3]),| … columns=[‘A’, ‘B’, ‘C’], index=[‘first’, ‘second’, ‘third’])| &gt;&gt;&gt; df| A B C| first 0.028208 0.992815 0.173891| second 0.038683 0.645646 0.577595| third 0.877076 0.149370 0.491027| &gt;&gt;&gt; df.round(2)| A B C| first 0.03 0.99 0.17| second 0.04 0.65 0.58| third 0.88 0.15 0.49| &gt;&gt;&gt; df.round({‘A’: 1, ‘C’: 2})| A B C| first 0.0 0.992815 0.17| second 0.0 0.645646 0.58| third 0.9 0.149370 0.49| &gt;&gt;&gt; decimals = pd.Series([1, 0, 2], index=[‘A’, ‘B’, ‘C’])| &gt;&gt;&gt; df.round(decimals)| A B C| first 0.0 1 0.17| second 0.0 1 0.58| third 0.9 0 0.49|| 【返回值】| ——-| DataFrame object|| rpow(self, other, axis=’columns’, level=None, fill_value=None)| Exponential power of dataframe and other, element-wise (binary operator rpow).|| Equivalent to other ** dataframe, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together97|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.pow|| rsub(self, other, axis=’columns’, level=None, fill_value=None)| Subtraction of dataframe and other, element-wise (binary operator rsub).|| Equivalent to other - dataframe, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.sub|| rtruediv(self, other, axis=’columns’, level=None, fill_value=None)| Floating division of dataframe and other, element-wise (binary operator rtruediv).|| Equivalent to other / dataframe, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|98| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.truediv|| select_dtypes(self, include=None, exclude=None)| Return a subset of a DataFrame including/excluding columns based on| their dtype.|| 【参数】| ———-| include, exclude : list-like| A list of dtypes or strings to be included/excluded. You must pass| in a non-empty sequence for at least one of these.|| 【Raises 引发错误】| ——| ValueError| If both of include and exclude are empty| If include and exclude have overlapping elements| If any kind of string dtype is passed in.| TypeError| If either of include or exclude is not a sequence|| 【返回值】| ——-| subset : DataFrame| The subset of the frame including the dtypes in include and| excluding the dtypes in exclude.|| 【注意】| —–| To select all numeric types use the numpy dtype numpy.number| To select strings you must use the object dtype, but note that| this will return all object dtype columns| See the numpy dtype hierarchy | &lt;http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html&gt;__| To select Pandas categorical dtypes, use ‘category’|| 【示例】| ——–| &gt;&gt;&gt; df = pd.DataFrame({‘a’: np.random.randn(6).astype(‘f4’),| … ‘b’: [True, False] 3,| … ‘c’: [1.0, 2.0] 3})| &gt;&gt;&gt; df| a b c| 0 0.3962 True 1| 1 0.1459 False 2| 2 0.2623 True 199| 3 0.0764 False 2| 4 -0.9703 True 1| 5 -1.2094 False 2| &gt;&gt;&gt; df.select_dtypes(include=[‘float64’])| c| 0 1| 1 2| 2 1| 3 2| 4 1| 5 2| &gt;&gt;&gt; df.select_dtypes(exclude=[‘floating’])| b| 0 True| 1 False| 2 True| 3 False| 4 True| 5 False|| sem(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, kwargs)| Return unbiased standard error of the mean over requested axis.|| Normalized by N-1 by default. This can be changed using the ddof argument|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| ddof : int, default 1| degrees of freedom| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| sem : Series or DataFrame (if level specified)|| set_index(self, keys, drop=True, append=False, inplace=False, verify_integrity=False)| Set the DataFrame index (row labels) using one or more existing| columns. By default yields a new object.|| 【参数】| ———-| keys : column label or list of column labels / arrays| drop : boolean, default True| Delete columns to be used as the new index| append : boolean, default False| Whether to append columns to existing index| inplace : boolean, default False| Modify the DataFrame in place (do not create a new object)100| verify_integrity : boolean, default False| Check the new index for duplicates. Otherwise defer the check until| necessary. Setting to False will improve the performance of this| method|| 【示例】| ——–| &gt;&gt;&gt; indexed_df = df.set_index([‘A’, ‘B’])| &gt;&gt;&gt; indexed_df2 = df.set_index([‘A’, [0, 1, 2, 0, 1, 2]])| &gt;&gt;&gt; indexed_df3 = df.set_index([[0, 1, 2, 0, 1, 2]])|| 【返回值】| ——-| dataframe : DataFrame|| set_value(self, index, col, value, takeable=False)| Put single value at passed column and index|| 【参数】| ———-| index : row label| col : column label| value : scalar value| takeable : interpret the index/col as indexers, default False|| 【返回值】| ——-| frame : DataFrame| If label pair is contained, will be reference to calling DataFrame,| otherwise a new object|| shift(self, periods=1, freq=None, axis=0)| Shift index by desired number of periods with an optional time freq|| 【参数】| ———-| periods : int| Number of periods to move, can be positive or negative| freq : DateOffset, timedelta, or time rule string, optional| Increment to use from datetools module or time rule (e.g. ‘EOM’).| See Notes.| axis : {0, 1, ‘index’, ‘columns’}|| 【注意】| —–| If freq is specified then the index values are shifted but the data| is not realigned. That is, use freq if you would like to extend the| index when shifting and preserve the original data.|| 【返回值】| ——-| shifted : DataFrame|| skew(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return unbiased skew over requested axis| Normalized by N-1|101| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| skew : Series or DataFrame (if level specified)|| sort(self, columns=None, axis=0, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’)| DEPRECATED: use :meth:DataFrame.sort_values|| Sort DataFrame either by labels (along either axis) or by the values in| column(s)|| 【参数】| ———-| columns : object| Column name(s) in frame. Accepts a column name or a list| for a nested sort. A tuple will be interpreted as the| levels of a multi-index.| ascending : boolean or list, default True| Sort ascending vs. descending. Specify list for multiple sort| orders| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| Sort index/rows versus columns| inplace : boolean, default False| Sort the DataFrame without creating a new instance| kind : {‘quicksort’, ‘mergesort’, ‘heapsort’}, optional| This option is only applied when sorting on a single column or label.| na_position : {‘first’, ‘last’} (optional, default=’last’)| ‘first’ puts NaNs at the beginning| ‘last’ puts NaNs at the end|| 【示例】| ——–| &gt;&gt;&gt; result = df.sort([‘A’, ‘B’], ascending=[1, 0])|| 【返回值】| ——-| sorted : DataFrame|| sort_index(self, axis=0, level=None, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’,sort_remaining=True, by=None)| Sort object by labels (along an axis)|| 【参数】| ———-| axis : index, columns to direct sorting102| level : int or level name or list of ints or list of level names| if not None, sort on values in specified index level(s)| ascending : boolean, default True| Sort ascending vs. descending| inplace : bool| if True, perform operation in-place| kind : {quicksort, mergesort, heapsort}| Choice of sorting algorithm. 参见：ndarray.np.sort for more information.| mergesort is the only stable algorithm. For DataFrames, this option is| only applied when sorting on a single column or label.| na_position : {‘first’, ‘last’}| first puts NaNs at the beginning, last puts NaNs at the end| sort_remaining : bool| if true and sorting by level and index is multilevel, sort by other levels| too (in order) after sorting by specified level|| 【返回值】| ——-| sorted_obj : DataFrame|| sort_values(self, by, axis=0, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’)| Sort by the values along either axis|| .. versionadded:: 0.17.0|| 【参数】| ———-| by : string name or list of names which refer to the axis items| axis : index, columns to direct sorting| ascending : bool or list of bool| Sort ascending vs. descending. Specify list for multiple sort orders.| If this is a list of bools, must match the length of the by| inplace : bool| if True, perform operation in-place| kind : {quicksort, mergesort, heapsort}| Choice of sorting algorithm. 参见：ndarray.np.sort for more information.| mergesort is the only stable algorithm. For DataFrames, this option is| only applied when sorting on a single column or label.| na_position : {‘first’, ‘last’}| first puts NaNs at the beginning, last puts NaNs at the end|| 【返回值】| ——-| sorted_obj : DataFrame|| sortlevel(self, level=0, axis=0, ascending=True, inplace=False, sort_remaining=True)| Sort multilevel index by chosen axis and primary level. Data will be| lexicographically sorted by the chosen level followed by the other| levels (in order)|| 【参数】| ———-| level : int| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| ascending : boolean, default True| inplace : boolean, default False| Sort the DataFrame without creating a new instance103| sort_remaining : boolean, default True| Sort by the other levels too.|| 【返回值】| ——-| sorted : DataFrame|| 【参见】| ——–| DataFrame.sort_index(level=…)|| stack(self, level=-1, dropna=True)| Pivot a level of the (possibly hierarchical) column labels, returning a| DataFrame (or Series in the case of an object with a single level of| column labels) having a hierarchical index with a new inner-most level| of row labels.| The level involved will automatically get sorted.|| 【参数】| ———-| level : int, string, or list of these, default last level| Level(s) to stack, can pass level name| dropna : boolean, default True| Whether to drop rows in the resulting Frame/Series with no valid| values|| 【示例】| ———-| &gt;&gt;&gt; s| a b| one 1. 2.| two 3. 4.|| &gt;&gt;&gt; s.stack()| one a 1| b 2| two a 3| b 4|| 【返回值】| ——-| stacked : DataFrame or Series|| std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, kwargs)| Return unbiased standard deviation over requested axis.|| Normalized by N-1 by default. This can be changed using the ddof argument|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series104| ddof : int, default 1| degrees of freedom| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| std : Series or DataFrame (if level specified)|| sub(self, other, axis=’columns’, level=None, fill_value=None)| Subtraction of dataframe and other, element-wise (binary operator sub).|| Equivalent to dataframe - other, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.rsub|| subtract = sub(self, other, axis=’columns’, level=None, fill_value=None)|| sum(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return the sum of the values for the requested axis|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data105|| 【返回值】| ——-| sum : Series or DataFrame (if level specified)|| swaplevel(self, i, j, axis=0)| Swap levels i and j in a MultiIndex on a particular axis|| 【参数】| ———-| i, j : int, string (can be mixed)| Level of index to be swapped. Can pass level name as string.|| 【返回值】| ——-| swapped : type of caller (new object)|| to_csv(self, path_or_buf=None, sep=’,’, na_rep=’’, float_format=None, columns=None, header=True, index=True,index_label=None, mode=’w’, encoding=None, compression=None, quoting=None, quotechar=’”‘, line_terminator=’\\n’,chunksize=None, tupleize_cols=False, date_format=None, doublequote=True, escapechar=None, decimal=’.’, kwds)| Write DataFrame to a comma-separated values (csv) file|| 【参数】| ———-| path_or_buf : string or file handle, default None| File path or object, if None is provided the result is returned as| a string.| sep : character, default ‘,’| Field delimiter for the output file.| na_rep : string, default ‘’| Missing data representation| float_format : string, default None| Format string for floating point numbers| columns : sequence, optional| Columns to write| header : boolean or list of string, default True| Write out column names. If a list of string is given it is assumed| to be aliases for the column names| index : boolean, default True| Write row names (index)| index_label : string or sequence, or False, default None| Column label for index column(s) if desired. If None is given, and| header and index are True, then the index names are used. A| sequence should be given if the DataFrame uses MultiIndex. If| False do not print fields for index names. Use index_label=False| for easier importing in R| nanRep : None| deprecated, use na_rep| mode : str| Python write mode, default ‘w’| encoding : string, optional| A string representing the encoding to use in the output file,| defaults to ‘ascii’ on Python 2 and ‘utf-8’ on Python 3.| compression : string, optional| a string representing the compression to use in the output file,| allowed values are ‘gzip’, ‘bz2’,| only used when the first argument is a filename106| line_terminator : string, default ‘\\n’| The newline character or character sequence to use in the output| file| quoting : optional constant from csv module| defaults to csv.QUOTE_MINIMAL| quotechar : string (length 1), default ‘“‘| character used to quote fields| doublequote : boolean, default True| Control quoting of quotechar inside a field| escapechar : string (length 1), default None| character used to escape sep and quotechar when appropriate| chunksize : int or None| rows to write at a time| tupleize_cols : boolean, default False| write multi_index columns as a list of tuples (if True)| or new (expanded format) if False)| date_format : string, default None| Format string for datetime objects| decimal: string, default ‘.’| Character recognized as decimal separator. E.g. use ‘,’ for European data|| .. versionadded:: 0.16.0|| to_dict(self, orient=’dict’)| Convert DataFrame to dictionary.|| 【参数】| ———-| orient : str {‘dict’, ‘list’, ‘series’, ‘split’, ‘records’, ‘index’}| Determines the type of the values of the dictionary.|| - dict (default) : dict like {column -&gt; {index -&gt; value}}| - list : dict like {column -&gt; [values]}| - series : dict like {column -&gt; Series(values)}| - split : dict like| {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]}| - records : list like| [{column -&gt; value}, … , {column -&gt; value}]| - index : dict like {index -&gt; {column -&gt; value}}|| .. versionadded:: 0.17.0|| Abbreviations are allowed. s indicates series and sp| indicates split.|| 【返回值】| ——-| result : dict like {column -&gt; {index -&gt; value}}|| to_excel(self, excel_writer, sheet_name=’Sheet1’, na_rep=’’, float_format=None, columns=None, header=True, index=True,index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep=’inf’, verbose=True)| Write DataFrame to a excel sheet|| 【参数】| ———-| excel_writer : string or ExcelWriter object| File path or existing ExcelWriter107| sheet_name : string, default ‘Sheet1’| Name of sheet which will contain DataFrame| na_rep : string, default ‘’| Missing data representation| float_format : string, default None| Format string for floating point numbers| columns : sequence, optional| Columns to write| header : boolean or list of string, default True| Write out column names. If a list of string is given it is| assumed to be aliases for the column names| index : boolean, default True| Write row names (index)| index_label : string or sequence, default None| Column label for index column(s) if desired. If None is given, and| header and index are True, then the index names are used. A| sequence should be given if the DataFrame uses MultiIndex.| startrow :| upper left cell row to dump data frame| startcol :| upper left cell column to dump data frame| engine : string, default None| write engine to use - you can also set this via the options| io.excel.xlsx.writer, io.excel.xls.writer, and| io.excel.xlsm.writer.| merge_cells : boolean, default True| Write MultiIndex and Hierarchical Rows as merged cells.| encoding: string, default None| encoding of the resulting excel file. Only necessary for xlwt,| other writers support unicode natively.| inf_rep : string, default ‘inf’| Representation for infinity (there is no native representation for| infinity in Excel)|| 【注意】| —–| If passing an existing ExcelWriter object, then the sheet will be added| to the existing workbook. This can be used to save different| DataFrames to one workbook:|| &gt;&gt;&gt; writer = ExcelWriter(‘output.xlsx’)| &gt;&gt;&gt; df1.to_excel(writer,’Sheet1’)| &gt;&gt;&gt; df2.to_excel(writer,’Sheet2’)| &gt;&gt;&gt; writer.save()|| For compatibility with to_csv, to_excel serializes lists and dicts to| strings before writing.|| to_gbq(self, destination_table, project_id, chunksize=10000, verbose=True, reauth=False, if_exists=’fail’)| Write a DataFrame to a Google BigQuery table.|| THIS IS AN EXPERIMENTAL LIBRARY|| 【参数】| ———-| dataframe : DataFrame| DataFrame to be written108| destination_table : string| Name of table to be written, in the form ‘dataset.tablename’| project_id : str| Google BigQuery Account project ID.| chunksize : int (default 10000)| Number of rows to be inserted in each chunk from the dataframe.| verbose : boolean (default True)| Show percentage complete| reauth : boolean (default False)| Force Google BigQuery to reauthenticate the user. This is useful| if multiple accounts are used.| if_exists : {‘fail’, ‘replace’, ‘append’}, default ‘fail’| ‘fail’: If table exists, do nothing.| ‘replace’: If table exists, drop it, recreate it, and insert data.| ‘append’: If table exists, insert data. Create if does not exist.|| .. versionadded:: 0.17.0|| to_html(self, buf=None, columns=None, col_space=None, colSpace=None, header=True, index=True, na_rep=’NaN’,formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, bold_rows=True, classes=None,escape=True, max_rows=None, max_cols=None, show_dimensions=False, notebook=False)| Render a DataFrame as an HTML table.|| to_html-specific options:|| bold_rows : boolean, default True| Make the row labels bold in the output| classes : str or list or tuple, default None| CSS class(es) to apply to the resulting html table| escape : boolean, default True| Convert the characters &lt;, &gt;, and &amp; to HTML-safe sequences.=| max_rows : int, optional| Maximum number of rows to show before truncating. If None, show| all.| max_cols : int, optional| Maximum number of columns to show before truncating. If None, show| all.||| 【参数】| ———-| buf : StringIO-like, optional| buffer to write to| columns : sequence, optional| the subset of columns to write; default None writes all columns| col_space : int, optional| the minimum width of each column| header : bool, optional| whether to print column labels, default True| index : bool, optional| whether to print index (row) labels, default True| na_rep : string, optional| string representation of NAN to use, default ‘NaN’| formatters : list or dict of one-parameter functions, optional| formatter functions to apply to columns’ elements by position or name,| default None. The result of each function must be a unicode string.| List must be of length equal to the number of columns.109| float_format : one-parameter function, optional| formatter function to apply to columns’ elements if they are floats,| default None. The result of this function must be a unicode string.| sparsify : bool, optional| Set to False for a DataFrame with a hierarchical index to print every| multiindex key at each row, default True| index_names : bool, optional| Prints the names of the indexes, default True| justify : {‘left’, ‘right’}, default None| Left or right-justify the column labels. If None uses the option from| the print configuration (controlled by set_option), ‘right’ out| of the box.|| 【返回值】| ——-| formatted : string (or unicode, depending on data and options)|| to_latex(self, buf=None, columns=None, col_space=None, colSpace=None, header=True, index=True, na_rep=’NaN’,formatters=None, float_format=None, sparsify=None, index_names=True, bold_rows=True, column_format=None,longtable=False, escape=True)| Render a DataFrame to a tabular environment table. You can splice| this into a LaTeX document. Requires \\usepackage{booktabs}.|| to_latex-specific options:|| bold_rows : boolean, default True| Make the row labels bold in the output| column_format : str, default None| The columns format as specified in LaTeX table format | &lt;https://en.wikibooks.org/wiki/LaTeX/Tables&gt;__ e.g ‘rcl’ for 3 columns| longtable : boolean, default False| Use a longtable environment instead of tabular. Requires adding| a \\usepackage{longtable} to your LaTeX preamble.| escape : boolean, default True| When set to False prevents from escaping latex special| characters in column names.||| 【参数】| ———-| buf : StringIO-like, optional| buffer to write to| columns : sequence, optional| the subset of columns to write; default None writes all columns| col_space : int, optional| the minimum width of each column| header : bool, optional| whether to print column labels, default True| index : bool, optional| whether to print index (row) labels, default True| na_rep : string, optional| string representation of NAN to use, default ‘NaN’| formatters : list or dict of one-parameter functions, optional| formatter functions to apply to columns’ elements by position or name,| default None. The result of each function must be a unicode string.| List must be of length equal to the number of columns.| float_format : one-parameter function, optional110| formatter function to apply to columns’ elements if they are floats,| default None. The result of this function must be a unicode string.| sparsify : bool, optional| Set to False for a DataFrame with a hierarchical index to print every| multiindex key at each row, default True| index_names : bool, optional| Prints the names of the indexes, default True|| 【返回值】| ——-| formatted : string (or unicode, depending on data and options)|| to_panel(self)| Transform long (stacked) format (DataFrame) into wide (3D, Panel)| format.|| Currently the index of the DataFrame must be a 2-level MultiIndex. This| may be generalized later|| 【返回值】| ——-| panel : Panel|| to_period(self, freq=None, axis=0, copy=True)| Convert DataFrame from DatetimeIndex to PeriodIndex with desired| frequency (inferred from index if not passed)|| 【参数】| ———-| freq : string, default| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| The axis to convert (the index by default)| copy : boolean, default True| If False then underlying input data is not copied|| 【返回值】| ——-| ts : TimeSeries with PeriodIndex|| to_records(self, index=True, convert_datetime64=True)| Convert DataFrame to record array. Index will be put in the| ‘index’ field of the record array if requested|| 【参数】| ———-| index : boolean, default True| Include index in resulting record array, stored in ‘index’ field| convert_datetime64 : boolean, default True| Whether to convert the index to datetime.datetime if it is a| DatetimeIndex|| 【返回值】| ——-| y : recarray|| to_sparse(self, fill_value=None, kind=’block’)| Convert to SparseDataFrame111|| 【参数】| ———-| fill_value : float, default NaN| kind : {‘block’, ‘integer’}|| 【返回值】| ——-| y : SparseDataFrame|| to_stata(self, fname, convert_dates=None, write_index=True, encoding=’latin-1’, byteorder=None, time_stamp=None,data_label=None)| A class for writing Stata binary dta files from array-like objects|| 【参数】| ———-| fname : file path or buffer| Where to save the dta file.| convert_dates : dict| Dictionary mapping column of datetime types to the stata internal| format that you want to use for the dates. Options are| ‘tc’, ‘td’, ‘tm’, ‘tw’, ‘th’, ‘tq’, ‘ty’. Column can be either a| number or a name.| encoding : str| Default is latin-1. Note that Stata does not support unicode.| byteorder : str| Can be “&gt;”, “&lt;”, “little”, or “big”. The default is None which uses| sys.byteorder|| 【示例】| ——–| &gt;&gt;&gt; writer = StataWriter(‘./data_file.dta’, data)| &gt;&gt;&gt; writer.write_file()|| Or with dates|| &gt;&gt;&gt; writer = StataWriter(‘./date_data_file.dta’, data, {2 : ‘tw’})| &gt;&gt;&gt; writer.write_file()|| to_string(self, buf=None, columns=None, col_space=None, header=True, index=True, na_rep=’NaN’, formatters=None,float_format=None, sparsify=None, index_names=True, justify=None, line_width=None, max_rows=None, max_cols=None,show_dimensions=False)| Render a DataFrame to a console-friendly tabular output.|| 【参数】| ———-| buf : StringIO-like, optional| buffer to write to| columns : sequence, optional| the subset of columns to write; default None writes all columns| col_space : int, optional| the minimum width of each column| header : bool, optional| whether to print column labels, default True| index : bool, optional| whether to print index (row) labels, default True| na_rep : string, optional112| string representation of NAN to use, default ‘NaN’| formatters : list or dict of one-parameter functions, optional| formatter functions to apply to columns’ elements by position or name,| default None. The result of each function must be a unicode string.| List must be of length equal to the number of columns.| float_format : one-parameter function, optional| formatter function to apply to columns’ elements if they are floats,| default None. The result of this function must be a unicode string.| sparsify : bool, optional| Set to False for a DataFrame with a hierarchical index to print every| multiindex key at each row, default True| index_names : bool, optional| Prints the names of the indexes, default True| justify : {‘left’, ‘right’}, default None| Left or right-justify the column labels. If None uses the option from| the print configuration (controlled by set_option), ‘right’ out| of the box.|| 【返回值】| ——-| formatted : string (or unicode, depending on data and options)|| to_timestamp(self, freq=None, how=’start’, axis=0, copy=True)| Cast to DatetimeIndex of timestamps, at beginning of period|| 【参数】| ———-| freq : string, default frequency of PeriodIndex| Desired frequency| how : {‘s’, ‘e’, ‘start’, ‘end’}| Convention for converting period to timestamp; start of period| vs. end| axis : {0 or ‘index’, 1 or ‘columns’}, default 0| The axis to convert (the index by default)| copy : boolean, default True| If false then underlying input data is not copied|| 【返回值】| ——-| df : DataFrame with DatetimeIndex|| to_wide = wrapper(*args, kwargs)|| transpose(self)| Transpose index and columns|| truediv(self, other, axis=’columns’, level=None, fill_value=None)| Floating division of dataframe and other, element-wise (binary operator truediv).|| Equivalent to dataframe / other, but with support to substitute a fill_value for| missing data in one of the inputs.|| 【参数】| ———-| other : Series, DataFrame, or constant| axis : {0, 1, ‘index’, ‘columns’}| For Series input, axis to match Series index on113| fill_value : None or float value, default None| Fill missing (NaN) values with this value. If both DataFrame locations are| missing, the result will be missing| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level|| 【注意】| —–| Mismatched indices will be unioned together|| 【返回值】| ——-| result : DataFrame|| 【参见】| ——–| DataFrame.rtruediv|| unstack(self, level=-1)| Pivot a level of the (necessarily hierarchical) index labels, returning| a DataFrame having a new level of column labels whose inner-most level| consists of the pivoted index labels. If the index is not a MultiIndex,| the output will be a Series (the analogue of stack when the columns are| not a MultiIndex).| The level involved will automatically get sorted.|| 【参数】| ———-| level : int, string, or list of these, default -1 (last level)| Level(s) of index to unstack, can pass level name|| 【参见】| ——–| DataFrame.pivot : Pivot a table based on column values.| DataFrame.stack : Pivot a level of the column labels (inverse operation| from unstack).|| 【示例】| ——–| &gt;&gt;&gt; index = pd.MultiIndex.from_tuples([(‘one’, ‘a’), (‘one’, ‘b’),| … (‘two’, ‘a’), (‘two’, ‘b’)])| &gt;&gt;&gt; s = pd.Series(np.arange(1.0, 5.0), index=index)| &gt;&gt;&gt; s| one a 1| b 2| two a 3| b 4| dtype: float64|| &gt;&gt;&gt; s.unstack(level=-1)| a b| one 1 2| two 3 4|| &gt;&gt;&gt; s.unstack(level=0)| one two114| a 1 3| b 2 4|| &gt;&gt;&gt; df = s.unstack(level=0)| &gt;&gt;&gt; df.unstack()| one a 1.| b 3.| two a 2.| b 4.|| 【返回值】| ——-| unstacked : DataFrame or Series|| update(self, other, join=’left’, overwrite=True, filter_func=None, raise_conflict=False)| Modify DataFrame in place using non-NA values from passed| DataFrame. Aligns on indices|| 【参数】| ———-| other : DataFrame, or object coercible into a DataFrame| join : {‘left’}, default ‘left’| overwrite : boolean, default True| If True then overwrite values for common keys in the calling frame| filter_func : callable(1d-array) -&gt; 1d-array, default None| Can choose to replace values other than NA. Return True for values| that should be updated| raise_conflict : boolean| If True, will raise an error if the DataFrame and other both| contain data in the same place.|| var(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, kwargs)| Return unbiased variance over requested axis.|| Normalized by N-1 by default. This can be changed using the ddof argument|| 【参数】| ———-| axis : {index (0), columns (1)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Series| ddof : int, default 1| degrees of freedom| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| var : Series or DataFrame (if level specified)|| ———————————————————————-| Class methods defined here:115|| from_csv(path, header=0, sep=’,’, index_col=0, parse_dates=True, encoding=None, tupleize_cols=False,infer_datetime_format=False) from builtins.type| Read CSV file (DISCOURAGED, please use :func:pandas.read_csv instead).|| It is preferable to use the more powerful :func:pandas.read_csv| for most general purposes, but from_csv makes for an easy| roundtrip to and from a file (the exact counterpart of| to_csv), especially with a DataFrame of time series data.|| This method only differs from the preferred :func:pandas.read_csv| in some defaults:|| - index_col is 0 instead of None (take first column as index| by default)| - parse_dates is True instead of False (try parsing the index| as datetime by default)|| So a pd.DataFrame.from_csv(path) can be replaced by| pd.read_csv(path, index_col=0, parse_dates=True).|| 【参数】| ———-| path : string file path or file handle / StringIO| header : int, default 0| Row to use as header (skip prior rows)| sep : string, default ‘,’| Field delimiter| index_col : int or sequence, default 0| Column to use for index. If a sequence is given, a MultiIndex| is used. Different default from read_table| parse_dates : boolean, default True| Parse dates. Different default from read_table| tupleize_cols : boolean, default False| write multi_index columns as a list of tuples (if True)| or new (expanded format) if False)| infer_datetime_format: boolean, default False| If True and parse_dates is True for a column, try to infer the| datetime format based on the first datetime string. If the format| can be inferred, there often will be a large parsing speed-up.|| 【参见】| ——–| pandas.read_csv|| 【返回值】| ——-| y : DataFrame|| from_dict(data, orient=’columns’, dtype=None) from builtins.type| Construct DataFrame from dict of array-like or dicts|| 【参数】| ———-| data : dict| {field : array-like} or {field : dict}| orient : {‘columns’, ‘index’}, default ‘columns’116| The “orientation” of the data. If the keys of the passed dict| should be the columns of the resulting DataFrame, pass ‘columns’| (default). Otherwise if the keys should be rows, pass ‘index’.| dtype : dtype, default None| Data type to force, otherwise infer|| 【返回值】| ——-| DataFrame|| from_items(items, columns=None, orient=’columns’) from builtins.type| Convert (key, value) pairs to DataFrame. The keys will be the axis| index (usually the columns, but depends on the specified| orientation). The values should be arrays or Series.|| 【参数】| ———-| items : sequence of (key, value) pairs| Values should be arrays or Series.| columns : sequence of column labels, optional| Must be passed if orient=’index’.| orient : {‘columns’, ‘index’}, default ‘columns’| The “orientation” of the data. If the keys of the| input correspond to column labels, pass ‘columns’| (default). Otherwise if the keys correspond to the index,| pass ‘index’.|| 【返回值】| ——-| frame : DataFrame|| from_records(data, index=None, exclude=None, columns=None, coerce_float=False, nrows=None) from builtins.type| Convert structured or record ndarray to DataFrame|| 【参数】| ———-| data : ndarray (structured dtype), list of tuples, dict, or DataFrame| index : string, list of fields, array-like| Field of array to use as the index, alternately a specific set of| input labels to use| exclude : sequence, default None| Columns or fields to exclude| columns : sequence, default None| Column names to use. If the passed data do not have names| associated with them, this argument provides names for the| columns. Otherwise this argument indicates the order of the columns| in the result (any names not found in the data will become all-NA| columns)| coerce_float : boolean, default False| Attempt to convert values to non-string, non-numeric objects (like| decimal.Decimal) to floating point, useful for SQL result sets|| 【返回值】| ——-| df : DataFrame|| ———————————————————————-117| Data descriptors defined here:|| T| Transpose index and columns|| axes| Return a list with the row axis labels and column axis labels as the| only members. They are returned in that order.|| columns|| index|| shape| Return a tuple representing the dimensionality of the DataFrame.|| style| Property returning a Styler object containing methods for| building a styled HTML representation fo the DataFrame.|| 【参见】| ——–| pandas.core.Styler|| ———————————————————————-| 其他数据、属性定义：|| plot = | DataFrame plotting accessor and method|| 【示例】| ——–| &gt;&gt;&gt; df.plot.line()| &gt;&gt;&gt; df.plot.scatter(‘x’, ‘y’)| &gt;&gt;&gt; df.plot.hexbin()|| These plotting methods can also be accessed by calling the accessor as a| method with the kind argument:| df.plot(kind=&#39;line&#39;) is equivalent to df.plot.line()|| ———————————————————————-| Methods inherited from pandas.core.generic.NDFrame:|| abs(self)|| array(self, dtype=None)|| array_wrap(self, result, context=None)|| bool = nonzero(self)|| contains(self, key)| True if the key is in the info axis|| delitem(self, key)| Delete item|118| finalize(self, other, method=None, kwargs)| propagate metadata from other to self|| 【参数】| ———-| other : the object from which to get the attributes that we are going| to propagate| method : optional, a passed method name ; possibly to take different| types of propagation actions based on this|| getattr(self, name)| After regular attribute access, try looking up the name| This allows simpler access to columns for interactive use.|| getstate(self)|| hash(self)| Return hash(self).|| invert(self)|| iter(self)| Iterate over infor axis|| neg(self)|| nonzero(self)|| setattr(self, name, value)| After regular attribute access, try setting the name| This allows simpler access to columns for interactive use.|| setstate(self, state)|| abs(self)| Return an object with absolute value taken. Only applicable to objects| that are all numeric|| 【返回值】| ——-| abs: type of caller|| add_prefix(self, prefix)| Concatenate prefix string with panel items names.|| 【参数】| ———-| prefix : string|| 【返回值】| ——-| with_prefix : type of caller|| add_suffix(self, suffix)| Concatenate suffix string with panel items names|| 【参数】119| ———-| suffix : string|| 【返回值】| ——-| with_suffix : type of caller|| as_blocks(self, copy=True)| Convert the frame to a dict of dtype -&gt; Constructor Types that each has| a homogeneous dtype.|| NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in| as_matrix)|| 【参数】| ———-| copy : boolean, default True|| .. versionadded: 0.16.1|| 【返回值】| ——-| values : a dict of dtype -&gt; Constructor Types|| as_matrix(self, columns=None)| Convert the frame to its Numpy-array representation.|| 【参数】| ———-| columns: list, optional, default:None| If None, return all columns, otherwise, returns specified columns.|| 【返回值】| ——-| values : ndarray| If the caller is heterogeneous and contains booleans or objects,| the result will be of dtype=object. See Notes.||| 【注意】| —–| Return is NOT a Numpy-matrix, rather, a Numpy-array.|| The dtype will be a lower-common-denominator dtype (implicit| upcasting); that is to say if the dtypes (even of numeric types)| are mixed, the one that accommodates all will be chosen. Use this| with care if you are not dealing with the blocks.|| e.g. If the dtypes are float16 and float32, dtype will be upcast to| float32. If dtypes are int32 and uint8, dtype will be upcase to| int32.|| This method is provided for backwards compatibility. Generally,| it is recommended to use ‘.values’.|| 【参见】| ——–120| pandas.DataFrame.values|| asfreq(self, freq, method=None, how=None, normalize=False)| Convert all TimeSeries inside to specified frequency using DateOffset| objects. Optionally provide fill method to pad/backfill missing values.|| 【参数】| ———-| freq : DateOffset object, or string| method : {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}| Method to use for filling holes in reindexed Series| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use NEXT valid observation to fill method| how : {‘start’, ‘end’}, default end| For PeriodIndex only, see PeriodIndex.asfreq| normalize : bool, default False| Whether to reset output index to midnight|| 【返回值】| ——-| converted : type of caller|| astype(self, dtype, copy=True, raise_on_error=True, kwargs)| Cast object to input numpy.dtype| Return a copy when copy = True (be really careful with this!)|| 【参数】| ———-| dtype : numpy.dtype or Python type| raise_on_error : raise on invalid input| kwargs : keyword arguments to pass on to the constructor|| 【返回值】| ——-| casted : type of caller|| at_time(self, time, asof=False)| Select values at particular time of day (e.g. 9:30AM)|| 【参数】| ———-| time : datetime.time or string|| 【返回值】| ——-| values_at_time : type of caller|| between_time(self, start_time, end_time, include_start=True, include_end=True)| Select values between particular times of the day (e.g., 9:00-9:30 AM)|| 【参数】| ———-| start_time : datetime.time or string| end_time : datetime.time or string| include_start : boolean, default True| include_end : boolean, default True|121| 【返回值】| ——-| values_between_time : type of caller|| bfill(self, axis=None, inplace=False, limit=None, downcast=None)| Synonym for NDFrame.fillna(method=’bfill’)|| bool(self)| Return the bool of a single element PandasObject| This must be a boolean scalar value, either True or False|| Raise a ValueError if the PandasObject does not have exactly| 1 element, or that element is not boolean|| clip(self, lower=None, upper=None, out=None, axis=None)| Trim values at input threshold(s)|| 【参数】| ———-| lower : float or array_like, default None| upper : float or array_like, default None| axis : int or string axis name, optional| Align object with lower and upper along the given axis.|| 【返回值】| ——-| clipped : Series|| 【示例】| ——–| &gt;&gt;&gt; df| 0 1| 0 0.335232 -1.256177| 1 -1.367855 0.746646| 2 0.027753 -1.176076| 3 0.230930 -0.679613| 4 1.261967 0.570967| &gt;&gt;&gt; df.clip(-1.0, 0.5)| 0 1| 0 0.335232 -1.000000| 1 -1.000000 0.500000| 2 0.027753 -1.000000| 3 0.230930 -0.679613| 4 0.500000 0.500000| &gt;&gt;&gt; t| 0 -0.3| 1 -0.2| 2 -0.1| 3 0.0| 4 0.1| dtype: float64| &gt;&gt;&gt; df.clip(t, t + 1, axis=0)| 0 1| 0 0.335232 -0.300000| 1 -0.200000 0.746646| 2 0.027753 -0.100000| 3 0.230930 0.000000122| 4 1.100000 0.570967|| clip_lower(self, threshold, axis=None)| Return copy of the input with values below given value(s) truncated|| 【参数】| ———-| threshold : float or array_like| axis : int or string axis name, optional| Align object with threshold along the given axis.|| 【参见】| ——–| clip|| 【返回值】| ——-| clipped : same type as input|| clip_upper(self, threshold, axis=None)| Return copy of input with values above given value(s) truncated|| 【参数】| ———-| threshold : float or array_like| axis : int or string axis name, optional| Align object with threshold along the given axis.|| 【参见】| ——–| clip|| 【返回值】| ——-| clipped : same type as input|| consolidate(self, inplace=False)| Compute NDFrame with “consolidated” internals (data of each dtype| grouped together in a single ndarray). Mainly an internal API function,| but available here to the savvy user|| 【参数】| ———-| inplace : boolean, default False| If False return new object, otherwise modify existing object|| 【返回值】| ——-| consolidated : type of caller|| convert_objects(self, convert_dates=True, convert_numeric=False, convert_timedeltas=True, copy=True)| Attempt to infer better dtype for object columns|| 【参数】| ———-| convert_dates : boolean, default True123| If True, convert to date where possible. If ‘coerce’, force| conversion, with unconvertible values becoming NaT.| convert_numeric : boolean, default False| If True, attempt to coerce to numbers (including strings), with| unconvertible values becoming NaN.| convert_timedeltas : boolean, default True| If True, convert to timedelta where possible. If ‘coerce’, force| conversion, with unconvertible values becoming NaT.| copy : boolean, default True| If True, return a copy even if no copy is necessary (e.g. no| conversion was done). Note: This is meant for internal use, and| should not be confused with inplace.|| 【返回值】| ——-| converted : same as input object|| copy(self, deep=True)| Make a copy of this object|| 【参数】| ———-| deep : boolean or string, default True| Make a deep copy, i.e. also copy data|| 【返回值】| ——-| copy : type of caller|| describe(self, percentiles=None, include=None, exclude=None)| Generate various summary statistics, excluding NaN values.|| 【参数】| ———-| percentiles : array-like, optional| The percentiles to include in the output. Should all| be in the interval [0, 1]. By default percentiles is| [.25, .5, .75], returning the 25th, 50th, and 75th percentiles.| include, exclude : list-like, ‘all’, or None (default)| Specify the form of the returned result. Either:|| - None to both (default). The result will include only numeric-typed| columns or, if none are, only categorical columns.| - A list of dtypes or strings to be included/excluded.| To select all numeric types use numpy numpy.number. To select| categorical objects use type object. 参见：the select_dtypes| documentation. eg. df.describe(include=[‘O’])| - If include is the string ‘all’, the output column-set will| match the input one.|| 【返回值】| ——-| summary: NDFrame of summary statistics|| 【注意】| —–| The output DataFrame index depends on the requested dtypes:124|| For numeric dtypes, it will include: count, mean, std, min,| max, and lower, 50, and upper percentiles.|| For object dtypes (e.g. timestamps or strings), the index| will include the count, unique, most common, and frequency of the| most common. Timestamps also include the first and last items.|| For mixed dtypes, the index will be the union of the corresponding| output types. Non-applicable entries will be filled with NaN.| Note that mixed-dtype outputs can only be returned from mixed-dtype| inputs and appropriate use of the include/exclude arguments.|| If multiple values have the highest count, then the| count and most common pair will be arbitrarily chosen from| among those with the highest count.|| The include, exclude arguments are ignored for Series.|| 【参见】| ——–| DataFrame.select_dtypes|| drop(self, labels, axis=0, level=None, inplace=False, errors=’raise’)| Return new object with labels in requested axis removed|| 【参数】| ———-| labels : single label or list-like| axis : int or axis name| level : int or level name, default None| For MultiIndex| inplace : bool, default False| If True, do operation inplace and return None.| errors : {‘ignore’, ‘raise’}, default ‘raise’| If ‘ignore’, suppress error and existing labels are dropped.|| .. versionadded:: 0.16.1|| 【返回值】| ——-| dropped : type of caller|| equals(self, other)| Determines if two NDFrame objects contain the same elements. NaNs in the| same location are considered equal.|| ffill(self, axis=None, inplace=False, limit=None, downcast=None)| Synonym for NDFrame.fillna(method=’ffill’)|| filter(self, items=None, like=None, regex=None, axis=None)| Restrict the info axis to set of items or wildcard|| 【参数】| ———-| items : list-like| List of info axis to restrict to (must not all be present)125| like : string| Keep info axis where “arg in col == True”| regex : string (regular expression)| Keep info axis with re.search(regex, col) == True| axis : int or None| The axis to filter on. By default this is the info axis. The “info| axis” is the axis that is used when indexing with []. For| example, df = DataFrame({&#39;a&#39;: [1, 2, 3, 4]]}); df[&#39;a&#39;]. So,| the DataFrame columns are the info axis.|| 【注意】| —–| Arguments are mutually exclusive, but this is not checked for|| first(self, offset)| Convenience method for subsetting initial periods of time series data| based on a date offset|| 【参数】| ———-| offset : string, DateOffset, dateutil.relativedelta|| 【示例】| ——–| ts.last(‘10D’) -&gt; First 10 days|| 【返回值】| ——-| subset : type of caller|| get(self, key, default=None)| Get item from object for given key (DataFrame column, Panel slice,| etc.). Returns default value if not found|| 【参数】| ———-| key : object|| 【返回值】| ——-| value : type of items contained in object|| get_dtype_counts(self)| Return the counts of dtypes in this object|| get_ftype_counts(self)| Return the counts of ftypes in this object|| get_values(self)| same as values (but handles sparseness conversions)|| groupby(self, by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False)| Group series using mapper (dict or key function, apply given function| to group, return result as series) or by a series of columns|| 【参数】| ———-126| by : mapping function / list of functions, dict, Series, or tuple /| list of column names.| Called on each element of the object index to determine the groups.| If a dict or Series is passed, the Series or dict VALUES will be| used to determine the groups| axis : int, default 0| level : int, level name, or sequence of such, default None| If the axis is a MultiIndex (hierarchical), group by a particular| level or levels| as_index : boolean, default True| For aggregated output, return object with group labels as the| index. Only relevant for DataFrame input. as_index=False is| effectively “SQL-style” grouped output| sort : boolean, default True| Sort group keys. Get better performance by turning this off.| Note this does not influence the order of observations within each group.| groupby preserves the order of rows within each group.| group_keys : boolean, default True| When calling apply, add group keys to index to identify pieces| squeeze : boolean, default False| reduce the dimensionality of the return type if possible,| otherwise return a consistent type|| 【示例】| ——–| DataFrame results|| &gt;&gt;&gt; data.groupby(func, axis=0).mean()| &gt;&gt;&gt; data.groupby([‘col1’, ‘col2’])[‘col3’].mean()|| DataFrame with hierarchical index|| &gt;&gt;&gt; data.groupby([‘col1’, ‘col2’]).mean()|| 【返回值】| ——-| GroupBy object|| head(self, n=5)| Returns first n rows|| interpolate(self, method=’linear’, axis=0, limit=None, inplace=False, limit_direction=’forward’, downcast=None, kwargs)| Interpolate values according to different methods.|| Please note that only method=&#39;linear&#39; is supported for DataFrames/Series| with a MultiIndex.|| 【参数】| ———-| method : {‘linear’, ‘time’, ‘index’, ‘values’, ‘nearest’, ‘zero’,| ‘slinear’, ‘quadratic’, ‘cubic’, ‘barycentric’, ‘krogh’,| ‘polynomial’, ‘spline’ ‘piecewise_polynomial’, ‘pchip’}|| ‘linear’: ignore the index and treat the values as equally| spaced. This is the only method supported on MultiIndexes.| default| ‘time’: interpolation works on daily and higher resolution127| data to interpolate given length of interval| ‘index’, ‘values’: use the actual numerical values of the index| ‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’,| ‘barycentric’, ‘polynomial’ is passed to| scipy.interpolate.interp1d. Both ‘polynomial’ and ‘spline’| require that you also specify an order (int),| e.g. df.interpolate(method=’polynomial’, order=4).| These use the actual numerical values of the index.| ‘krogh’, ‘piecewise_polynomial’, ‘spline’, and ‘pchip’ are all| wrappers around the scipy interpolation methods of similar| names. These use the actual numerical values of the index. See| the scipy documentation for more on their behavior| here &lt;http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation&gt;| and here &lt;http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html&gt;|| axis : {0, 1}, default 0| 0: fill column-by-column| 1: fill row-by-row| limit : int, default None.| Maximum number of consecutive NaNs to fill.| limit_direction : {‘forward’, ‘backward’, ‘both’}, defaults to ‘forward’| If limit is specified, consecutive NaNs will be filled in this| direction.|| .. versionadded:: 0.17.0|| inplace : bool, default False| Update the NDFrame in place if possible.| downcast : optional, ‘infer’ or None, defaults to None| Downcast dtypes if possible.| kwargs : keyword arguments to pass on to the interpolating function.|| 【返回值】| ——-| Series or DataFrame of same shape interpolated at the NaNs|| 【参见】| ——–| reindex, replace, fillna|| 【示例】| ——–|| Filling in NaNs|| &gt;&gt;&gt; s = pd.Series([0, 1, np.nan, 3])| &gt;&gt;&gt; s.interpolate()| 0 0| 1 1| 2 2| 3 3| dtype: float64|| isnull(self)| Return a boolean same-sized object indicating if the values are null|| 【参见】128| ——–| notnull : boolean inverse of isnull|| iterkv(self, args, kwargs)| iteritems alias used to get around 2to3. Deprecated|| keys(self)| Get the ‘info axis’ (see Indexing for more)|| This is index for Series, columns for DataFrame and major_axis for| Panel.|| last(self, offset)| Convenience method for subsetting final periods of time series data| based on a date offset|| 【参数】| ———-| offset : string, DateOffset, dateutil.relativedelta|| 【示例】| ——–| ts.last(‘5M’) -&gt; Last 5 months|| 【返回值】| ——-| subset : type of caller|| mask(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True)| Return an object of same shape as self and whose corresponding| entries are from self where cond is False and otherwise are from other.|| 【参数】| ———-| cond : boolean NDFrame or array| other : scalar or NDFrame| inplace : boolean, default False| Whether to perform the operation in place on the data| axis : alignment axis if needed, default None| level : alignment level if needed, default None| try_cast : boolean, default False| try to cast the result back to the input type (if possible),| raise_on_error : boolean, default True| Whether to raise on invalid data types (e.g. trying to where on| strings)|| 【返回值】| ——-| wh : same type as caller|| notnull(self)| Return a boolean same-sized object indicating if the values are| not null|| 【参见】| ——–| isnull : boolean inverse of notnull129|| pct_change(self, periods=1, fill_method=’pad’, limit=None, freq=None, kwargs)| Percent change over given number of periods.|| 【参数】| ———-| periods : int, default 1| Periods to shift for forming percent change| fill_method : str, default ‘pad’| How to handle NAs before computing percent changes| limit : int, default None| The number of consecutive NAs to fill before stopping| freq : DateOffset, timedelta, or offset alias string, optional| Increment to use from time series API (e.g. ‘M’ or BDay())|| 【返回值】| ——-| chg : NDFrame|| 【注意】| —–|| By default, the percentage change is calculated along the stat| axis: 0, or Index, for DataFrame and 1, or minor for| Panel. You can change this with the axis keyword argument.|| pipe(self, func, args, **kwargs)| Apply func(self, \\args, **kwargs)|| .. versionadded:: 0.16.2|| 【参数】| ———-| func : function| function to apply to the NDFrame.| args, and kwargs are passed into func.| Alternatively a (callable, data_keyword) tuple where| data_keyword is a string indicating the keyword of| callable that expects the NDFrame.| args : positional arguments passed into func.| kwargs : a dictionary of keyword arguments passed into func.|| 【返回值】| ——-| object : the return type of func.|| 【注意】| —–|| Use .pipe when chaining together functions that expect| on Series or DataFrames. Instead of writing|| &gt;&gt;&gt; f(g(h(df), arg1=a), arg2=b, arg3=c)|| You can write|| &gt;&gt;&gt; (df.pipe(h)130| … .pipe(g, arg1=a)| … .pipe(f, arg2=b, arg3=c)| … )|| If you have a function that takes the data as (say) the second| argument, pass a tuple indicating which keyword expects the| data. For example, suppose f takes its data as arg2:|| &gt;&gt;&gt; (df.pipe(h)| … .pipe(g, arg1=a)| … .pipe((f, ‘arg2’), arg1=a, arg3=c)| … )|| 【参见】| ——–| pandas.DataFrame.apply| pandas.DataFrame.applymap| pandas.Series.map|| pop(self, item)| Return item and drop from frame. Raise KeyError if not found.|| reindex_like(self, other, method=None, copy=True, limit=None, tolerance=None)| return an object with matching indicies to myself|| 【参数】| ———-| other : Object| method : string or None| copy : boolean, default True| limit : int, default None| Maximum number of consecutive labels to fill for inexact matches.| tolerance : optional| Maximum distance between labels of the other object and this| object for inexact matches.|| .. versionadded:: 0.17.0|| 【注意】| —–| Like calling s.reindex(index=other.index, columns=other.columns,| method=…)|| 【返回值】| ——-| reindexed : same as input|| rename_axis(self, mapper, axis=0, copy=True, inplace=False)| Alter index and / or columns using input function or functions.| Function / dict values must be unique (1-to-1). Labels not contained in| a dict / Series will be left as-is.|| 【参数】| ———-| mapper : dict-like or function, optional| axis : int or string, default 0| copy : boolean, default True131| Also copy underlying data| inplace : boolean, default False|| 【返回值】| ——-| renamed : type of caller|| replace(self, to_replace=None, value=None, inplace=False, limit=None, regex=False, method=’pad’, axis=None)| Replace values given in ‘to_replace’ with ‘value’.|| 【参数】| ———-| to_replace : str, regex, list, dict, Series, numeric, or None|| str or regex:|| - str: string exactly matching to_replace will be replaced| with value| - regex: regexs matching to_replace will be replaced with| value|| list of str, regex, or numeric:|| - First, if to_replace and value are both lists, they| must be the same length.| - Second, if regex=True then all of the strings in both| lists will be interpreted as regexs otherwise they will match| directly. This doesn’t matter much for value since there| are only a few possible substitution regexes you can use.| - str and regex rules apply as above.|| dict:|| - Nested dictionaries, e.g., {‘a’: {‘b’: nan}}, are read as| follows: look in column ‘a’ for the value ‘b’ and replace it| with nan. You can nest regular expressions as well. Note that| column names (the top-level dictionary keys in a nested| dictionary) cannot be regular expressions.| - Keys map to column names and values map to substitution| values. You can treat this as a special case of passing two| lists except that you are specifying the column to search in.|| None:|| - This means that the regex argument must be a string,| compiled regular expression, or list, dict, ndarray or Series| of such elements. If value is also None then this| must be a nested dictionary or Series.|| See the examples section for examples of each of these.| value : scalar, dict, list, str, regex, default None| Value to use to fill holes (e.g. 0), alternately a dict of values| specifying which value to use for each column (columns not in the| dict will not be filled). Regular expressions, strings and lists or| dicts of such objects are also allowed.| inplace : boolean, default False| If True, in place. Note: this will modify any132| other views on this object (e.g. a column form a DataFrame).| Returns the caller if this is True.| limit : int, default None| Maximum size gap to forward or backward fill| regex : bool or same types as to_replace, default False| Whether to interpret to_replace and/or value as regular| expressions. If this is True then to_replace must be a| string. Otherwise, to_replace must be None because this| parameter will be interpreted as a regular expression or a list,| dict, or array of regular expressions.| method : string, optional, {‘pad’, ‘ffill’, ‘bfill’}| The method to use when for replacement, when to_replace is a| list.|| 【参见】| ——–| NDFrame.reindex| NDFrame.asfreq| NDFrame.fillna|| 【返回值】| ——-| filled : NDFrame|| 【Raises 引发错误】| ——| AssertionError| If regex is not a bool and to_replace is not None.| TypeError| If to_replace is a dict and value is not a list,| dict, ndarray, or Series| If to_replace is None and regex is not compilable into a| regular expression or is a list, dict, ndarray, or Series.| ValueError| If to_replace and value are list s or ndarray s, but| they are not the same length.|| 【注意】| —–| Regex substitution is performed under the hood with re.sub. The| rules for substitution for re.sub are the same.| Regular expressions will only substitute on strings, meaning you| cannot provide, for example, a regular expression matching floating| point numbers and expect the columns in your frame that have a| numeric dtype to be matched. However, if those floating point numbers| are strings, then you can do this.| This method has a lot of options. You are encouraged to experiment| and play with this method to gain intuition about how it works.|| resample(self, rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention=’start’, kind=None,loffset=None, limit=None, base=0)| Convenience method for frequency conversion and resampling of regular| time-series data.|| 【参数】| ———-| rule : string133| the offset string or object representing target conversion| how : string| method for down- or re-sampling, default to ‘mean’ for| downsampling| axis : int, optional, default 0| fill_method : string, default None| fill_method for upsampling| closed : {‘right’, ‘left’}| Which side of bin interval is closed| label : {‘right’, ‘left’}| Which bin edge label to label bucket with| convention : {‘start’, ‘end’, ‘s’, ‘e’}| kind : “period”/“timestamp”| loffset : timedelta| Adjust the resampled time labels| limit : int, default None| Maximum size gap to when reindexing with fill_method| base : int, default 0| For frequencies that evenly subdivide 1 day, the “origin” of the| aggregated intervals. For example, for ‘5min’ frequency, base could| range from 0 through 4. Defaults to 0||| 【示例】| ——–|| Start by creating a series with 9 one minute timestamps.|| &gt;&gt;&gt; index = pd.date_range(‘1/1/2000’, periods=9, freq=’T’)| &gt;&gt;&gt; series = pd.Series(range(9), index=index)| &gt;&gt;&gt; series| 2000-01-01 00:00:00 0| 2000-01-01 00:01:00 1| 2000-01-01 00:02:00 2| 2000-01-01 00:03:00 3| 2000-01-01 00:04:00 4| 2000-01-01 00:05:00 5| 2000-01-01 00:06:00 6| 2000-01-01 00:07:00 7| 2000-01-01 00:08:00 8| Freq: T, dtype: int64|| Downsample the series into 3 minute bins and sum the values| of the timestamps falling into a bin.|| &gt;&gt;&gt; series.resample(‘3T’, how=’sum’)| 2000-01-01 00:00:00 3| 2000-01-01 00:03:00 12| 2000-01-01 00:06:00 21| Freq: 3T, dtype: int64|| Downsample the series into 3 minute bins as above, but label each| bin using the right edge instead of the left. Please note that the| value in the bucket used as the label is not included in the bucket,| which it labels. For example, in the original series the| bucket 2000-01-01 00:03:00 contains the value 3, but the summed| value in the resampled bucket with the label2000-01-01 00:03:00134| does not include 3 (if it did, the summed value would be 6, not 3).| To include this value close the right side of the bin interval as| illustrated in the example below this one.|| &gt;&gt;&gt; series.resample(‘3T’, how=’sum’, label=’right’)| 2000-01-01 00:03:00 3| 2000-01-01 00:06:00 12| 2000-01-01 00:09:00 21| Freq: 3T, dtype: int64|| Downsample the series into 3 minute bins as above, but close the right| side of the bin interval.|| &gt;&gt;&gt; series.resample(‘3T’, how=’sum’, label=’right’, closed=’right’)| 2000-01-01 00:00:00 0| 2000-01-01 00:03:00 6| 2000-01-01 00:06:00 15| 2000-01-01 00:09:00 15| Freq: 3T, dtype: int64|| Upsample the series into 30 second bins.|| &gt;&gt;&gt; series.resample(‘30S’)[0:5] #select first 5 rows| 2000-01-01 00:00:00 0| 2000-01-01 00:00:30 NaN| 2000-01-01 00:01:00 1| 2000-01-01 00:01:30 NaN| 2000-01-01 00:02:00 2| Freq: 30S, dtype: float64|| Upsample the series into 30 second bins and fill the NaN| values using the pad method.|| &gt;&gt;&gt; series.resample(‘30S’, fill_method=’pad’)[0:5]| 2000-01-01 00:00:00 0| 2000-01-01 00:00:30 0| 2000-01-01 00:01:00 1| 2000-01-01 00:01:30 1| 2000-01-01 00:02:00 2| Freq: 30S, dtype: int64|| Upsample the series into 30 second bins and fill the| NaN values using the bfill method.|| &gt;&gt;&gt; series.resample(‘30S’, fill_method=’bfill’)[0:5]| 2000-01-01 00:00:00 0| 2000-01-01 00:00:30 1| 2000-01-01 00:01:00 1| 2000-01-01 00:01:30 2| 2000-01-01 00:02:00 2| Freq: 30S, dtype: int64|| Pass a custom function to how.|| &gt;&gt;&gt; def custom_resampler(array_like):| … return np.sum(array_like)+5|135| &gt;&gt;&gt; series.resample(‘3T’, how=custom_resampler)| 2000-01-01 00:00:00 8| 2000-01-01 00:03:00 17| 2000-01-01 00:06:00 26| Freq: 3T, dtype: int64|| sample(self, n=None, frac=None, replace=False, weights=None, random_state=None, axis=None)| Returns a random sample of items from an axis of object.|| .. versionadded:: 0.16.1|| 【参数】| ———-| n : int, optional| Number of items from axis to return. Cannot be used with frac.| Default = 1 if frac = None.| frac : float, optional| Fraction of axis items to return. Cannot be used with n.| replace : boolean, optional| Sample with or without replacement. Default = False.| weights : str or ndarray-like, optional| Default ‘None’ results in equal probability weighting.| If passed a Series, will align with target object on index. Index| values in weights not found in sampled object will be ignored and| index values in sampled object not in weights will be assigned| weights of zero.| If called on a DataFrame, will accept the name of a column| when axis = 0.| Unless weights are a Series, weights must be same length as axis| being sampled.| If weights do not sum to 1, they will be normalized to sum to 1.| Missing values in the weights column will be treated as zero.| inf and -inf values not allowed.| random_state : int or numpy.random.RandomState, optional| Seed for the random number generator (if int), or numpy RandomState| object.| axis : int or string, optional| Axis to sample. Accepts axis number or name. Default is stat axis| for given data type (0 for Series and DataFrames, 1 for Panels).|| 【返回值】| ——-| A new object of same type as caller.|| select(self, crit, axis=0)| Return data corresponding to axis labels matching criteria|| 【参数】| ———-| crit : function| To be called on each index (label). Should return True or False| axis : int|| 【返回值】| ——-| selection : type of caller|136| set_axis(self, axis, labels)| public verson of axis assignment|| slice_shift(self, periods=1, axis=0)| Equivalent to shift without copying data. The shifted data will| not include the dropped periods and the shifted axis will be smaller| than the original.|| 【参数】| ———-| periods : int| Number of periods to move, can be positive or negative|| 【注意】| —–| While the slice_shift is faster than shift, you may pay for it| later during alignment.|| 【返回值】| ——-| shifted : same type as caller|| squeeze(self)| squeeze length 1 dimensions|| swapaxes(self, axis1, axis2, copy=True)| Interchange axes and swap values axes appropriately|| 【返回值】| ——-| y : same as input|| tail(self, n=5)| Returns last n rows|| take(self, indices, axis=0, convert=True, is_copy=True)| Analogous to ndarray.take|| 【参数】| ———-| indices : list / array of ints| axis : int, default 0| convert : translate neg to pos indices (default)| is_copy : mark the returned frame as a copy|| 【返回值】| ——-| taken : type of caller|| to_clipboard(self, excel=None, sep=None, kwargs)| Attempt to write text representation of object to the system clipboard| This can be pasted into Excel, for example.|| 【参数】| ———-| excel : boolean, defaults to True| if True, use the provided separator, writing in a csv137| format for allowing easy pasting into excel.| if False, write a string representation of the object| to the clipboard| sep : optional, defaults to tab| other keywords are passed to to_csv|| 【注意】| —–| Requirements for your platform| - Linux: xclip, or xsel (with gtk or PyQt4 modules)| - Windows: none| - OS X: none|| to_dense(self)| Return dense representation of NDFrame (as opposed to sparse)|| to_hdf(self, path_or_buf, key, kwargs)| activate the HDFStore|| 【参数】| ———-| path_or_buf : the path (string) or HDFStore object| key : string| indentifier for the group in the store| mode : optional, {‘a’, ‘w’, ‘r’, ‘r+’}, default ‘a’|| &#39;r&#39;| Read-only; no data can be modified.| &#39;w&#39;| Write; a new file is created (an existing file with the same| name would be deleted).| &#39;a&#39;| Append; an existing file is opened for reading and writing,| and if the file does not exist it is created.| &#39;r+&#39;| It is similar to &#39;a&#39;, but the file must already exist.| format : ‘fixed(f)|table(t)’, default is ‘fixed’| fixed(f) : Fixed format| Fast writing/reading. Not-appendable, nor searchable| table(t) : Table format| Write as a PyTables Table structure which may perform| worse but allow more flexible operations like searching| / selecting subsets of the data| append : boolean, default False| For Table formats, append the input data to the existing| complevel : int, 1-9, default 0| If a complib is specified compression will be applied| where possible| complib : {‘zlib’, ‘bzip2’, ‘lzo’, ‘blosc’, None}, default None| If complevel is &gt; 0 apply compression to objects written| in the store wherever possible| fletcher32 : bool, default False| If applying compression use the fletcher32 checksum| dropna : boolean, default False.| If true, ALL nan rows will not be written to store.|| to_json(self, path_or_buf=None, orient=None, date_format=’epoch’, double_precision=10, force_ascii=True, date_unit=’ms’,138default_handler=None)| Convert the object to a JSON string.|| Note NaN’s and None will be converted to null and datetime objects| will be converted to UNIX timestamps.|| 【参数】| ———-| path_or_buf : the path or buffer to write the result string| if this is None, return a StringIO of the converted string| orient : string|| Series|| - default is ‘index’| - allowed values are: {‘split’,’records’,’index’}|| DataFrame|| - default is ‘columns’| - allowed values are:| {‘split’,’records’,’index’,’columns’,’values’}|| The format of the JSON string|| - split : dict like| {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]}| - records : list like| [{column -&gt; value}, … , {column -&gt; value}]| - index : dict like {index -&gt; {column -&gt; value}}| - columns : dict like {column -&gt; {index -&gt; value}}| - values : just the values array|| date_format : {‘epoch’, ‘iso’}| Type of date conversion. epoch = epoch milliseconds,| iso`` = ISO8601, default is epoch. | double_precision : The number of decimal places to use when encoding | floating point values, default 10. | force_ascii : force encoded string to be ASCII, default True. | date_unit : string, default &#39;ms&#39; (milliseconds) | The time unit to encode to, governs timestamp and ISO8601 | precision. One of &#39;s&#39;, &#39;ms&#39;, &#39;us&#39;, &#39;ns&#39; for second, millisecond, | microsecond, and nanosecond respectively. | default_handler : callable, default None | Handler to call if object cannot otherwise be converted to a | suitable format for JSON. Should receive a single argument which is | the object to convert and return a serialisable object. | | 【返回值】 | -------| same type as input object with filtered info axis | | to_msgpack(self, path_or_buf=None, **kwargs) | msgpack (serialize) object to input file path | | THIS IS AN EXPERIMENTAL LIBRARY and the storage format | may not be stable until a future release. 139 | | 【参数】 | ----------| path : string File path, buffer-like, or None | if None, return generated string | append : boolean whether to append to an existing msgpack | (default is False) | compress : type of compressor (zlib or blosc), default to None (no | compression) | | to_pickle(self, path) | Pickle (serialize) object to input file path | | 【参数】 | ----------| path : string | File path | | to_sql(self, name, con, flavor=&#39;sqlite&#39;, schema=None, if_exists=&#39;fail&#39;, index=True, index_label=None, chunksize=None, dtype=None) | Write records stored in a DataFrame to a SQL database. | | 【参数】 | ----------| name : string | Name of SQL table | con : SQLAlchemy engine or DBAPI2 connection (legacy mode) | Using SQLAlchemy makes it possible to use any DB supported by that | library. | If a DBAPI2 object, only sqlite3 is supported. | flavor : {&#39;sqlite&#39;, &#39;mysql&#39;}, default &#39;sqlite&#39; | The flavor of SQL to use. Ignored when using SQLAlchemy engine. | &#39;mysql&#39; is deprecated and will be removed in future versions, but it | will be further supported through SQLAlchemy engines. | schema : string, default None | Specify the schema (if database flavor supports this). If None, use | default schema. | if_exists : {&#39;fail&#39;, &#39;replace&#39;, &#39;append&#39;}, default &#39;fail&#39; | - fail: If table exists, do nothing. | - replace: If table exists, drop it, recreate it, and insert data. | - append: If table exists, insert data. Create if does not exist. | index : boolean, default True | Write DataFrame index as a column. | index_label : string or sequence, default None | Column label for index column(s). If None is given (default) and |indexis True, then the index names are used. | A sequence should be given if the DataFrame uses MultiIndex. | chunksize : int, default None | If not None, then rows will be written in batches of this size at a | time. If None, all rows will be written at once. | dtype : dict of column name to SQL type, default None | Optional specifying the datatype for columns. The SQL type should | be a SQLAlchemy type, or a string for sqlite3 fallback connection. | | truncate(self, before=None, after=None, axis=None, copy=True) | Truncates a sorted NDFrame before and/or after some particular | dates. 140 | | 【参数】 | ----------| before : date | Truncate before date | after : date | Truncate after date | axis : the truncation axis, defaults to the stat axis | copy : boolean, default is True, | return a copy of the truncated section | | 【返回值】 | -------| truncated : type of caller | | tshift(self, periods=1, freq=None, axis=0) | Shift the time index, using the index&#39;s frequency if available | | 【参数】 | ----------| periods : int | Number of periods to move, can be positive or negative | freq : DateOffset, timedelta, or time rule string, default None | Increment to use from datetools module or time rule (e.g. &#39;EOM&#39;) | axis : int or basestring | Corresponds to the axis that contains the Index | | 【注意】 | -----| If freq is not specified then tries to use the freq or inferred_freq | attributes of the index. If neither of those attributes exist, a | ValueError is thrown | | 【返回值】 | -------| shifted : NDFrame | | tz_convert(self, tz, axis=0, level=None, copy=True) | Convert tz-aware axis to target time zone. | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to convert | level : int, str, default None | If axis ia a MultiIndex, convert a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data | | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError 141 | If the axis is tz-naive. | | tz_localize(self, tz, axis=0, level=None, copy=True, ambiguous=&#39;raise&#39;) | Localize tz-naive TimeSeries to target time zone | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to localize | level : int, str, default None | If axis ia a MultiIndex, localize a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data | ambiguous : &#39;infer&#39;, bool-ndarray, &#39;NaT&#39;, default &#39;raise&#39; | - &#39;infer&#39; will attempt to infer fall dst-transition hours based on order | - bool-ndarray where True signifies a DST time, False designates | a non-DST time (note that this flag is only applicable for ambiguous times) | - &#39;NaT&#39; will return NaT where there are ambiguous times | - &#39;raise&#39; will raise an AmbiguousTimeError if there are ambiguous times | infer_dst : boolean, default False (DEPRECATED) | Attempt to infer fall dst-transition hours based on order | | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError | If the TimeSeries is tz-aware and tz is not None. | | where(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True) | Return an object of same shape as self and whose corresponding | entries are from self where cond is True and otherwise are from other. | | 【参数】 | ----------| cond : boolean NDFrame or array | other : scalar or NDFrame | inplace : boolean, default False | Whether to perform the operation in place on the data | axis : alignment axis if needed, default None | level : alignment level if needed, default None | try_cast : boolean, default False | try to cast the result back to the input type (if possible), | raise_on_error : boolean, default True | Whether to raise on invalid data types (e.g. trying to where on | strings) | | 【返回值】 | -------| wh : same type as caller | | xs(self, key, axis=0, level=None, copy=None, drop_level=True) | Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. | Defaults to cross-section on the rows (axis=0). | 142 | 【参数】 | ----------| key : object | Some label contained in the index, or partially in a MultiIndex | axis : int, default 0 | Axis to retrieve cross-section on | level : object, defaults to first n levels (n=1 or len(key)) | In case of a key partially contained in a MultiIndex, indicate | which levels are used. Levels can be referred by label or position. | copy : boolean [deprecated] | Whether to make a copy of the data | drop_level : boolean, default True | If False, returns object with same levels as self. | | 【示例】 | --------| &gt;&gt;&gt; df | A B C | a 4 5 2 | b 4 0 9 | c 9 7 3 | &gt;&gt;&gt; df.xs(&#39;a&#39;) | A 4 | B 5 | C 2 | Name: a | &gt;&gt;&gt; df.xs(&#39;C&#39;, axis=1) | a 2 | b 9 | c 3 | Name: C | | &gt;&gt;&gt; df | A B C D | first second third | bar one 1 4 1 8 9 | two 1 7 5 5 0 | baz one 1 6 6 8 0 | three 2 5 3 5 3 | &gt;&gt;&gt; df.xs((&#39;baz&#39;, &#39;three&#39;)) | A B C D | third | 2 5 3 5 3 | &gt;&gt;&gt; df.xs(&#39;one&#39;, level=1) | A B C D | first third | bar 1 4 1 8 9 | baz 1 6 6 8 0 | &gt;&gt;&gt; df.xs((&#39;baz&#39;, 2), level=[0, &#39;third&#39;]) | A B C D | second | three 5 3 5 3 | | 【返回值】 | -------| xs : Series or DataFrame | 143 | 【注意】 | -----| xs is only for getting, not setting values. | | MultiIndex Slicers is a generic way to get/set values on any level or levels | it is a superset of xs functionality, see :ref:MultiIndex Slicers | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.generic.NDFrame: | | at | Fast label-based scalar accessor | | Similarly to ``loc``, ``at`` provides **label** based scalar lookups. | You can also set using these indexers. | | blocks | Internal property, property synonym for as_blocks() | | dtypes | Return the dtypes in this object | | empty | True if NDFrame is entirely empty [no items] | | ftypes | Return the ftypes (indication of sparse/dense and dtype) | in this object. | | iat | Fast integer location scalar accessor. | | Similarly to ``iloc``, ``iat`` provides **integer** based lookups. | You can also set using these indexers. | | iloc | Purely integer-location based indexing for selection by position. | | ``.iloc[]`` is primarily integer position based (from ``0`` to | ``length-1`` of the axis), but may also be used with a boolean | array. | | Allowed inputs are: | | - An integer, e.g. ``5``. | - A list or array of integers, e.g. ``[4, 3, 0]``. | - A slice object with ints, e.g. ``1:7``. | - A boolean array. | | ``.iloc`` will raise ``IndexError`` if a requested indexer is | out-of-bounds, except *slice* indexers which allow out-of-bounds | indexing (this conforms with python/numpy *slice* semantics). | | See more at :ref:Selection by Position | | ix | A primarily label-location based indexer, with integer position 144 | fallback. | | ``.ix[]`` supports mixed integer and label based access. It is | primarily label based, but will fall back to integer positional | access unless the corresponding axis is of integer type. | | ``.ix`` is the most general indexer and will support any of the | inputs in ``.loc`` and ``.iloc``. ``.ix`` also supports floating | point label schemes. ``.ix`` is exceptionally useful when dealing | with mixed positional and label based hierachical indexes. | | However, when an axis is integer based, ONLY label based access | and not positional access is supported. Thus, in such cases, it&#39;s | usually better to be explicit and use ``.iloc`` or ``.loc``. | | See more at :ref:Advanced Indexing . | | loc | Purely label-location based indexer for selection by label. | | ``.loc[]`` is primarily label based, but may also be used with a | boolean array. | | Allowed inputs are: | | - A single label, e.g. ``5`` or ``&#39;a&#39;``, (note that ``5`` is | interpreted as a *label* of the index, and **never** as an | integer position along the index). | - A list or array of labels, e.g. ``[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]``. | - A slice object with labels, e.g. ``&#39;a&#39;:&#39;f&#39;`` (note that contrary | to usual python slices, **both** the start and the stop are included!). | - A boolean array. | | ``.loc`` will raise a ``KeyError`` when the items are not found. | | See more at :ref:Selection by Label | | ndim | Number of axes / array dimensions | | size | number of elements in the NDFrame | | values | Numpy representation of NDFrame | | 【注意】 | -----| The dtype will be a lower-common-denominator dtype (implicit | upcasting); that is to say if the dtypes (even of numeric types) | are mixed, the one that accommodates all will be chosen. Use this | with care if you are not dealing with the blocks. | | e.g. If the dtypes are float16 and float32, dtype will be upcast to | float32. If dtypes are int32 and uint8, dtype will be upcase to | int32. | 145 | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.generic.NDFrame: | | is_copy = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.StringMixin: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) DateOffset DateOffset 模块所属：pandas.tseries.offsets: 类定义：DateOffset(builtins.object) | Standard kind of date increment used for a date range. | | Works exactly like relativedelta in terms of the keyword args you | pass in, use of the keyword n is discouraged-- you would be better 146 | off specifying n in the keywords you use, but regardless it is | there for you. n is needed for DateOffset subclasses. | | DateOffets work as follows. Each offset specify a set of dates | that conform to the DateOffset. For example, Bday defines this | set to be the set of dates that are weekdays (M-F). To test if a | date is in the set of a DateOffset dateOffset we can use the | onOffset method: dateOffset.onOffset(date). | | If a date is not on a valid date, the rollback and rollforward | methods can be used to roll the date to the nearest valid date | before/after the date. | | DateOffsets can be created to move dates forward a given number of | valid dates. For example, Bday(2) can be added to a date to move | it two business days forward. If the date does not start on a | valid date, first it is moved to a valid date. Thus psedo code | is: | | def __add__(date): | date = rollback(date) # does nothing if date is valid | return date + &lt;n number of periods&gt; | | When a date offset is created for a negitive number of periods, | the date is first rolled forward. The pseudo code is: | | def __add__(date): | date = rollforward(date) # does nothing is date is valid | return date + &lt;n number of periods&gt; | | Zero presents a problem. Should it roll forward or back? We | arbitrarily have it rollforward: | | date + BDay(0) == BDay.rollforward(date) | | Since 0 is a bit weird, we suggest avoiding its use. | | 【方法定义】 | | __add__(self, other) | | __call__(self, other) | Call self as a function. | | __eq__(self, other) | Return self==value. | | __hash__(self) | Return hash(self). | | __init__(self, n=1, normalize=False, **kwds) | Initialize self. See help(type(self)) for accurate signature. | | __mul__(self, someInt) | | __ne__(self, other) | Return self!=value. 147 | | __neg__(self) | | __radd__(self, other) | | __repr__(self) | Return repr(self). | | __rmul__(self, someInt) | | __rsub__(self, other) | | __sub__(self, other) | | apply(self, other) | | apply_index(self, i) | Vectorized apply of DateOffset to DatetimeIndex, | raises NotImplentedError for offsets without a | vectorized implementation | | .. versionadded:: 0.17.0 | | 【参数】 | ----------| i : DatetimeIndex | | 【返回值】 | -------| y : DatetimeIndex | | copy(self) | | isAnchored(self) | | onOffset(self, dt) | | rollback(self, dt) | Roll provided date backward to next offset only if not on offset | | rollforward(self, dt) | Roll provided date forward to next offset only if not on offset | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | freqstr | | name | | rule_code 148 | | ----------------------------------------------------------------------| 其他数据、属性定义： | | normalize = False DatetimeIndex DatetimeIndex 模块所属：pandas.tseries.index: 类 定 义 ： DatetimeIndex(pandas.tseries.base.DatelikeOps, pandas.tseries.base.DatetimeIndexOpsMixin, pandas.core.index.Int64Index) | Immutable ndarray of datetime64 data, represented internally as int64, and | which can be boxed to Timestamp objects that are subclasses of datetime and | carry metadata such as frequency information. | | 【参数】 | ----------| data : array-like (1-dimensional), optional | Optional datetime-like data to construct index with | copy : bool | Make a copy of input ndarray | freq : string or pandas offset object, optional | One of pandas date offset strings or corresponding objects | start : starting value, datetime-like, optional | If data is None, start is used as the start point in generating regular | timestamp data. | periods : int, optional, &gt; 0 | Number of periods to generate, if generating index. Takes precedence | over end argument | end : end time, datetime-like, optional | If periods is none, generated index will extend to first conforming | time on or just past end argument | closed : string or None, default None | Make the interval closed with respect to the given frequency to | the &#39;left&#39;, &#39;right&#39;, or both sides (None) | tz : pytz.timezone or dateutil.tz.tzfile | ambiguous : &#39;infer&#39;, bool-ndarray, &#39;NaT&#39;, default &#39;raise&#39; | - &#39;infer&#39; will attempt to infer fall dst-transition hours based on order | - bool-ndarray where True signifies a DST time, False signifies | a non-DST time (note that this flag is only applicable for ambiguous times) | - &#39;NaT&#39; will return NaT where there are ambiguous times | - &#39;raise&#39; will raise an AmbiguousTimeError if there are ambiguous times | infer_dst : boolean, default False (DEPRECATED) | Attempt to infer fall dst-transition hours based on order | name : object | Name to be stored in the index | | 【方法排序】 | DatetimeIndex 149 | pandas.tseries.base.DatelikeOps | pandas.tseries.base.DatetimeIndexOpsMixin | pandas.core.index.Int64Index | pandas.core.index.NumericIndex | pandas.core.index.Index | pandas.core.base.IndexOpsMixin | pandas.core.strings.StringAccessorMixin | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __abs__(self, other=None) | | __add__(self, other) | | __eq__ = wrapper(self, other) | | __floordiv__(self, other=None) | | __ge__ = wrapper(self, other) | | __gt__ = wrapper(self, other) | | __iadd__ = __add__(self, other) | | __inv__(self, other=None) | | __isub__ = __sub__(self, other) | | __iter__(self) | Return an iterator over the boxed values | | 【返回值】 | -------| Timestamps : ndarray | | __le__ = wrapper(self, other) | | __lt__ = wrapper(self, other) | | __mul__(self, other=None) | | __ne__ = wrapper(self, other) | | __neg__(self, other=None) | | __pos__(self, other=None) | | __radd__ = __add__(self, other) | | __reduce__(self) | helper for pickle | | __rfloordiv__ = __floordiv__(self, other=None) | 150 | __rmul__ = __mul__(self, other=None) | | __rsub__(self, other) | | __rtruediv__ = __truediv__(self, other=None) | | __setstate__(self, state) | Necessary for making this object picklable | | __sub__(self, other) | | __truediv__(self, other=None) | | all(self, other=None) | | any(self, other=None) | | append(self, other) | Append a collection of Index options together | | 【参数】 | ----------| other : Index or list/tuple of indices | | 【返回值】 | -------| appended : Index | | astype(self, dtype) | | delete(self, loc) | Make a new DatetimeIndex with passed location(s) deleted. | | 【参数】 | ----------| loc: int, slice or array of ints | Indicate which sub-arrays to remove. | | 【返回值】 | -------| new_index : DatetimeIndex | | equals(self, other) | Determines if two Index objects contain the same elements. | | get_loc(self, key, method=None, tolerance=None) | Get integer location for requested label | | 【返回值】 | -------| loc : int | | get_value(self, series, key) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | get_value_maybe_box(self, series, key) 151 | | indexer_at_time(self, time, asof=False) | Select values at particular time of day (e.g. 9:30AM) | | 【参数】 | ----------| time : datetime.time or string | | 【返回值】 | -------| values_at_time : TimeSeries | | indexer_between_time(self, start_time, end_time, include_start=True, include_end=True) | Select values between particular times of day (e.g., 9:00-9:30AM) | | 【参数】 | ----------| start_time : datetime.time or string | end_time : datetime.time or string | include_start : boolean, default True | include_end : boolean, default True | tz : string or pytz.timezone or dateutil.tz.tzfile, default None | | 【返回值】 | -------| values_between_time : TimeSeries | | insert(self, loc, item) | Make new Index inserting new item at location | | 【参数】 | ----------| loc : int | item : object | if not either a Python datetime or a numpy integer-like, returned | Index dtype will be object rather than datetime. | | 【返回值】 | -------| new_index : Index | | intersection(self, other) | Specialized intersection for DatetimeIndex objects. May be much faster | than Index.intersection | | 【参数】 | ----------| other : DatetimeIndex or array-like | | 【返回值】 | -------| y : Index or DatetimeIndex | | is_type_compatible(self, typ) | | join(self, other, how=&#39;left&#39;, level=None, return_indexers=False) 152 | See Index.join | | normalize(self) | Return DatetimeIndex with times to midnight. Length is unaltered | | 【返回值】 | -------| normalized : DatetimeIndex | | searchsorted(self, key, side=&#39;left&#39;) | np.ndarray searchsorted compat | | slice_indexer(self, start=None, end=None, step=None, kind=None) | Return indexer for specified label slice. | Index.slice_indexer, customized to handle time slicing. | | In addition to functionality provided by Index.slice_indexer, does the | following: | | - if bothstartandendare instances ofdatetime.time, it | invokesindexer_between_time| - ifstartandendare both either string or None perform | value-based selection in non-monotonic cases. | | snap(self, freq=&#39;S&#39;) | Snap time stamps to nearest occurring frequency | | to_datetime(self, dayfirst=False) | For an Index containing strings or datetime.datetime objects, attempt | conversion to DatetimeIndex | | to_julian_date(self) | Convert DatetimeIndex to Float64Index of Julian Dates. | 0 Julian date is noon January 1, 4713 BC. | http://en.wikipedia.org/wiki/Julian_day | | to_period(self, freq=None) | Cast to PeriodIndex at a particular frequency | | to_perioddelta(self, freq) | Calcuates TimedeltaIndex of difference between index | values and index converted to PeriodIndex at specified | freq. Used for vectorized offsets | | .. versionadded:: 0.17.0 | | 【参数】 | ----------| freq : Period frequency | | 【返回值】 | -------| y : TimedeltaIndex | | to_pydatetime(self) | Return DatetimeIndex as object ndarray of datetime.datetime objects | 153 | 【返回值】 | -------| datetimes : ndarray | | to_series(self, keep_tz=False) | Create a Series with both index and values equal to the index keys | useful with map for returning an indexer based on an index | | 【参数】 | ----------| keep_tz : optional, defaults False. | return the data keeping the timezone. | | If keep_tz is True: | | If the timezone is not set, the resulting | Series will have a datetime64[ns] dtype. | | Otherwise the Series will have an datetime64[ns, tz] dtype; the | tz will be preserved. | | If keep_tz is False: | | Series will have a datetime64[ns] dtype. TZ aware | objects will have the tz removed. | | 【返回值】 | -------| Series | | tz_convert(self, tz) | Convert tz-aware DatetimeIndex from one time zone to another (using pytz/dateutil) | | 【参数】 | ----------| tz : string, pytz.timezone, dateutil.tz.tzfile or None | Time zone for time. Corresponding timestamps would be converted to | time zone of the TimeSeries. | None will remove timezone holding UTC time. | | 【返回值】 | -------| normalized : DatetimeIndex | | 【Raises 引发错误】 | ------| TypeError | If DatetimeIndex is tz-naive. | | tz_localize(self, tz, ambiguous=&#39;raise&#39;) | Localize tz-naive DatetimeIndex to given time zone (using pytz/dateutil), | or remove timezone from tz-aware DatetimeIndex | | 【参数】 | ----------| tz : string, pytz.timezone, dateutil.tz.tzfile or None | Time zone for time. Corresponding timestamps would be converted to 154 | time zone of the TimeSeries. | None will remove timezone holding local time. | ambiguous : &#39;infer&#39;, bool-ndarray, &#39;NaT&#39;, default &#39;raise&#39; | - &#39;infer&#39; will attempt to infer fall dst-transition hours based on order | - bool-ndarray where True signifies a DST time, False signifies | a non-DST time (note that this flag is only applicable for ambiguous times) | - &#39;NaT&#39; will return NaT where there are ambiguous times | - &#39;raise&#39; will raise an AmbiguousTimeError if there are ambiguous times | infer_dst : boolean, default False (DEPRECATED) | Attempt to infer fall dst-transition hours based on order | | 【返回值】 | -------| localized : DatetimeIndex | | 【Raises 引发错误】 | ------| TypeError | If the DatetimeIndex is tz-aware and tz is not None. | | union(self, other) | Specialized union for DatetimeIndex objects. If combine | overlapping ranges with the same DateOffset, will be much | faster than Index.union | | 【参数】 | ----------| other : DatetimeIndex or array-like | | 【返回值】 | -------| y : Index or DatetimeIndex | | union_many(self, others) | A bit of a hack to accelerate unioning a collection of indexes | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, data=None, freq=None, start=None, end=None, periods=None, copy=False, name=None, tz=None, verify_integrity=True, normalize=False, closed=None, ambiguous=&#39;raise&#39;, dtype=None, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | date | Returns numpy array of datetime.date. The date part of the Timestamps. | | day | The days of the datetime | | dayofweek | The day of the week with Monday=0, Sunday=6 | | dayofyear | The ordinal day of the year 155 | | days_in_month | The number of days in the month | | .. versionadded:: 0.16.0 | | daysinmonth | The number of days in the month | | .. versionadded:: 0.16.0 | | dtype | | freq | get/set the frequncy of the Index | | hour | The hours of the datetime | | inferred_type | | is_all_dates | Checks that all the labels are datetime objects | | is_month_end | Logical indicating if last day of month (defined by frequency) | | is_month_start | Logical indicating if first day of month (defined by frequency) | | is_normalized | | is_quarter_end | Logical indicating if last day of quarter (defined by frequency) | | is_quarter_start | Logical indicating if first day of quarter (defined by frequency) | | is_year_end | Logical indicating if last day of year (defined by frequency) | | is_year_start | Logical indicating if first day of year (defined by frequency) | | microsecond | The microseconds of the datetime | | millisecond | The milliseconds of the datetime | | minute | The minutes of the datetime | | month | The month as January=1, December=12 | | nanosecond 156 | The nanoseconds of the datetime | | quarter | The quarter of the date | | second | The seconds of the datetime | | time | Returns numpy array of datetime.time. The time part of the Timestamps. | | tzinfo | Alias for tz attribute | | week | The week ordinal of the year | | weekday | The day of the week with Monday=0, Sunday=6 | | weekofyear | The week ordinal of the year | | year | The year of the datetime | | ----------------------------------------------------------------------| 其他数据、属性定义： | | __hash__ = None | | offset = None | | tz = None | | ----------------------------------------------------------------------| Methods inherited from pandas.tseries.base.DatelikeOps: | | strftime(self, date_format) | Return an array of formatted strings specified by date_format, which | supports the same string format as the python standard library. Details | of the string format can be found in thepython string format doc| https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior__ | | .. versionadded:: 0.17.0 | | 【参数】 | ----------| date_format : str | date format string (e.g. &quot;%Y-%m-%d&quot;) | | 【返回值】 | -------| ndarray of formatted strings | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.tseries.base.DatelikeOps: 157 | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | ----------------------------------------------------------------------| Methods inherited from pandas.tseries.base.DatetimeIndexOpsMixin: | | __contains__(self, key) | | __getitem__(self, key) | | argmax(self, axis=None) | return a ndarray of the maximum argument indexer | | 【参见】 | --------| numpy.ndarray.argmax | | argmin(self, axis=None) | return a ndarray of the minimum argument indexer | | 【参见】 | --------| numpy.ndarray.argmin | | get_duplicates(self) | | groupby(self, f) | | isin(self, values) | Compute boolean array of whether each index value is found in the | passed set of values | | 【参数】 | ----------| values : set or sequence of values | | 【返回值】 | -------| is_contained : ndarray (boolean dtype) | | map(self, f) | # Try to run function on index first, and then on elements of index | # Especially important for group-by functionality | | max(self, axis=None) | return the maximum value of the Index | | 【参见】 | --------| numpy.ndarray.max | | min(self, axis=None) | return the minimum value of the Index 158 | | 【参见】 | --------| numpy.ndarray.min | | repeat(self, repeats, axis=None) | Analogous to ndarray.repeat | | shift(self, n, freq=None) | Specialized shift which produces a DatetimeIndex | | 【参数】 | ----------| n : int | Periods to shift by | freq : DateOffset or timedelta-like, optional | | 【返回值】 | -------| shifted : DatetimeIndex | | sort_values(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | summary(self, name=None) | return a summarized representation | | take(self, indices, axis=0, allow_fill=True, fill_value=None) | Analogous to ndarray.take | | tolist(self) | return a list of the underlying data | | unique(self) | Index.unique with handling for DatetimeIndex/PeriodIndex metadata | | 【返回值】 | -------| result : DatetimeIndex or PeriodIndex | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.tseries.base.DatetimeIndexOpsMixin: | | asobject | | freqstr | return the frequency object as a string if its set, otherwise None | | hasnans | | inferred_freq | | resolution | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.index.Int64Index: | 159 | asi8 | | ----------------------------------------------------------------------| Methods inherited from pandas.core.index.Index: | | __and__(self, other) | | __array__(self, dtype=None) | the array interface, return my values | | __array_wrap__(self, result, context=None) | Gets called after a ufunc | | __bool__ = __nonzero__(self) | | __copy__ = copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | __deepcopy__(self, memo={}) | | __len__(self) | return the length of the Index | | __nonzero__(self) | | __or__(self, other) | | __setitem__(self, key, value) | | __unicode__(self) | Return a string representation for this object. | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | __xor__(self, other) | | argsort(self, *args, **kwargs) | return an ndarray indexer of the underlying data | | 【参见】 | -------- 160 | numpy.ndarray.argsort | | asof(self, label) | For a sorted index, return the most recent label up to and including | the passed label. Return NaN if not found. | | 【参见】 | --------| get_loc : asof is a thin wrapper around get_loc with method=&#39;pad&#39; | | asof_locs(self, where, mask) | where : array of timestamps | mask : array of booleans where data is not NA | | copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | diff = wrapper(*args, **kwargs) | | difference(self, other) | Return a new Index with elements from the index that are not inother. | | This is the sorted set difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| difference : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.difference(idx2) | Int64Index([1, 2], dtype=&#39;int64&#39;) | | drop(self, labels, errors=&#39;raise&#39;) | Make new Index with passed list of labels deleted 161 | | 【参数】 | ----------| labels : array-like | errors : {&#39;ignore&#39;, &#39;raise&#39;}, default &#39;raise&#39; | If &#39;ignore&#39;, suppress error and existing labels are dropped. | | 【返回值】 | -------| dropped : Index | | drop_duplicates(self, keep=&#39;first&#39;) | Return Index with duplicate values removed | | 【参数】 | ----------| | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Drop duplicates except for the first occurrence. | - ``last`` : Drop duplicates except for the last occurrence. | - False : Drop all duplicates. | take_last : deprecated | | | 【返回值】 | -------| deduplicated : Index | | duplicated(self, keep=&#39;first&#39;) | Return boolean np.array denoting duplicate values | | 【参数】 | ----------| keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Mark duplicates as ``True`` except for the first occurrence. | - ``last`` : Mark duplicates as ``True`` except for the last occurrence. | - False : Mark all duplicates as ``True``. | take_last : deprecated | | 【返回值】 | -------| duplicated : np.array | | fillna(self, value=None, downcast=None) | Fill NA/NaN values with the specified value | | 【参数】 | ----------| value : scalar | Scalar value to use to fill holes (e.g. 0). | This value cannot be a list-likes. | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | 162 | 【返回值】 | -------| filled : Index | | format(self, name=False, formatter=None, **kwargs) | Render a string representation of the Index | | get_indexer(self, target, method=None, limit=None, tolerance=None) | Compute indexer and mask for new index given the current index. The | indexer should be then used as an input to ndarray.take to align the | current data to the new index. | | 【参数】 | ----------| target : Index | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied | distances are broken by preferring the larger index value. | limit : int, optional | Maximum number of consecutive labels in ``target`` to match for | inexact matches. | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【示例】 | --------| &gt;&gt;&gt; indexer = index.get_indexer(new_index) | &gt;&gt;&gt; new_values = cur_values.take(indexer) | | 【返回值】 | -------| indexer : ndarray of int | Integers from 0 to n - 1 indicating that the index at these | positions matches the corresponding target values. Missing values | in the target are marked by -1. | | get_indexer_for(self, target, **kwargs) | guaranteed return of an indexer even when non-unique | | get_indexer_non_unique(self, target) | return an indexer suitable for taking from a non unique index | return the labels in the same order as the target, and | return a missing indexer into the target (missing are marked as -1 | in the indexer); target must be an iterable | | get_level_values(self, level) | Return vector of label values for requested level, equal to the length | of the index | | 【参数】 163 | ----------| level : int | | 【返回值】 | -------| values : ndarray | | get_slice_bound(self, label, side, kind) | Calculate slice bound that corresponds to given label. | | Returns leftmost (one-past-the-rightmost if ``side==&#39;right&#39;``) position | of given label. | | 【参数】 | ----------| label : object | side : {&#39;left&#39;, &#39;right&#39;} | kind : string / None, the type of indexer | | get_values(self) | return the underlying data as an ndarray | | holds_integer(self) | | identical(self, other) | Similar to equals, but check that other comparable attributes are | also equal | | is_(self, other) | More flexible, faster check like ``is`` but that works through views | | Note: this is *not* the same as ``Index.identical()``, which checks | that metadata is also the same. | | 【参数】 | ----------| other : object | other object to compare against. | | 【返回值】 | -------| True if both have same underlying data, False otherwise : bool | | is_boolean(self) | | is_categorical(self) | | is_floating(self) | | is_integer(self) | | is_lexsorted_for_tuple(self, tup) | | is_mixed(self) | | is_numeric(self) | 164 | is_object(self) | | order(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | DEPRECATED: use :meth:Index.sort_values| | putmask(self, mask, value) | return a new Index of the values set with the mask | | 【参见】 | --------| numpy.ndarray.putmask | | ravel(self, order=&#39;C&#39;) | return an ndarray of the flattened values of the underlying data | | 【参见】 | --------| numpy.ndarray.ravel | | reindex(self, target, method=None, level=None, limit=None, tolerance=None) | Create index with target&#39;s values (move/add/delete values as necessary) | | 【参数】 | ----------| target : an iterable | | 【返回值】 | -------| new_index : pd.Index | Resulting index | indexer : np.ndarray or None | Indices of output values in original index | | rename(self, name, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| name : str or list | name to set | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | set_names(self, names, level=None, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| names : str or sequence | name(s) to set | level : int or level name, or sequence of int / level names (default None) 165 | If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels) | Otherwise level must be None | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | 【示例】 | --------| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(&#39;foo&#39;) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([&#39;foo&#39;]) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_names([&#39;baz&#39;, &#39;quz&#39;]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;quz&#39;]) | &gt;&gt;&gt; idx.set_names(&#39;baz&#39;, level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;bar&#39;]) | | set_value(self, arr, key, value) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | slice_locs(self, start=None, end=None, step=None, kind=None) | Compute slice locations for input labels. | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, defaults None | If None, defaults to 1 | kind : string, defaults None | | 【返回值】 | -------| start, end : int | | sort(self, *args, **kwargs) | | sortlevel(self, level=None, ascending=True, sort_remaining=None) | For internal compatibility with with the Index API | | Sort the Index. This is for compat with MultiIndex | | 【参数】 | ---------- 166 | ascending : boolean, default True | False to sort in descending order | | level, sort_remaining are compat paramaters | | 【返回值】 | -------| sorted_index : Index | | sym_diff(self, other, result_name=None) | Compute the sorted symmetric difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | result_name : str | | 【返回值】 | -------| sym_diff : Index | | 【注意】 | -----| ``sym_diff`` contains elements that appear in either ``idx1`` or | ``idx2`` but not both. Equivalent to the Index created by | ``(idx1 - idx2) + (idx2 - idx1)`` with duplicates dropped. | | The sorting of a result containing ``NaN`` values is not guaranteed | across Python versions. See GitHub issue #6444. | | 【示例】 | --------| &gt;&gt;&gt; idx1 = Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = Index([2, 3, 4, 5]) | &gt;&gt;&gt; idx1.sym_diff(idx2) | Int64Index([1, 5], dtype=&#39;int64&#39;) | | You can also use the ``^`` operator: | | &gt;&gt;&gt; idx1 ^ idx2 | Int64Index([1, 5], dtype=&#39;int64&#39;) | | to_native_types(self, slicer=None, **kwargs) | slice and dice then format | | view(self, cls=None) | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.index.Index: | | dtype_str | | has_duplicates | | is_monotonic | alias for is_monotonic_increasing (deprecated) | 167 | is_monotonic_decreasing | return if the index is monotonic decreasing (only equal or | decreasing) values. | | is_monotonic_increasing | return if the index is monotonic increasing (only equal or | increasing) values. | | is_unique | | names | | nlevels | | values | return the underlying data as an ndarray | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.index.Index: | | name = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.IndexOpsMixin: | | factorize(self, sort=False, na_sentinel=-1) | Encode the object as an enumerated type or categorical variable | | 【参数】 | ----------| sort : boolean, default False | Sort by values | na_sentinel: int, default -1 | Value to mark &quot;not found&quot; | | 【返回值】 | -------| labels : the indexer to the original array | uniques : the unique Index | | item(self) | return the first element of the underlying data as a python scalar | | memory_usage(self, deep=False) | Memory usage of my values | | 【参数】 | ----------| deep : bool | Introspect the data deeply, interrogate |objectdtypes for system-level memory consumption | | 【返回值】 | -------| bytes used | | 【注意】 168 | -----| Memory usage does not include memory consumed by elements that | are not components of the array if deep=False | | 【参见】 | --------| numpy.ndarray.nbytes | | nunique(self, dropna=True) | Return number of unique elements in the object. | | Excludes NA values by default. | | 【参数】 | ----------| dropna : boolean, default True | Don&#39;t include NaN in the count. | | 【返回值】 | -------| nunique : int | | transpose(self) | return the transpose, which is by definition self | | value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True) | Returns object containing counts of unique values. | | The resulting object will be in descending order so that the | first element is the most frequently-occurring element. | Excludes NA values by default. | | 【参数】 | ----------| normalize : boolean, default False | If True then the object returned will contain the relative | frequencies of the unique values. | sort : boolean, default True | Sort by values | ascending : boolean, default False | Sort in ascending order | bins : integer, optional | Rather than count values, group them into half-open bins, | a convenience for pd.cut, only works with numeric data | dropna : boolean, default True | Don&#39;t include counts of NaN. | | 【返回值】 | -------| counts : Series | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.IndexOpsMixin: | | T | return the transpose, which is by definition self | 169 | base | return the base object if the memory of the underlying data is shared | | data | return the data pointer of the underlying data | | flags | return the ndarray.flags for the underlying data | | itemsize | return the size of the dtype of the item of the underlying data | | nbytes | return the number of bytes in the underlying data | | ndim | return the number of dimensions of the underlying data, by definition 1 | | shape | return a tuple of the shape of the underlying data | | size | return the number of elements in the underlying data | | strides | return the strides of the underlying data | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.base.IndexOpsMixin: | | __array_priority__ = 1000 | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin: | | str = &lt;class &#39;pandas.core.strings.StringMethods&#39;&gt; | Vectorized string functions for Series and Index. NAs stay NA unless | handled otherwise by a particular method. Patterned after Python&#39;s string | methods, with some inspiration from R&#39;s stringr package. | | 【示例】 | --------| &gt;&gt;&gt; s.str.split(&#39;_&#39;) | &gt;&gt;&gt; s.str.replace(&#39;_&#39;, &#39;&#39;) | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. 170 | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. ExcelFile ExcelFile 模块所属：pandas.io.excel: 类定义：ExcelFile(builtins.object) | Class for parsing tabular excel sheets into DataFrame objects. | Uses xlrd. See read_excel for more documentation | | 【参数】 | ----------| io : string, file-like object or xlrd workbook | If a string, expected to be a path to xls or xlsx file | engine: string, default None | If io is not a buffer or path, this must be set to identify io. | Acceptable values are None or xlrd | | 【方法定义】 | | __enter__(self) | | __exit__(self, exc_type, exc_value, traceback) | | __init__(self, io, **kwds) | Initialize self. See help(type(self)) for accurate signature. | | close(self) | close io if necessary | | parse(self, sheetname=0, header=0, skiprows=None, skip_footer=0, index_col=None, parse_cols=None, parse_dates=False, date_parser=None, na_values=None, thousands=None, convert_float=True, has_index_names=None, converters=None, **kwds) | Parse specified sheet(s) into a DataFrame | | Equivalent to read_excel(ExcelFile, ...) See the read_excel | docstring for more info on accepted parameters 171 | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | sheet_names ExcelWriter ExcelWriter 模块所属：pandas.io.excel: 类定义：ExcelWriter(builtins.object) | Class for writing DataFrame objects into excel sheets, default is to use | xlwt for xls, openpyxl for xlsx. See DataFrame.to_excel for typical usage. | | 【参数】 | ----------| path : string | Path to xls or xlsx file. | engine : string (optional) | Engine to use for writing. If None, defaults to | ``io.excel.&lt;extension&gt;.writer``. NOTE: can only be passed as a keyword | argument. | date_format : string, default None | Format string for dates written into Excel files (e.g. &#39;YYYY-MM-DD&#39;) | datetime_format : string, default None | Format string for datetime objects written into Excel files | (e.g. &#39;YYYY-MM-DD HH:MM:SS&#39;) | |【注意】 | -----| For compatibility with CSV writers, ExcelWriter serializes lists | and dicts to strings before writing. | | 【方法定义】 | | __enter__(self) | # Allow use as a contextmanager | | __exit__(self, exc_type, exc_value, traceback) | | __init__(self, path, engine=None, date_format=None, datetime_format=None, **engine_kwargs) | Initialize self. See help(type(self)) for accurate signature. | | close(self) 172 | synonym for save, to make it more file-like | | save(self) | Save workbook to disk. | | write_cells(self, cells, sheet_name=None, startrow=0, startcol=0) | Write given formated cells into Excel an excel sheet | | 【参数】 | ----------| cells : generator | cell of formated data to save to Excel sheet | sheet_name : string, default None | Name of Excel sheet, if None, then use self.cur_sheet | startrow: upper left cell row to dump data frame | startcol: upper left cell column to dump data frame | | ----------------------------------------------------------------------| Class methods defined here: | | check_extension(ext) from abc.ABCMeta | checks that path&#39;s extension against the Writer&#39;s supported | extensions. If it isn&#39;t supported, raises UnsupportedFiletypeError. | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, path, engine=None, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | engine | name of engine | | supported_extensions | extensions that writer engine supports | | ----------------------------------------------------------------------| 其他数据、属性定义： | | __abstractmethods__ = frozenset({&#39;engine&#39;, &#39;save&#39;, &#39;supported_extensio... | | book = None | | curr_sheet = None | | path = None 173 Expr Expr 模块所属：pandas.computation.expr: 类定义：Expr(pandas.core.base.StringMixin) | Object encapsulating an expression. | | 【参数】 | ----------| expr : str | engine : str, optional, default &#39;numexpr&#39; | parser : str, optional, default &#39;pandas&#39; | env : Scope, optional, default None | truediv : bool, optional, default True | level : int, optional, default 2 | | 【方法排序】 | Expr | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __call__(self) | Call self as a function. | | __init__(self, expr, engine=&#39;numexpr&#39;, parser=&#39;pandas&#39;, env=None, truediv=True, level=0) | Initialize self. See help(type(self)) for accurate signature. | | __len__(self) | | __unicode__(self) | | parse(self) | Parse an expression | | ----------------------------------------------------------------------| Data descriptors defined here: | | assigner | | names | Get the names in an expression | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. 174 | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.StringMixin: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) Float64Index Float64Index 模块所属：pandas.core.index: 类定义：Float64Index(NumericIndex) | Immutable ndarray implementing an ordered, sliceable set. The basic object | storing axis labels for all pandas objects. Float64Index is a special case | ofIndexwith purely floating point labels. | | 【参数】 | ----------| data : array-like (1-dimensional) | dtype : NumPy dtype (default: object) | copy : bool | Make a copy of input ndarray | name : object | Name to be stored in the index | |【注意】 | -----| An Float64Index instance can **only** contain hashable objects | | 【方法排序】 | Float64Index | NumericIndex | Index | pandas.core.base.IndexOpsMixin | pandas.core.strings.StringAccessorMixin 175 | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __abs__ = _evaluate_numeric_unary(self) | | __add__ = _evaluate_numeric_binop(self, other) | | __contains__(self, other) | | __floordiv__ = _evaluate_numeric_binop(self, other) | | __inv__ = _evaluate_numeric_unary(self) | | __mul__ = _evaluate_numeric_binop(self, other) | | __neg__ = _evaluate_numeric_unary(self) | | __pos__ = _evaluate_numeric_unary(self) | | __radd__ = _evaluate_numeric_binop(self, other) | | __rfloordiv__ = _evaluate_numeric_binop(self, other) | | __rmul__ = _evaluate_numeric_binop(self, other) | | __rsub__ = _evaluate_numeric_binop(self, other) | | __rtruediv__ = _evaluate_numeric_binop(self, other) | | __sub__ = _evaluate_numeric_binop(self, other) | | __truediv__ = _evaluate_numeric_binop(self, other) | | all(self, other=None) | | any(self, other=None) | | astype(self, dtype) | | equals(self, other) | Determines if two Index objects contain the same elements. | | get_loc(self, key, method=None, tolerance=None) | Get integer location for requested label | | 【参数】 | ----------| key : label | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied | distances are broken by preferring the larger index value. 176 | tolerance : optional | Maximum distance from index value for inexact matches. The value of | the index at the matching location most satisfy the equation | ``abs(index[loc] - key) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【返回值】 | -------| loc : int if unique index, possibly slice or mask if not | | get_value(self, series, key) | we always want to get an index value, never a value | | isin(self, values, level=None) | Compute boolean array of whether each index value is found in the | passed set of values. | | 【参数】 | ----------| values : set or sequence of values | Sought values. | level : str or int, optional | Name or position of the index level to use (if the index is a | MultiIndex). | | 【注意】 | -----| Iflevelis specified: | | - if it is the name of one *and only one* index level, use that level; | - otherwise it should be a number indicating level position. | | 【返回值】 | -------| is_contained : ndarray (boolean dtype) | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=False, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | inferred_type | | is_all_dates | Checks that all the labels are datetime objects | | is_unique | | ----------------------------------------------------------------------| Methods inherited from Index: | | __and__(self, other) 177 | | __array__(self, dtype=None) | the array interface, return my values | | __array_wrap__(self, result, context=None) | Gets called after a ufunc | | __bool__ = __nonzero__(self) | | __copy__ = copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | __deepcopy__(self, memo={}) | | __eq__ = _evaluate_compare(self, other) | | __ge__ = _evaluate_compare(self, other) | | __getitem__(self, key) | Override numpy.ndarray&#39;s __getitem__ method to work as desired. | | This function adds lists and Series as valid boolean indexers | (ndarrays only supports ndarray with dtype=bool). | | If resulting ndim != 1, plain ndarray is returned instead of | correspondingIndexsubclass. | | __gt__ = _evaluate_compare(self, other) | | __hash__(self) | Return hash(self). | | __iadd__ = __add__(self, other) | | __iter__(self) | | __le__ = _evaluate_compare(self, other) | | __len__(self) | return the length of the Index | | __lt__ = _evaluate_compare(self, other) 178 | | __ne__ = _evaluate_compare(self, other) | | __nonzero__(self) | | __or__(self, other) | | __reduce__(self) | helper for pickle | | __setitem__(self, key, value) | | __setstate__(self, state) | Necessary for making this object picklable | | __unicode__(self) | Return a string representation for this object. | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | __xor__(self, other) | | append(self, other) | Append a collection of Index options together | | 【参数】 | ----------| other : Index or list/tuple of indices | | 【返回值】 | -------| appended : Index | | argsort(self, *args, **kwargs) | return an ndarray indexer of the underlying data | | 【参见】 | --------| numpy.ndarray.argsort | | asof(self, label) | For a sorted index, return the most recent label up to and including | the passed label. Return NaN if not found. | | 【参见】 | --------| get_loc : asof is a thin wrapper around get_loc with method=&#39;pad&#39; | | asof_locs(self, where, mask) | where : array of timestamps | mask : array of booleans where data is not NA | | copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | 179 | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | delete(self, loc) | Make new Index with passed location(-s) deleted | | 【返回值】 | -------| new_index : Index | | diff = wrapper(*args, **kwargs) | | difference(self, other) | Return a new Index with elements from the index that are not inother. | | This is the sorted set difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| difference : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.difference(idx2) | Int64Index([1, 2], dtype=&#39;int64&#39;) | | drop(self, labels, errors=&#39;raise&#39;) | Make new Index with passed list of labels deleted | | 【参数】 | ----------| labels : array-like | errors : {&#39;ignore&#39;, &#39;raise&#39;}, default &#39;raise&#39; | If &#39;ignore&#39;, suppress error and existing labels are dropped. | | 【返回值】 | -------| dropped : Index 180 | | drop_duplicates(self, keep=&#39;first&#39;) | Return Index with duplicate values removed | | 【参数】 | ----------| | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Drop duplicates except for the first occurrence. | - ``last`` : Drop duplicates except for the last occurrence. | - False : Drop all duplicates. | take_last : deprecated | | | 【返回值】 | -------| deduplicated : Index | | duplicated(self, keep=&#39;first&#39;) | Return boolean np.array denoting duplicate values | | 【参数】 | ----------| keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Mark duplicates as ``True`` except for the first occurrence. | - ``last`` : Mark duplicates as ``True`` except for the last occurrence. | - False : Mark all duplicates as ``True``. | take_last : deprecated | | 【返回值】 | -------| duplicated : np.array | | fillna(self, value=None, downcast=None) | Fill NA/NaN values with the specified value | | 【参数】 | ----------| value : scalar | Scalar value to use to fill holes (e.g. 0). | This value cannot be a list-likes. | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | | 【返回值】 | -------| filled : Index | | format(self, name=False, formatter=None, **kwargs) | Render a string representation of the Index | | get_duplicates(self) | | get_indexer(self, target, method=None, limit=None, tolerance=None) | Compute indexer and mask for new index given the current index. The 181 | indexer should be then used as an input to ndarray.take to align the | current data to the new index. | | 【参数】 | ----------| target : Index | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied | distances are broken by preferring the larger index value. | limit : int, optional | Maximum number of consecutive labels in ``target`` to match for | inexact matches. | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【示例】 | --------| &gt;&gt;&gt; indexer = index.get_indexer(new_index) | &gt;&gt;&gt; new_values = cur_values.take(indexer) | | 【返回值】 | -------| indexer : ndarray of int | Integers from 0 to n - 1 indicating that the index at these | positions matches the corresponding target values. Missing values | in the target are marked by -1. | | get_indexer_for(self, target, **kwargs) | guaranteed return of an indexer even when non-unique | | get_indexer_non_unique(self, target) | return an indexer suitable for taking from a non unique index | return the labels in the same order as the target, and | return a missing indexer into the target (missing are marked as -1 | in the indexer); target must be an iterable | | get_level_values(self, level) | Return vector of label values for requested level, equal to the length | of the index | | 【参数】 | ----------| level : int | | 【返回值】 | -------| values : ndarray | | get_slice_bound(self, label, side, kind) | Calculate slice bound that corresponds to given label. 182 | | Returns leftmost (one-past-the-rightmost if ``side==&#39;right&#39;``) position | of given label. | | 【参数】 | ----------| label : object | side : {&#39;left&#39;, &#39;right&#39;} | kind : string / None, the type of indexer | | get_values(self) | return the underlying data as an ndarray | | groupby(self, to_groupby) | Group the index labels by a given array of values. | | 【参数】 | ----------| to_groupby : array | Values used to determine the groups. | | 【返回值】 | -------| groups : dict | {group name -&gt; group labels} | | holds_integer(self) | | identical(self, other) | Similar to equals, but check that other comparable attributes are | also equal | | insert(self, loc, item) | Make new Index inserting new item at location. Follows | Python list.append semantics for negative values | | 【参数】 | ----------| loc : int | item : object | | 【返回值】 | -------| new_index : Index | | intersection(self, other) | Form the intersection of two Index objects. | | This returns a new Index with elements common to the index andother. | Sortedness of the result is not guaranteed. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | ------- 183 | intersection : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.intersection(idx2) | Int64Index([3, 4], dtype=&#39;int64&#39;) | | is_(self, other) | More flexible, faster check like ``is`` but that works through views | | Note: this is *not* the same as ``Index.identical()``, which checks | that metadata is also the same. | | 【参数】 | ----------| other : object | other object to compare against. | | 【返回值】 | -------| True if both have same underlying data, False otherwise : bool | | is_boolean(self) | | is_categorical(self) | | is_floating(self) | | is_integer(self) | | is_lexsorted_for_tuple(self, tup) | | is_mixed(self) | | is_numeric(self) | | is_object(self) | | is_type_compatible(self, kind) | | join(self, other, how=&#39;left&#39;, level=None, return_indexers=False) | *this is an internal non-public method* | | Compute join_index and indexers to conform data | structures to the new index. | | 【参数】 | ----------| other : Index | how : {&#39;left&#39;, &#39;right&#39;, &#39;inner&#39;, &#39;outer&#39;} | level : int or level name, default None | return_indexers : boolean, default False | | 【返回值】 184 | -------| join_index, (left_indexer, right_indexer) | | map(self, mapper) | | order(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | DEPRECATED: use :meth:Index.sort_values| | putmask(self, mask, value) | return a new Index of the values set with the mask | | 【参见】 | --------| numpy.ndarray.putmask | | ravel(self, order=&#39;C&#39;) | return an ndarray of the flattened values of the underlying data | | 【参见】 | --------| numpy.ndarray.ravel | | reindex(self, target, method=None, level=None, limit=None, tolerance=None) | Create index with target&#39;s values (move/add/delete values as necessary) | | 【参数】 | ----------| target : an iterable | | 【返回值】 | -------| new_index : pd.Index | Resulting index | indexer : np.ndarray or None | Indices of output values in original index | | rename(self, name, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| name : str or list | name to set | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | repeat(self, n) | return a new Index of the values repeated n times | | 【参见】 | -------- 185 | numpy.ndarray.repeat | | set_names(self, names, level=None, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| names : str or sequence | name(s) to set | level : int or level name, or sequence of int / level names (default None) | If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels) | Otherwise level must be None | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | 【示例】 | --------| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(&#39;foo&#39;) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([&#39;foo&#39;]) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_names([&#39;baz&#39;, &#39;quz&#39;]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;quz&#39;]) | &gt;&gt;&gt; idx.set_names(&#39;baz&#39;, level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;bar&#39;]) | | set_value(self, arr, key, value) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | shift(self, periods=1, freq=None) | Shift Index containing datetime objects by input number of periods and | DateOffset | | 【返回值】 | -------| shifted : Index | | slice_indexer(self, start=None, end=None, step=None, kind=None) | For an ordered Index, compute the slice indexer for input labels and | step | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning 186 | end : label, default None | If None, defaults to the end | step : int, default None | kind : string, default None | | 【返回值】 | -------| indexer : ndarray or slice | | 【注意】 | -----| This function assumes that the data is sorted, so use at your own peril | | slice_locs(self, start=None, end=None, step=None, kind=None) | Compute slice locations for input labels. | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, defaults None | If None, defaults to 1 | kind : string, defaults None | | 【返回值】 | -------| start, end : int | | sort(self, *args, **kwargs) | | sort_values(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | sortlevel(self, level=None, ascending=True, sort_remaining=None) | For internal compatibility with with the Index API | | Sort the Index. This is for compat with MultiIndex | | 【参数】 | ----------| ascending : boolean, default True | False to sort in descending order | | level, sort_remaining are compat paramaters | | 【返回值】 | -------| sorted_index : Index | | summary(self, name=None) | | sym_diff(self, other, result_name=None) | Compute the sorted symmetric difference of two Index objects. | | 【参数】 187 | ----------| other : Index or array-like | result_name : str | | 【返回值】 | -------| sym_diff : Index | | 【注意】 | -----| ``sym_diff`` contains elements that appear in either ``idx1`` or | ``idx2`` but not both. Equivalent to the Index created by | ``(idx1 - idx2) + (idx2 - idx1)`` with duplicates dropped. | | The sorting of a result containing ``NaN`` values is not guaranteed | across Python versions. See GitHub issue #6444. | | 【示例】 | --------| &gt;&gt;&gt; idx1 = Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = Index([2, 3, 4, 5]) | &gt;&gt;&gt; idx1.sym_diff(idx2) | Int64Index([1, 5], dtype=&#39;int64&#39;) | | You can also use the ``^`` operator: | | &gt;&gt;&gt; idx1 ^ idx2 | Int64Index([1, 5], dtype=&#39;int64&#39;) | | take(self, indices, axis=0, allow_fill=True, fill_value=None) | return a new Index of the values selected by the indexer | | For internal compatibility with numpy arrays. | | # filling must always be None/nan here | # but is passed thru internally | | 【参见】 | --------| numpy.ndarray.take | | to_datetime(self, dayfirst=False) | For an Index containing strings or datetime.datetime objects, attempt | conversion to DatetimeIndex | | to_native_types(self, slicer=None, **kwargs) | slice and dice then format | | to_series(self, **kwargs) | Create a Series with both index and values equal to the index keys | useful with map for returning an indexer based on an index | | 【返回值】 | -------| Series : dtype will be based on the type of the Index values. | | tolist(self) 188 | return a list of the Index values | | union(self, other) | Form the union of two Index objects and sorts if possible. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| union : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.union(idx2) | Int64Index([1, 2, 3, 4, 5, 6], dtype=&#39;int64&#39;) | | view(self, cls=None) | | ----------------------------------------------------------------------| Data descriptors inherited from Index: | | dtype | | dtype_str | | has_duplicates | | hasnans | | is_monotonic | alias for is_monotonic_increasing (deprecated) | | is_monotonic_decreasing | return if the index is monotonic decreasing (only equal or | decreasing) values. | | is_monotonic_increasing | return if the index is monotonic increasing (only equal or | increasing) values. | | names | | nlevels | | values | return the underlying data as an ndarray | | ----------------------------------------------------------------------| Data and other attributes inherited from Index: | | asi8 = None | 189 | name = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.IndexOpsMixin: | | argmax(self, axis=None) | return a ndarray of the maximum argument indexer | | 【参见】 | --------| numpy.ndarray.argmax | | argmin(self, axis=None) | return a ndarray of the minimum argument indexer | | 【参见】 | --------| numpy.ndarray.argmin | | factorize(self, sort=False, na_sentinel=-1) | Encode the object as an enumerated type or categorical variable | | 【参数】 | ----------| sort : boolean, default False | Sort by values | na_sentinel: int, default -1 | Value to mark &quot;not found&quot; | | 【返回值】 | -------| labels : the indexer to the original array | uniques : the unique Index | | item(self) | return the first element of the underlying data as a python scalar | | max(self) | The maximum value of the object | | memory_usage(self, deep=False) | Memory usage of my values | | 【参数】 | ----------| deep : bool | Introspect the data deeply, interrogate |objectdtypes for system-level memory consumption | | 【返回值】 | -------| bytes used | | 【注意】 | -----| Memory usage does not include memory consumed by elements that | are not components of the array if deep=False 190 | | 【参见】 | --------| numpy.ndarray.nbytes | | min(self) | The minimum value of the object | | nunique(self, dropna=True) | Return number of unique elements in the object. | | Excludes NA values by default. | | 【参数】 | ----------| dropna : boolean, default True | Don&#39;t include NaN in the count. | | 【返回值】 | -------| nunique : int | | searchsorted(self, key, side=&#39;left&#39;) | np.ndarray searchsorted compat | | transpose(self) | return the transpose, which is by definition self | | unique(self) | Return array of unique values in the object. Significantly faster than | numpy.unique. Includes NA values. | | 【返回值】 | -------| uniques : ndarray | | value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True) | Returns object containing counts of unique values. | | The resulting object will be in descending order so that the | first element is the most frequently-occurring element. | Excludes NA values by default. | | 【参数】 | ----------| normalize : boolean, default False | If True then the object returned will contain the relative | frequencies of the unique values. | sort : boolean, default True | Sort by values | ascending : boolean, default False | Sort in ascending order | bins : integer, optional | Rather than count values, group them into half-open bins, | a convenience for pd.cut, only works with numeric data | dropna : boolean, default True | Don&#39;t include counts of NaN. 191 | | 【返回值】 | -------| counts : Series | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.IndexOpsMixin: | | T | return the transpose, which is by definition self | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | base | return the base object if the memory of the underlying data is shared | | data | return the data pointer of the underlying data | | flags | return the ndarray.flags for the underlying data | | itemsize | return the size of the dtype of the item of the underlying data | | nbytes | return the number of bytes in the underlying data | | ndim | return the number of dimensions of the underlying data, by definition 1 | | shape | return a tuple of the shape of the underlying data | | size | return the number of elements in the underlying data | | strides | return the strides of the underlying data | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.base.IndexOpsMixin: | | __array_priority__ = 1000 | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin: | | str = &lt;class &#39;pandas.core.strings.StringMethods&#39;&gt; | Vectorized string functions for Series and Index. NAs stay NA unless | handled otherwise by a particular method. Patterned after Python&#39;s string | methods, with some inspiration from R&#39;s stringr package. | 192 | 【示例】 | --------| &gt;&gt;&gt; s.str.split(&#39;_&#39;) | &gt;&gt;&gt; s.str.replace(&#39;_&#39;, &#39;&#39;) | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. Grouper Grouper 模块所属：pandas.core.groupby: 类定义：Grouper(builtins.object) | A Grouper allows the user to specify a groupby instruction for a target object | | This specification will select a column via the key parameter, or if the level and/or | axis parameters are given, a level of the index of the target object. | | These are local specifications and will override &#39;global&#39; settings, that is the parameters | axis and level which are passed to the groupby itself. | | 【参数】 | ----------| key : string, defaults to None | groupby key, which selects the grouping column of the target | level : name/number, defaults to None 193 | the level for the target index | freq : string / frequency object, defaults to None | This will groupby the specified frequency if the target selection (via key or level) is | a datetime-like object. For full specification of available frequencies, please see |here http://pandas.pydata.org/pandas-docs/stable/timeseries.html_. | axis : number/name of the axis, defaults to 0 | sort : boolean, default to False | whether to sort the resulting labels | | additional kwargs to control time-like groupers (when freq is passed) | | closed : closed end of interval; left or right | label : interval boundary to use for labeling; left or right | convention : {&#39;start&#39;, &#39;end&#39;, &#39;e&#39;, &#39;s&#39;} | If grouper is PeriodIndex | | Returns | -------| A specification for a groupby instruction | | 【示例】 | --------| | Syntactic sugar for ``df.groupby(&#39;A&#39;)`` | | &gt;&gt;&gt; df.groupby(Grouper(key=&#39;A&#39;)) | | Specify a resample operation on the column &#39;date&#39; | | &gt;&gt;&gt; df.groupby(Grouper(key=&#39;date&#39;, freq=&#39;60s&#39;)) | | Specify a resample operation on the level &#39;date&#39; on the columns axis | with a frequency of 60s | | &gt;&gt;&gt; df.groupby(Grouper(level=&#39;date&#39;, freq=&#39;60s&#39;, axis=1)) | | 【方法定义】 | | __init__(self, key=None, level=None, freq=None, axis=0, sort=False) | Initialize self. See help(type(self)) for accurate signature. | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, *args, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | ax 194 | | groups HDFStore HDFStore 模块所属：pandas.io.pytables: 类定义：HDFStore(pandas.core.base.StringMixin) | dict-like IO interface for storing pandas objects in PyTables | either Fixed or Table format. | | 【参数】 | ----------| path : string | File path to HDF5 file | mode : {&#39;a&#39;, &#39;w&#39;, &#39;r&#39;, &#39;r+&#39;}, default &#39;a&#39; | | ``&#39;r&#39;`` | Read-only; no data can be modified. | ``&#39;w&#39;`` | Write; a new file is created (an existing file with the same | name would be deleted). | ``&#39;a&#39;`` | Append; an existing file is opened for reading and writing, | and if the file does not exist it is created. | ``&#39;r+&#39;`` | It is similar to ``&#39;a&#39;``, but the file must already exist. | complevel : int, 1-9, default 0 | If a complib is specified compression will be applied | where possible | complib : {&#39;zlib&#39;, &#39;bzip2&#39;, &#39;lzo&#39;, &#39;blosc&#39;, None}, default None | If complevel is &gt; 0 apply compression to objects written | in the store wherever possible | fletcher32 : bool, default False | If applying compression use the fletcher32 checksum | | 【示例】 | --------| &gt;&gt;&gt; from pandas import DataFrame | &gt;&gt;&gt; from numpy.random import randn | &gt;&gt;&gt; bar = DataFrame(randn(10, 4)) | &gt;&gt;&gt; store = HDFStore(&#39;test.h5&#39;) | &gt;&gt;&gt; store[&#39;foo&#39;] = bar # write to HDF5 | &gt;&gt;&gt; bar = store[&#39;foo&#39;] # retrieve | &gt;&gt;&gt; store.close() | | 【方法排序】 | HDFStore | pandas.core.base.StringMixin | 【内置对象】 195 | | 【方法定义】 | | __contains__(self, key) | check for existance of this key | can match the exact pathname or the pathnm w/o the leading &#39;/&#39; | | __delitem__(self, key) | | __enter__(self) | | __exit__(self, exc_type, exc_value, traceback) | | __getattr__(self, name) | allow attribute access to get stores | | __getitem__(self, key) | | __init__(self, path, mode=None, complevel=None, complib=None, fletcher32=False, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __len__(self) | | __setitem__(self, key, value) | | __unicode__(self) | | append(self, key, value, format=None, append=True, columns=None, dropna=None, **kwargs) | Append to Table in file. Node must already exist and be Table | format. | | 【参数】 | ----------| key : object | value : {Series, DataFrame, Panel, Panel4D} | format: &#39;table&#39; is the default | table(t) : table format | Write as a PyTables Table structure which may perform | worse but allow more flexible operations like searching | / selecting subsets of the data | append : boolean, default True, append the input data to the | existing | data_columns : list of columns to create as data columns, or True to | use all columns | min_itemsize : dict of columns that specify minimum string sizes | nan_rep : string to use as string nan represenation | chunksize : size to chunk the writing | expectedrows : expected TOTAL row size of this table | encoding : default None, provide an encoding for strings | dropna : boolean, default False, do not write an ALL nan row to | the store settable by the option &#39;io.hdf.dropna_table&#39; | 【注意】 | -----| Does *not* check if data being appended overlaps with existing | data in the table, so be careful | | append_to_multiple(self, d, value, selector, data_columns=None, axes=None, dropna=False, **kwargs) 196 | Append to multiple tables | | 【参数】 | ----------| d : a dict of table_name to table_columns, None is acceptable as the | values of one node (this will get all the remaining columns) | value : a pandas object | selector : a string that designates the indexable table; all of its | columns will be designed as data_columns, unless data_columns is | passed, in which case these are used | data_columns : list of columns to create as data columns, or True to | use all columns | dropna : if evaluates to True, drop rows from all tables if any single | row in each table has all NaN. Default False. | | 【注意】 | -----| axes parameter is currently not accepted | | close(self) | Close the PyTables file handle | | copy(self, file, mode=&#39;w&#39;, propindexes=True, keys=None, complib=None, complevel=None, fletcher32=False, overwrite=True) | copy the existing store to a new file, upgrading in place | | 【参数】 | ----------| propindexes: restore indexes in copied file (defaults to True) | keys : list of keys to include in the copy (defaults to all) | overwrite : overwrite (remove and replace) existing nodes in the | new store (default is True) | mode, complib, complevel, fletcher32 same as in HDFStore.__init__ | | 【返回值】 | -------| open file handle of the new store | | create_table_index(self, key, **kwargs) | Create a pytables index on the table | Paramaters | ----------| key : object (the node to index) | | Exceptions | ----------| raises if the node is not a table | | flush(self, fsync=False) | Force all buffered modifications to be written to disk. | | 【参数】 | ----------| fsync : bool (default False) | call ``os.fsync()`` on the file handle to force writing to disk. | | 【注意】 197 | -----| Without ``fsync=True``, flushing may not guarantee that the OS writes | to disk. With fsync, the operation will block until the OS claims the | file has been written; however, other caching layers may still | interfere. | | get(self, key) | Retrieve pandas object stored in file | | 【参数】 | ----------| key : object | | 【返回值】 | -------| obj : type of object stored in file | | get_node(self, key) | return the node with the key or None if it does not exist | | get_storer(self, key) | return the storer object for a key, raise if not in the file | | groups(self) | return a list of all the top-level nodes (that are not themselves a | pandas storage object) | | items(self) | iterate on key-&gt;group | | iteritems = items(self) | | keys(self) | Return a (potentially unordered) list of the keys corresponding to the | objects stored in the HDFStore. These are ABSOLUTE path-names (e.g. | have the leading &#39;/&#39; | | open(self, mode=&#39;a&#39;, **kwargs) | Open the file in the specified mode | | 【参数】 | ----------| mode : {&#39;a&#39;, &#39;w&#39;, &#39;r&#39;, &#39;r+&#39;}, default &#39;a&#39; | See HDFStore docstring or tables.open_file for info about modes | | put(self, key, value, format=None, append=False, **kwargs) | Store object in HDFStore | | 【参数】 | ----------| key : object | value : {Series, DataFrame, Panel} | format : &#39;fixed(f)|table(t)&#39;, default is &#39;fixed&#39; | fixed(f) : Fixed format | Fast writing/reading. Not-appendable, nor searchable | table(t) : Table format | Write as a PyTables Table structure which may perform 198 | worse but allow more flexible operations like searching | / selecting subsets of the data | append : boolean, default False | This will force Table format, append the input data to the | existing. | encoding : default None, provide an encoding for strings | dropna : boolean, default False, do not write an ALL nan row to | the store settable by the option &#39;io.hdf.dropna_table&#39; | | remove(self, key, where=None, start=None, stop=None) | Remove pandas object partially by specifying the where condition | | 【参数】 | ----------| key : string | Node to remove or delete rows from | where : list of Term (or convertable) objects, optional | start : integer (defaults to None), row number to start selection | stop : integer (defaults to None), row number to stop selection | | 【返回值】 | -------| number of rows removed (or None if not a Table) | | Exceptions | ----------| raises KeyError if key is not a valid store | | select(self, key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, auto_close=False, **kwargs) | Retrieve pandas object stored in file, optionally based on where | criteria | | 【参数】 | ----------| key : object | where : list of Term (or convertable) objects, optional | start : integer (defaults to None), row number to start selection | stop : integer (defaults to None), row number to stop selection | columns : a list of columns that if not None, will limit the return | columns | iterator : boolean, return an iterator, default False | chunksize : nrows to include in iteration, return an iterator | auto_close : boolean, should automatically close the store when | finished, default is False | | 【返回值】 | -------| The selected object | | select_as_coordinates(self, key, where=None, start=None, stop=None, **kwargs) | return the selection as an Index | | 【参数】 | ----------| key : object | where : list of Term (or convertable) objects, optional 199 | start : integer (defaults to None), row number to start selection | stop : integer (defaults to None), row number to stop selection | | select_as_multiple(self, keys, where=None, selector=None, columns=None, start=None, stop=None, iterator=False, chunksize=None, auto_close=False, **kwargs) | Retrieve pandas objects from multiple tables | | 【参数】 | ----------| keys : a list of the tables | selector : the table to apply the where criteria (defaults to keys[0] | if not supplied) | columns : the columns I want back | start : integer (defaults to None), row number to start selection | stop : integer (defaults to None), row number to stop selection | iterator : boolean, return an iterator, default False | chunksize : nrows to include in iteration, return an iterator | | Exceptions | ----------| raises KeyError if keys or selector is not found or keys is empty | raises TypeError if keys is not a list or tuple | raises ValueError if the tables are not ALL THE SAME DIMENSIONS | | select_column(self, key, column, **kwargs) | return a single column from the table. This is generally only useful to | select an indexable | | 【参数】 | ----------| key : object | column: the column of interest | | Exceptions | ----------| raises KeyError if the column is not found (or key is not a valid | store) | raises ValueError if the column can not be extracted individually (it | is part of a data block) | | ----------------------------------------------------------------------| Data descriptors defined here: | | filename | | is_open | return a boolean indicating whether the file is open | | root | return the root node | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | 200 | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.StringMixin: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) Index Index 模块所属：pandas.core.index: 类定义：Index(pandas.core.base.IndexOpsMixin, pandas.core.strings.StringAccessorMixin, pandas.core.base.PandasObject) | Immutable ndarray implementing an ordered, sliceable set. The basic object | storing axis labels for all pandas objects | | 【参数】 | ----------| data : array-like (1-dimensional) | dtype : NumPy dtype (default: object) | copy : bool | Make a copy of input ndarray | name : object | Name to be stored in the index | tupleize_cols : bool (default: True) | When True, attempt to create a MultiIndex if possible | |【注意】 | -----| An Index instance can **only** contain hashable objects | | 【方法排序】 | Index | pandas.core.base.IndexOpsMixin | pandas.core.strings.StringAccessorMixin 201 | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __abs__(self, other=None) | | __add__(self, other) | | __and__(self, other) | | __array__(self, dtype=None) | the array interface, return my values | | __array_wrap__(self, result, context=None) | Gets called after a ufunc | | __bool__ = __nonzero__(self) | | __contains__(self, key) | | __copy__ = copy(self, names=None, name=None, dtype=None, deep=False) | | __deepcopy__(self, memo={}) | | __eq__ = _evaluate_compare(self, other) | | __floordiv__(self, other=None) | | __ge__ = _evaluate_compare(self, other) | | __getitem__(self, key) | Override numpy.ndarray&#39;s __getitem__ method to work as desired. | | This function adds lists and Series as valid boolean indexers | (ndarrays only supports ndarray with dtype=bool). | | If resulting ndim != 1, plain ndarray is returned instead of | correspondingIndexsubclass. | | __gt__ = _evaluate_compare(self, other) | | __hash__(self) | Return hash(self). | | __iadd__ = __add__(self, other) | | __inv__(self, other=None) | | __iter__(self) | | __le__ = _evaluate_compare(self, other) | | __len__(self) | return the length of the Index | 202 | __lt__ = _evaluate_compare(self, other) | | __mul__(self, other=None) | | __ne__ = _evaluate_compare(self, other) | | __neg__(self, other=None) | | __nonzero__(self) | | __or__(self, other) | | __pos__(self, other=None) | | __radd__(self, other) | | __reduce__(self) | helper for pickle | | __rfloordiv__ = __floordiv__(self, other=None) | | __rmul__ = __mul__(self, other=None) | | __rtruediv__ = __truediv__(self, other=None) | | __setitem__(self, key, value) | | __setstate__(self, state) | Necessary for making this object picklable | | __sub__(self, other) | | __truediv__(self, other=None) | | __unicode__(self) | Return a string representation for this object. | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | __xor__(self, other) | | all(self, *args, **kwargs) | Return whether all elements are True | | 【参数】 | ----------| All arguments to numpy.all are accepted. | | 【返回值】 | -------| all : bool or array_like (if axis is specified) | A single element array_like may be converted to bool. | | any(self, *args, **kwargs) | Return whether any element is True | 203 | 【参数】 | ----------| All arguments to numpy.any are accepted. | | 【返回值】 | -------| any : bool or array_like (if axis is specified) | A single element array_like may be converted to bool. | | append(self, other) | Append a collection of Index options together | | 【参数】 | ----------| other : Index or list/tuple of indices | | 【返回值】 | -------| appended : Index | | argsort(self, *args, **kwargs) | return an ndarray indexer of the underlying data | | 【参见】 | --------| numpy.ndarray.argsort | | asof(self, label) | For a sorted index, return the most recent label up to and including | the passed label. Return NaN if not found. | | 【参见】 | --------| get_loc : asof is a thin wrapper around get_loc with method=&#39;pad&#39; | | asof_locs(self, where, mask) | where : array of timestamps | mask : array of booleans where data is not NA | | astype(self, dtype) | | copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | ----- 204 | In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | delete(self, loc) | Make new Index with passed location(-s) deleted | | 【返回值】 | -------| new_index : Index | | diff = wrapper(*args, **kwargs) | | difference(self, other) | Return a new Index with elements from the index that are not inother. | | This is the sorted set difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| difference : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.difference(idx2) | Int64Index([1, 2], dtype=&#39;int64&#39;) | | drop(self, labels, errors=&#39;raise&#39;) | Make new Index with passed list of labels deleted | | 【参数】 | ----------| labels : array-like | errors : {&#39;ignore&#39;, &#39;raise&#39;}, default &#39;raise&#39; | If &#39;ignore&#39;, suppress error and existing labels are dropped. | | 【返回值】 | -------| dropped : Index | | drop_duplicates(self, keep=&#39;first&#39;) | Return Index with duplicate values removed | | 【参数】 | ----------| | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Drop duplicates except for the first occurrence. | - ``last`` : Drop duplicates except for the last occurrence. | - False : Drop all duplicates. | take_last : deprecated 205 | | | 【返回值】 | -------| deduplicated : Index | | duplicated(self, keep=&#39;first&#39;) | Return boolean np.array denoting duplicate values | | 【参数】 | ----------| keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Mark duplicates as ``True`` except for the first occurrence. | - ``last`` : Mark duplicates as ``True`` except for the last occurrence. | - False : Mark all duplicates as ``True``. | take_last : deprecated | | 【返回值】 | -------| duplicated : np.array | | equals(self, other) | Determines if two Index objects contain the same elements. | | fillna(self, value=None, downcast=None) | Fill NA/NaN values with the specified value | | 【参数】 | ----------| value : scalar | Scalar value to use to fill holes (e.g. 0). | This value cannot be a list-likes. | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | | 【返回值】 | -------| filled : Index | | format(self, name=False, formatter=None, **kwargs) | Render a string representation of the Index | | get_duplicates(self) | | get_indexer(self, target, method=None, limit=None, tolerance=None) | Compute indexer and mask for new index given the current index. The | indexer should be then used as an input to ndarray.take to align the | current data to the new index. | | 【参数】 | ----------| target : Index | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. 206 | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied | distances are broken by preferring the larger index value. | limit : int, optional | Maximum number of consecutive labels in ``target`` to match for | inexact matches. | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【示例】 | --------| &gt;&gt;&gt; indexer = index.get_indexer(new_index) | &gt;&gt;&gt; new_values = cur_values.take(indexer) | | 【返回值】 | -------| indexer : ndarray of int | Integers from 0 to n - 1 indicating that the index at these | positions matches the corresponding target values. Missing values | in the target are marked by -1. | | get_indexer_for(self, target, **kwargs) | guaranteed return of an indexer even when non-unique | | get_indexer_non_unique(self, target) | return an indexer suitable for taking from a non unique index | return the labels in the same order as the target, and | return a missing indexer into the target (missing are marked as -1 | in the indexer); target must be an iterable | | get_level_values(self, level) | Return vector of label values for requested level, equal to the length | of the index | | 【参数】 | ----------| level : int | | 【返回值】 | -------| values : ndarray | | get_loc(self, key, method=None, tolerance=None) | Get integer location for requested label | | 【参数】 | ----------| key : label | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied 207 | distances are broken by preferring the larger index value. | tolerance : optional | Maximum distance from index value for inexact matches. The value of | the index at the matching location most satisfy the equation | ``abs(index[loc] - key) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【返回值】 | -------| loc : int if unique index, possibly slice or mask if not | | get_slice_bound(self, label, side, kind) | Calculate slice bound that corresponds to given label. | | Returns leftmost (one-past-the-rightmost if ``side==&#39;right&#39;``) position | of given label. | | 【参数】 | ----------| label : object | side : {&#39;left&#39;, &#39;right&#39;} | kind : string / None, the type of indexer | | get_value(self, series, key) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | get_values(self) | return the underlying data as an ndarray | | groupby(self, to_groupby) | Group the index labels by a given array of values. | | 【参数】 | ----------| to_groupby : array | Values used to determine the groups. | | 【返回值】 | -------| groups : dict | {group name -&gt; group labels} | | holds_integer(self) | | identical(self, other) | Similar to equals, but check that other comparable attributes are | also equal | | insert(self, loc, item) | Make new Index inserting new item at location. Follows | Python list.append semantics for negative values | | 【参数】 | ----------| loc : int 208 | item : object | | 【返回值】 | -------| new_index : Index | | intersection(self, other) | Form the intersection of two Index objects. | | This returns a new Index with elements common to the index andother. | Sortedness of the result is not guaranteed. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| intersection : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.intersection(idx2) | Int64Index([3, 4], dtype=&#39;int64&#39;) | | is_(self, other) | More flexible, faster check like ``is`` but that works through views | | Note: this is *not* the same as ``Index.identical()``, which checks | that metadata is also the same. | | 【参数】 | ----------| other : object | other object to compare against. | | 【返回值】 | -------| True if both have same underlying data, False otherwise : bool | | is_boolean(self) | | is_categorical(self) | | is_floating(self) | | is_integer(self) | | is_lexsorted_for_tuple(self, tup) | | is_mixed(self) | | is_numeric(self) | 209 | is_object(self) | | is_type_compatible(self, kind) | | isin(self, values, level=None) | Compute boolean array of whether each index value is found in the | passed set of values. | | 【参数】 | ----------| values : set or sequence of values | Sought values. | level : str or int, optional | Name or position of the index level to use (if the index is a | MultiIndex). | | 【注意】 | -----| Iflevelis specified: | | - if it is the name of one *and only one* index level, use that level; | - otherwise it should be a number indicating level position. | | 【返回值】 | -------| is_contained : ndarray (boolean dtype) | | join(self, other, how=&#39;left&#39;, level=None, return_indexers=False) | *this is an internal non-public method* | | Compute join_index and indexers to conform data | structures to the new index. | | 【参数】 | ----------| other : Index | how : {&#39;left&#39;, &#39;right&#39;, &#39;inner&#39;, &#39;outer&#39;} | level : int or level name, default None | return_indexers : boolean, default False | | 【返回值】 | -------| join_index, (left_indexer, right_indexer) | | map(self, mapper) | | order(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | DEPRECATED: use :meth:Index.sort_values| | putmask(self, mask, value) | return a new Index of the values set with the mask | | 【参见】 | --------| numpy.ndarray.putmask 210 | | ravel(self, order=&#39;C&#39;) | return an ndarray of the flattened values of the underlying data | | 【参见】 | --------| numpy.ndarray.ravel | | reindex(self, target, method=None, level=None, limit=None, tolerance=None) | Create index with target&#39;s values (move/add/delete values as necessary) | | 【参数】 | ----------| target : an iterable | | 【返回值】 | -------| new_index : pd.Index | Resulting index | indexer : np.ndarray or None | Indices of output values in original index | | rename(self, name, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| name : str or list | name to set | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | repeat(self, n) | return a new Index of the values repeated n times | | 【参见】 | --------| numpy.ndarray.repeat | | set_names(self, names, level=None, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| names : str or sequence | name(s) to set | level : int or level name, or sequence of int / level names (default None) | If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels) | Otherwise level must be None | inplace : bool | if True, mutates in place | 211 | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | 【示例】 | --------| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(&#39;foo&#39;) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([&#39;foo&#39;]) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_names([&#39;baz&#39;, &#39;quz&#39;]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;quz&#39;]) | &gt;&gt;&gt; idx.set_names(&#39;baz&#39;, level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;bar&#39;]) | | set_value(self, arr, key, value) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | shift(self, periods=1, freq=None) | Shift Index containing datetime objects by input number of periods and | DateOffset | | 【返回值】 | -------| shifted : Index | | slice_indexer(self, start=None, end=None, step=None, kind=None) | For an ordered Index, compute the slice indexer for input labels and | step | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, default None | kind : string, default None | | 【返回值】 | -------| indexer : ndarray or slice | | 【注意】 | -----| This function assumes that the data is sorted, so use at your own peril | | slice_locs(self, start=None, end=None, step=None, kind=None) | Compute slice locations for input labels. 212 | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, defaults None | If None, defaults to 1 | kind : string, defaults None | | 【返回值】 | -------| start, end : int | | sort(self, *args, **kwargs) | | sort_values(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | sortlevel(self, level=None, ascending=True, sort_remaining=None) | For internal compatibility with with the Index API | | Sort the Index. This is for compat with MultiIndex | | 【参数】 | ----------| ascending : boolean, default True | False to sort in descending order | | level, sort_remaining are compat paramaters | | 【返回值】 | -------| sorted_index : Index | | summary(self, name=None) | | sym_diff(self, other, result_name=None) | Compute the sorted symmetric difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | result_name : str | | 【返回值】 | -------| sym_diff : Index | | 【注意】 | -----| ``sym_diff`` contains elements that appear in either ``idx1`` or | ``idx2`` but not both. Equivalent to the Index created by | ``(idx1 - idx2) + (idx2 - idx1)`` with duplicates dropped. | | The sorting of a result containing ``NaN`` values is not guaranteed 213 | across Python versions. See GitHub issue #6444. | | 【示例】 | --------| &gt;&gt;&gt; idx1 = Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = Index([2, 3, 4, 5]) | &gt;&gt;&gt; idx1.sym_diff(idx2) | Int64Index([1, 5], dtype=&#39;int64&#39;) | | You can also use the ``^`` operator: | | &gt;&gt;&gt; idx1 ^ idx2 | Int64Index([1, 5], dtype=&#39;int64&#39;) | | take(self, indices, axis=0, allow_fill=True, fill_value=None) | return a new Index of the values selected by the indexer | | For internal compatibility with numpy arrays. | | # filling must always be None/nan here | # but is passed thru internally | | 【参见】 | --------| numpy.ndarray.take | | to_datetime(self, dayfirst=False) | For an Index containing strings or datetime.datetime objects, attempt | conversion to DatetimeIndex | | to_native_types(self, slicer=None, **kwargs) | slice and dice then format | | to_series(self, **kwargs) | Create a Series with both index and values equal to the index keys | useful with map for returning an indexer based on an index | | 【返回值】 | -------| Series : dtype will be based on the type of the Index values. | | tolist(self) | return a list of the Index values | | union(self, other) | Form the union of two Index objects and sorts if possible. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| union : Index | | 【示例】 | -------- 214 | | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.union(idx2) | Int64Index([1, 2, 3, 4, 5, 6], dtype=&#39;int64&#39;) | | view(self, cls=None) | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=False, tupleize_cols=True, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | dtype | | dtype_str | | has_duplicates | | hasnans | | inferred_type | | is_all_dates | | is_monotonic | alias for is_monotonic_increasing (deprecated) | | is_monotonic_decreasing | return if the index is monotonic decreasing (only equal or | decreasing) values. | | is_monotonic_increasing | return if the index is monotonic increasing (only equal or | increasing) values. | | is_unique | | names | | nlevels | | values | return the underlying data as an ndarray | | ----------------------------------------------------------------------| 其他数据、属性定义： | | asi8 = None | | name = None | | ---------------------------------------------------------------------- 215 | Methods inherited from pandas.core.base.IndexOpsMixin: | | argmax(self, axis=None) | return a ndarray of the maximum argument indexer | | 【参见】 | --------| numpy.ndarray.argmax | | argmin(self, axis=None) | return a ndarray of the minimum argument indexer | | 【参见】 | --------| numpy.ndarray.argmin | | factorize(self, sort=False, na_sentinel=-1) | Encode the object as an enumerated type or categorical variable | | 【参数】 | ----------| sort : boolean, default False | Sort by values | na_sentinel: int, default -1 | Value to mark &quot;not found&quot; | | 【返回值】 | -------| labels : the indexer to the original array | uniques : the unique Index | | item(self) | return the first element of the underlying data as a python scalar | | max(self) | The maximum value of the object | | memory_usage(self, deep=False) | Memory usage of my values | | 【参数】 | ----------| deep : bool | Introspect the data deeply, interrogate |objectdtypes for system-level memory consumption | | 【返回值】 | -------| bytes used | | 【注意】 | -----| Memory usage does not include memory consumed by elements that | are not components of the array if deep=False | | 【参见】 216 | --------| numpy.ndarray.nbytes | | min(self) | The minimum value of the object | | nunique(self, dropna=True) | Return number of unique elements in the object. | | Excludes NA values by default. | | 【参数】 | ----------| dropna : boolean, default True | Don&#39;t include NaN in the count. | | 【返回值】 | -------| nunique : int | | searchsorted(self, key, side=&#39;left&#39;) | np.ndarray searchsorted compat | | transpose(self) | return the transpose, which is by definition self | | unique(self) | Return array of unique values in the object. Significantly faster than | numpy.unique. Includes NA values. | | 【返回值】 | -------| uniques : ndarray | | value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True) | Returns object containing counts of unique values. | | The resulting object will be in descending order so that the | first element is the most frequently-occurring element. | Excludes NA values by default. | | 【参数】 | ----------| normalize : boolean, default False | If True then the object returned will contain the relative | frequencies of the unique values. | sort : boolean, default True | Sort by values | ascending : boolean, default False | Sort in ascending order | bins : integer, optional | Rather than count values, group them into half-open bins, | a convenience for pd.cut, only works with numeric data | dropna : boolean, default True | Don&#39;t include counts of NaN. | | 【返回值】 217 | -------| counts : Series | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.IndexOpsMixin: | | T | return the transpose, which is by definition self | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | base | return the base object if the memory of the underlying data is shared | | data | return the data pointer of the underlying data | | flags | return the ndarray.flags for the underlying data | | itemsize | return the size of the dtype of the item of the underlying data | | nbytes | return the number of bytes in the underlying data | | ndim | return the number of dimensions of the underlying data, by definition 1 | | shape | return a tuple of the shape of the underlying data | | size | return the number of elements in the underlying data | | strides | return the strides of the underlying data | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.base.IndexOpsMixin: | | __array_priority__ = 1000 | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin: | | str = &lt;class &#39;pandas.core.strings.StringMethods&#39;&gt; | Vectorized string functions for Series and Index. NAs stay NA unless | handled otherwise by a particular method. Patterned after Python&#39;s string | methods, with some inspiration from R&#39;s stringr package. | | 【示例】 | -------- 218 | &gt;&gt;&gt; s.str.split(&#39;_&#39;) | &gt;&gt;&gt; s.str.replace(&#39;_&#39;, &#39;&#39;) | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. IndexSlice _IndexSlice 模块所属：pandas.core.indexing object: 类定义：_IndexSlice(builtins.object) | # the public IndexSlicerMaker | | 【方法定义】 | | __getitem__(self, arg) | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) 219 Int64Index Int64Index 模块所属：pandas.core.index: 类定义：Int64Index(NumericIndex) | Immutable ndarray implementing an ordered, sliceable set. The basic object | storing axis labels for all pandas objects. Int64Index is a special case | ofIndexwith purely integer labels. This is the default index type used | by the DataFrame and Series ctors when no explicit index is provided by the | user. | | 【参数】 | ----------| data : array-like (1-dimensional) | dtype : NumPy dtype (default: int64) | copy : bool | Make a copy of input ndarray | name : object | Name to be stored in the index | |【注意】 | -----| An Index instance can **only** contain hashable objects | | 【方法排序】 | Int64Index | NumericIndex | Index | pandas.core.base.IndexOpsMixin | pandas.core.strings.StringAccessorMixin | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __abs__ = _evaluate_numeric_unary(self) | | __add__ = _evaluate_numeric_binop(self, other) | | __floordiv__ = _evaluate_numeric_binop(self, other) | | __inv__ = _evaluate_numeric_unary(self) | | __mul__ = _evaluate_numeric_binop(self, other) | | __neg__ = _evaluate_numeric_unary(self) | | __pos__ = _evaluate_numeric_unary(self) | 220 | __radd__ = _evaluate_numeric_binop(self, other) | | __rfloordiv__ = _evaluate_numeric_binop(self, other) | | __rmul__ = _evaluate_numeric_binop(self, other) | | __rsub__ = _evaluate_numeric_binop(self, other) | | __rtruediv__ = _evaluate_numeric_binop(self, other) | | __sub__ = _evaluate_numeric_binop(self, other) | | __truediv__ = _evaluate_numeric_binop(self, other) | | all(self, *args, **kwargs) | Return whether all elements are True | | 【参数】 | ----------| All arguments to numpy.all are accepted. | | 【返回值】 | -------| all : bool or array_like (if axis is specified) | A single element array_like may be converted to bool. | | any(self, *args, **kwargs) | Return whether any element is True | | 【参数】 | ----------| All arguments to numpy.any are accepted. | | 【返回值】 | -------| any : bool or array_like (if axis is specified) | A single element array_like may be converted to bool. | | equals(self, other) | Determines if two Index objects contain the same elements. | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=False, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | asi8 | | inferred_type | | is_all_dates | Checks that all the labels are datetime objects | 221 | ----------------------------------------------------------------------| Methods inherited from Index: | | __and__(self, other) | | __array__(self, dtype=None) | the array interface, return my values | | __array_wrap__(self, result, context=None) | Gets called after a ufunc | | __bool__ = __nonzero__(self) | | __contains__(self, key) | | __copy__ = copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | __deepcopy__(self, memo={}) | | __eq__ = _evaluate_compare(self, other) | | __ge__ = _evaluate_compare(self, other) | | __getitem__(self, key) | Override numpy.ndarray&#39;s __getitem__ method to work as desired. | | This function adds lists and Series as valid boolean indexers | (ndarrays only supports ndarray with dtype=bool). | | If resulting ndim != 1, plain ndarray is returned instead of | correspondingIndexsubclass. | | __gt__ = _evaluate_compare(self, other) | | __hash__(self) | Return hash(self). | | __iadd__ = __add__(self, other) | | __iter__(self) | 222 | __le__ = _evaluate_compare(self, other) | | __len__(self) | return the length of the Index | | __lt__ = _evaluate_compare(self, other) | | __ne__ = _evaluate_compare(self, other) | | __nonzero__(self) | | __or__(self, other) | | __reduce__(self) | helper for pickle | | __setitem__(self, key, value) | | __setstate__(self, state) | Necessary for making this object picklable | | __unicode__(self) | Return a string representation for this object. | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | __xor__(self, other) | | append(self, other) | Append a collection of Index options together | | 【参数】 | ----------| other : Index or list/tuple of indices | | 【返回值】 | -------| appended : Index | | argsort(self, *args, **kwargs) | return an ndarray indexer of the underlying data | | 【参见】 | --------| numpy.ndarray.argsort | | asof(self, label) | For a sorted index, return the most recent label up to and including | the passed label. Return NaN if not found. | | 【参见】 | --------| get_loc : asof is a thin wrapper around get_loc with method=&#39;pad&#39; | | asof_locs(self, where, mask) | where : array of timestamps 223 | mask : array of booleans where data is not NA | | astype(self, dtype) | | copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | delete(self, loc) | Make new Index with passed location(-s) deleted | | 【返回值】 | -------| new_index : Index | | diff = wrapper(*args, **kwargs) | | difference(self, other) | Return a new Index with elements from the index that are not inother. | | This is the sorted set difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| difference : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.difference(idx2) | Int64Index([1, 2], dtype=&#39;int64&#39;) | | drop(self, labels, errors=&#39;raise&#39;) | Make new Index with passed list of labels deleted | | 【参数】 224 | ----------| labels : array-like | errors : {&#39;ignore&#39;, &#39;raise&#39;}, default &#39;raise&#39; | If &#39;ignore&#39;, suppress error and existing labels are dropped. | | 【返回值】 | -------| dropped : Index | | drop_duplicates(self, keep=&#39;first&#39;) | Return Index with duplicate values removed | | 【参数】 | ----------| | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Drop duplicates except for the first occurrence. | - ``last`` : Drop duplicates except for the last occurrence. | - False : Drop all duplicates. | take_last : deprecated | | | 【返回值】 | -------| deduplicated : Index | | duplicated(self, keep=&#39;first&#39;) | Return boolean np.array denoting duplicate values | | 【参数】 | ----------| keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Mark duplicates as ``True`` except for the first occurrence. | - ``last`` : Mark duplicates as ``True`` except for the last occurrence. | - False : Mark all duplicates as ``True``. | take_last : deprecated | | 【返回值】 | -------| duplicated : np.array | | fillna(self, value=None, downcast=None) | Fill NA/NaN values with the specified value | | 【参数】 | ----------| value : scalar | Scalar value to use to fill holes (e.g. 0). | This value cannot be a list-likes. | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | | 【返回值】 | -------| filled : Index 225 | | format(self, name=False, formatter=None, **kwargs) | Render a string representation of the Index | | get_duplicates(self) | | get_indexer(self, target, method=None, limit=None, tolerance=None) | Compute indexer and mask for new index given the current index. The | indexer should be then used as an input to ndarray.take to align the | current data to the new index. | | 【参数】 | ----------| target : Index | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied | distances are broken by preferring the larger index value. | limit : int, optional | Maximum number of consecutive labels in ``target`` to match for | inexact matches. | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【示例】 | --------| &gt;&gt;&gt; indexer = index.get_indexer(new_index) | &gt;&gt;&gt; new_values = cur_values.take(indexer) | | 【返回值】 | -------| indexer : ndarray of int | Integers from 0 to n - 1 indicating that the index at these | positions matches the corresponding target values. Missing values | in the target are marked by -1. | | get_indexer_for(self, target, **kwargs) | guaranteed return of an indexer even when non-unique | | get_indexer_non_unique(self, target) | return an indexer suitable for taking from a non unique index | return the labels in the same order as the target, and | return a missing indexer into the target (missing are marked as -1 | in the indexer); target must be an iterable | | get_level_values(self, level) | Return vector of label values for requested level, equal to the length | of the index | | 【参数】 | ---------- 226 | level : int | | 【返回值】 | -------| values : ndarray | | get_loc(self, key, method=None, tolerance=None) | Get integer location for requested label | | 【参数】 | ----------| key : label | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied | distances are broken by preferring the larger index value. | tolerance : optional | Maximum distance from index value for inexact matches. The value of | the index at the matching location most satisfy the equation | ``abs(index[loc] - key) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【返回值】 | -------| loc : int if unique index, possibly slice or mask if not | | get_slice_bound(self, label, side, kind) | Calculate slice bound that corresponds to given label. | | Returns leftmost (one-past-the-rightmost if ``side==&#39;right&#39;``) position | of given label. | | 【参数】 | ----------| label : object | side : {&#39;left&#39;, &#39;right&#39;} | kind : string / None, the type of indexer | | get_value(self, series, key) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | get_values(self) | return the underlying data as an ndarray | | groupby(self, to_groupby) | Group the index labels by a given array of values. | | 【参数】 | ----------| to_groupby : array | Values used to determine the groups. | | 【返回值】 227 | -------| groups : dict | {group name -&gt; group labels} | | holds_integer(self) | | identical(self, other) | Similar to equals, but check that other comparable attributes are | also equal | | insert(self, loc, item) | Make new Index inserting new item at location. Follows | Python list.append semantics for negative values | | 【参数】 | ----------| loc : int | item : object | | 【返回值】 | -------| new_index : Index | | intersection(self, other) | Form the intersection of two Index objects. | | This returns a new Index with elements common to the index andother. | Sortedness of the result is not guaranteed. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| intersection : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.intersection(idx2) | Int64Index([3, 4], dtype=&#39;int64&#39;) | | is_(self, other) | More flexible, faster check like ``is`` but that works through views | | Note: this is *not* the same as ``Index.identical()``, which checks | that metadata is also the same. | | 【参数】 | ----------| other : object | other object to compare against. | | 【返回值】 228 | -------| True if both have same underlying data, False otherwise : bool | | is_boolean(self) | | is_categorical(self) | | is_floating(self) | | is_integer(self) | | is_lexsorted_for_tuple(self, tup) | | is_mixed(self) | | is_numeric(self) | | is_object(self) | | is_type_compatible(self, kind) | | isin(self, values, level=None) | Compute boolean array of whether each index value is found in the | passed set of values. | | 【参数】 | ----------| values : set or sequence of values | Sought values. | level : str or int, optional | Name or position of the index level to use (if the index is a | MultiIndex). | | 【注意】 | -----| Iflevelis specified: | | - if it is the name of one *and only one* index level, use that level; | - otherwise it should be a number indicating level position. | | 【返回值】 | -------| is_contained : ndarray (boolean dtype) | | join(self, other, how=&#39;left&#39;, level=None, return_indexers=False) | *this is an internal non-public method* | | Compute join_index and indexers to conform data | structures to the new index. | | 【参数】 | ----------| other : Index | how : {&#39;left&#39;, &#39;right&#39;, &#39;inner&#39;, &#39;outer&#39;} | level : int or level name, default None | return_indexers : boolean, default False | 229 | 【返回值】 | -------| join_index, (left_indexer, right_indexer) | | map(self, mapper) | | order(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | DEPRECATED: use :meth:Index.sort_values| | putmask(self, mask, value) | return a new Index of the values set with the mask | | 【参见】 | --------| numpy.ndarray.putmask | | ravel(self, order=&#39;C&#39;) | return an ndarray of the flattened values of the underlying data | | 【参见】 | --------| numpy.ndarray.ravel | | reindex(self, target, method=None, level=None, limit=None, tolerance=None) | Create index with target&#39;s values (move/add/delete values as necessary) | | 【参数】 | ----------| target : an iterable | | 【返回值】 | -------| new_index : pd.Index | Resulting index | indexer : np.ndarray or None | Indices of output values in original index | | rename(self, name, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| name : str or list | name to set | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | repeat(self, n) | return a new Index of the values repeated n times | 230 | 【参见】 | --------| numpy.ndarray.repeat | | set_names(self, names, level=None, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| names : str or sequence | name(s) to set | level : int or level name, or sequence of int / level names (default None) | If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels) | Otherwise level must be None | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | 【示例】 | --------| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(&#39;foo&#39;) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([&#39;foo&#39;]) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_names([&#39;baz&#39;, &#39;quz&#39;]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;quz&#39;]) | &gt;&gt;&gt; idx.set_names(&#39;baz&#39;, level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;bar&#39;]) | | set_value(self, arr, key, value) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | shift(self, periods=1, freq=None) | Shift Index containing datetime objects by input number of periods and | DateOffset | | 【返回值】 | -------| shifted : Index | | slice_indexer(self, start=None, end=None, step=None, kind=None) | For an ordered Index, compute the slice indexer for input labels and | step | | 【参数】 | ---------- 231 | start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, default None | kind : string, default None | | 【返回值】 | -------| indexer : ndarray or slice | | 【注意】 | -----| This function assumes that the data is sorted, so use at your own peril | | slice_locs(self, start=None, end=None, step=None, kind=None) | Compute slice locations for input labels. | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, defaults None | If None, defaults to 1 | kind : string, defaults None | | 【返回值】 | -------| start, end : int | | sort(self, *args, **kwargs) | | sort_values(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | sortlevel(self, level=None, ascending=True, sort_remaining=None) | For internal compatibility with with the Index API | | Sort the Index. This is for compat with MultiIndex | | 【参数】 | ----------| ascending : boolean, default True | False to sort in descending order | | level, sort_remaining are compat paramaters | | 【返回值】 | -------| sorted_index : Index | | summary(self, name=None) | | sym_diff(self, other, result_name=None) | Compute the sorted symmetric difference of two Index objects. 232 | | 【参数】 | ----------| other : Index or array-like | result_name : str | | 【返回值】 | -------| sym_diff : Index | | 【注意】 | -----| ``sym_diff`` contains elements that appear in either ``idx1`` or | ``idx2`` but not both. Equivalent to the Index created by | ``(idx1 - idx2) + (idx2 - idx1)`` with duplicates dropped. | | The sorting of a result containing ``NaN`` values is not guaranteed | across Python versions. See GitHub issue #6444. | | 【示例】 | --------| &gt;&gt;&gt; idx1 = Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = Index([2, 3, 4, 5]) | &gt;&gt;&gt; idx1.sym_diff(idx2) | Int64Index([1, 5], dtype=&#39;int64&#39;) | | You can also use the ``^`` operator: | | &gt;&gt;&gt; idx1 ^ idx2 | Int64Index([1, 5], dtype=&#39;int64&#39;) | | take(self, indices, axis=0, allow_fill=True, fill_value=None) | return a new Index of the values selected by the indexer | | For internal compatibility with numpy arrays. | | # filling must always be None/nan here | # but is passed thru internally | | 【参见】 | --------| numpy.ndarray.take | | to_datetime(self, dayfirst=False) | For an Index containing strings or datetime.datetime objects, attempt | conversion to DatetimeIndex | | to_native_types(self, slicer=None, **kwargs) | slice and dice then format | | to_series(self, **kwargs) | Create a Series with both index and values equal to the index keys | useful with map for returning an indexer based on an index | | 【返回值】 | -------| Series : dtype will be based on the type of the Index values. 233 | | tolist(self) | return a list of the Index values | | union(self, other) | Form the union of two Index objects and sorts if possible. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| union : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.union(idx2) | Int64Index([1, 2, 3, 4, 5, 6], dtype=&#39;int64&#39;) | | view(self, cls=None) | | ----------------------------------------------------------------------| Data descriptors inherited from Index: | | dtype | | dtype_str | | has_duplicates | | hasnans | | is_monotonic | alias for is_monotonic_increasing (deprecated) | | is_monotonic_decreasing | return if the index is monotonic decreasing (only equal or | decreasing) values. | | is_monotonic_increasing | return if the index is monotonic increasing (only equal or | increasing) values. | | is_unique | | names | | nlevels | | values | return the underlying data as an ndarray | | ---------------------------------------------------------------------- 234 | Data and other attributes inherited from Index: | | name = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.IndexOpsMixin: | | argmax(self, axis=None) | return a ndarray of the maximum argument indexer | | 【参见】 | --------| numpy.ndarray.argmax | | argmin(self, axis=None) | return a ndarray of the minimum argument indexer | | 【参见】 | --------| numpy.ndarray.argmin | | factorize(self, sort=False, na_sentinel=-1) | Encode the object as an enumerated type or categorical variable | | 【参数】 | ----------| sort : boolean, default False | Sort by values | na_sentinel: int, default -1 | Value to mark &quot;not found&quot; | | 【返回值】 | -------| labels : the indexer to the original array | uniques : the unique Index | | item(self) | return the first element of the underlying data as a python scalar | | max(self) | The maximum value of the object | | memory_usage(self, deep=False) | Memory usage of my values | | 【参数】 | ----------| deep : bool | Introspect the data deeply, interrogate |objectdtypes for system-level memory consumption | | 【返回值】 | -------| bytes used | | 【注意】 | ----- 235 | Memory usage does not include memory consumed by elements that | are not components of the array if deep=False | | 【参见】 | --------| numpy.ndarray.nbytes | | min(self) | The minimum value of the object | | nunique(self, dropna=True) | Return number of unique elements in the object. | | Excludes NA values by default. | | 【参数】 | ----------| dropna : boolean, default True | Don&#39;t include NaN in the count. | | 【返回值】 | -------| nunique : int | | searchsorted(self, key, side=&#39;left&#39;) | np.ndarray searchsorted compat | | transpose(self) | return the transpose, which is by definition self | | unique(self) | Return array of unique values in the object. Significantly faster than | numpy.unique. Includes NA values. | | 【返回值】 | -------| uniques : ndarray | | value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True) | Returns object containing counts of unique values. | | The resulting object will be in descending order so that the | first element is the most frequently-occurring element. | Excludes NA values by default. | | 【参数】 | ----------| normalize : boolean, default False | If True then the object returned will contain the relative | frequencies of the unique values. | sort : boolean, default True | Sort by values | ascending : boolean, default False | Sort in ascending order | bins : integer, optional | Rather than count values, group them into half-open bins, | a convenience for pd.cut, only works with numeric data 236 | dropna : boolean, default True | Don&#39;t include counts of NaN. | | 【返回值】 | -------| counts : Series | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.IndexOpsMixin: | | T | return the transpose, which is by definition self | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | base | return the base object if the memory of the underlying data is shared | | data | return the data pointer of the underlying data | | flags | return the ndarray.flags for the underlying data | | itemsize | return the size of the dtype of the item of the underlying data | | nbytes | return the number of bytes in the underlying data | | ndim | return the number of dimensions of the underlying data, by definition 1 | | shape | return a tuple of the shape of the underlying data | | size | return the number of elements in the underlying data | | strides | return the strides of the underlying data | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.base.IndexOpsMixin: | | __array_priority__ = 1000 | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin: | | str = &lt;class &#39;pandas.core.strings.StringMethods&#39;&gt; | Vectorized string functions for Series and Index. NAs stay NA unless | handled otherwise by a particular method. Patterned after Python&#39;s string 237 | methods, with some inspiration from R&#39;s stringr package. | | 【示例】 | --------| &gt;&gt;&gt; s.str.split(&#39;_&#39;) | &gt;&gt;&gt; s.str.replace(&#39;_&#39;, &#39;&#39;) | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. LooseVersion LooseVersion in module distutils.version: 类定义：LooseVersion(Version) | Version numbering for anarchists and software realists. | Implements the standard interface for version number classes as | described above. A version number consists of a series of numbers, | separated by either periods or strings of letters. When comparing | version numbers, the numeric components will be compared | numerically, and the alphabetic components lexically. The following | are all valid version numbers, in no particular order: | | 1.5.1 | 1.5.2b2 | 161 238 | 3.10a | 8.02 | 3.4j | 1996.07.12 | 3.2.pl0 | 3.1.1.6 | 2g6 | 11g | 0.960923 | 2.2beta29 | 1.13++ | 5.5.kw | 2.0b1pl0 | | In fact, there is no such thing as an invalid version number under | this scheme; the rules for comparison are simple and predictable, | but may not always give the results you want (for some definition | of &quot;want&quot;). | | 【方法排序】 | LooseVersion | Version | 【内置对象】 | | 【方法定义】 | | __init__(self, vstring=None) | Initialize self. See help(type(self)) for accurate signature. | | __repr__(self) | Return repr(self). | | __str__(self) | Return str(self). | | parse(self, vstring) | | ----------------------------------------------------------------------| 其他数据、属性定义： | | component_re = re.compile(&#39;(\\d+ | [a-z]+ | \\.)&#39;, re.VERBOSE) | | ----------------------------------------------------------------------| Methods inherited from Version: | | __eq__(self, other) | Return self==value. | | __ge__(self, other) | Return self&gt;=value. | | __gt__(self, other) | Return self&gt;value. | | __le__(self, other) | Return self&lt;=value. | 239 | __lt__(self, other) | Return self&lt;value. | | ----------------------------------------------------------------------| Data descriptors inherited from Version: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | ----------------------------------------------------------------------| Data and other attributes inherited from Version: | | __hash__ = None MultiIndex MultiIndex 模块所属：pandas.core.index: 类定义：MultiIndex(Index) | Implements multi-level, a.k.a. hierarchical, index object for pandas | objects | | 【参数】 | ----------| levels : sequence of arrays | The unique labels for each level | labels : sequence of arrays | Integers for each level designating which label at each location | sortorder : optional int | Level of sortedness (must be lexicographically sorted by that | level) | names : optional sequence of objects | Names for each of the index levels. (name is accepted for compat) | copy : boolean, default False | Copy the meta-data | verify_integrity : boolean, default True | Check that the levels/labels are consistent and valid | | 【方法排序】 | MultiIndex | Index | pandas.core.base.IndexOpsMixin | pandas.core.strings.StringAccessorMixin | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | 240 | 【方法定义】 | | __abs__(self, other=None) | | __array__(self, dtype=None) | the array interface, return my values | | __contains__(self, key) | | __floordiv__(self, other=None) | | __getitem__(self, key) | Override numpy.ndarray&#39;s __getitem__ method to work as desired. | | This function adds lists and Series as valid boolean indexers | (ndarrays only supports ndarray with dtype=bool). | | If resulting ndim != 1, plain ndarray is returned instead of | correspondingIndexsubclass. | | __getslice__(self, i, j) | | __inv__(self, other=None) | | __len__(self) | return the length of the Index | | __mul__(self, other=None) | | __neg__(self, other=None) | | __pos__(self, other=None) | | __reduce__(self) | Necessary for making this object picklable | | __rfloordiv__ = __floordiv__(self, other=None) | | __rmul__ = __mul__(self, other=None) | | __rtruediv__ = __truediv__(self, other=None) | | __setstate__(self, state) | Necessary for making this object picklable | | __truediv__(self, other=None) | | all(self, other=None) | | any(self, other=None) | | append(self, other) | Append a collection of Index options together | | 【参数】 | ----------| other : Index or list/tuple of indices 241 | | 【返回值】 | -------| appended : Index | | argsort(self, *args, **kwargs) | return an ndarray indexer of the underlying data | | 【参见】 | --------| numpy.ndarray.argsort | | astype(self, dtype) | | copy(self, names=None, dtype=None, levels=None, labels=None, deep=False, _set_identity=False) | Make a copy of this object. Names, dtype, levels and labels can be | passed and will be set on new copy. | | 【参数】 | ----------| names : sequence, optional | dtype : numpy dtype or pandas type, optional | levels : sequence, optional | labels : sequence, optional | | 【返回值】 | -------| copy : MultiIndex | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | This could be potentially expensive on large MultiIndex objects. | | delete(self, loc) | Make new index with passed location deleted | | 【返回值】 | -------| new_index : MultiIndex | | difference(self, other) | Compute sorted set difference of two MultiIndex objects | | 【返回值】 | -------| diff : MultiIndex | | drop(self, labels, level=None, errors=&#39;raise&#39;) | Make new MultiIndex with passed list of labels deleted | | 【参数】 | ----------| labels : array-like | Must be a list of tuples 242 | level : int or level name, default None | | 【返回值】 | -------| dropped : MultiIndex | | droplevel(self, level=0) | Return Index with requested level removed. If MultiIndex has only 2 | levels, the result will be of Index type not MultiIndex. | | 【参数】 | ----------| level : int/level name or list thereof | | 【注意】 | -----| Does not check if result index is unique or not | | 【返回值】 | -------| index : Index or MultiIndex | | duplicated(self, keep=&#39;first&#39;) | Return boolean np.array denoting duplicate values | | 【参数】 | ----------| keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Mark duplicates as ``True`` except for the first occurrence. | - ``last`` : Mark duplicates as ``True`` except for the last occurrence. | - False : Mark all duplicates as ``True``. | take_last : deprecated | | 【返回值】 | -------| duplicated : np.array | | equal_levels(self, other) | Return True if the levels of both MultiIndex objects are the same | | equals(self, other) | Determines if two MultiIndex objects have the same labeling information | (the levels themselves do not necessarily have to be the same) | | 【参见】 | --------| equal_levels | | fillna(self, value=None, downcast=None) | Fill NA/NaN values with the specified value | | 【参数】 | ----------| value : scalar | Scalar value to use to fill holes (e.g. 0). | This value cannot be a list-likes. 243 | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | | 【返回值】 | -------| filled : Index | | format(self, space=2, sparsify=None, adjoin=True, names=False, na_rep=None, formatter=None) | Render a string representation of the Index | | get_indexer(self, target, method=None, limit=None, tolerance=None) | Compute indexer and mask for new index given the current index. The | indexer should be then used as an input to ndarray.take to align the | current data to the new index. The mask determines whether labels are | found or not in the current index | | 【参数】 | ----------| target : MultiIndex or Index (of tuples) | method : {&#39;pad&#39;, &#39;ffill&#39;, &#39;backfill&#39;, &#39;bfill&#39;} | pad / ffill: propagate LAST valid observation forward to next valid | backfill / bfill: use NEXT valid observation to fill gap | | 【注意】 | -----| This is a low-level method and probably should be used at your own risk | | 【示例】 | --------| &gt;&gt;&gt; indexer, mask = index.get_indexer(new_index) | &gt;&gt;&gt; new_values = cur_values.take(indexer) | &gt;&gt;&gt; new_values[-mask] = np.nan | | 【返回值】 | -------| (indexer, mask) : (ndarray, ndarray) | | get_level_values(self, level) | Return vector of label values for requested level, equal to the length | of the index | | 【参数】 | ----------| level : int or level name | | 【返回值】 | -------| values : ndarray | | get_loc(self, key, method=None) | Get integer location, slice or boolean mask for requested label or tuple | If the key is past the lexsort depth, the return may be a boolean mask | array, otherwise it is always a slice or int. | 244 | 【参数】 | ----------| key : label or tuple | method : None | | 【返回值】 | -------| loc : int, slice object or boolean mask | | get_loc_level(self, key, level=0, drop_level=True) | Get integer location slice for requested label or tuple | | 【参数】 | ----------| key : label or tuple | level : int/level name or list thereof | | 【返回值】 | -------| loc : int or slice object | | get_locs(self, tup) | Given a tuple of slices/lists/labels/boolean indexer to a level-wise spec | produce an indexer to extract those locations | | 【参数】 | ----------| key : tuple of (slices/list/labels) | | 【返回值】 | -------| locs : integer list of locations or boolean indexer suitable | for passing to iloc | | get_major_bounds = slice_locs(self, start=None, end=None, step=None, kind=None) | | get_slice_bound(self, label, side, kind) | Calculate slice bound that corresponds to given label. | | Returns leftmost (one-past-the-rightmost if ``side==&#39;right&#39;``) position | of given label. | | 【参数】 | ----------| label : object | side : {&#39;left&#39;, &#39;right&#39;} | kind : string / None, the type of indexer | | get_value(self, series, key) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | insert(self, loc, item) | Make new MultiIndex inserting new item at location | | 【参数】 245 | ----------| loc : int | item : tuple | Must be same length as number of levels in the MultiIndex | | 【返回值】 | -------| new_index : Index | | intersection(self, other) | Form the intersection of two MultiIndex objects, sorting if possible | | 【参数】 | ----------| other : MultiIndex or array / Index of tuples | | 【返回值】 | -------| Index | | is_lexsorted(self) | Return True if the labels are lexicographically sorted | | is_lexsorted_for_tuple(self, tup) | Return True if we are correctly lexsorted given the passed tuple | | isin(self, values, level=None) | Compute boolean array of whether each index value is found in the | passed set of values. | | 【参数】 | ----------| values : set or sequence of values | Sought values. | level : str or int, optional | Name or position of the index level to use (if the index is a | MultiIndex). | | 【注意】 | -----| Iflevelis specified: | | - if it is the name of one *and only one* index level, use that level; | - otherwise it should be a number indicating level position. | | 【返回值】 | -------| is_contained : ndarray (boolean dtype) | | reindex(self, target, method=None, level=None, limit=None, tolerance=None) | Create index with target&#39;s values (move/add/delete values as necessary) | | 【返回值】 | -------| new_index : pd.MultiIndex | Resulting index | indexer : np.ndarray or None 246 | Indices of output values in original index | | rename = set_names(self, names, level=None, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| names : str or sequence | name(s) to set | level : int or level name, or sequence of int / level names (default None) | If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels) | Otherwise level must be None | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | 【示例】 | --------| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(&#39;foo&#39;) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([&#39;foo&#39;]) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_names([&#39;baz&#39;, &#39;quz&#39;]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;quz&#39;]) | &gt;&gt;&gt; idx.set_names(&#39;baz&#39;, level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;bar&#39;]) | | reorder_levels(self, order) | Rearrange levels using input order. May not drop or duplicate levels | | 【参数】 | ----------| | repeat(self, n) | return a new Index of the values repeated n times | | 【参见】 | --------| numpy.ndarray.repeat | | set_labels(self, labels, level=None, inplace=False, verify_integrity=True) | Set new labels on MultiIndex. Defaults to returning | new index. | | 【参数】 | ----------| labels : sequence or list of sequence 247 | new labels to apply | level : int or level name, or sequence of int / level names (default None) | level(s) to set (None for all levels) | inplace : bool | if True, mutates in place | verify_integrity : bool (default True) | if True, checks that levels and labels are compatible | | 【返回值】 | -------| new index (of same type and class...etc) | | 【示例】 | --------| &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_labels([[1,0,1,0], [0,0,1,1]]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[1, 0, 1, 0], [0, 0, 1, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | &gt;&gt;&gt; idx.set_labels([1,0,1,0], level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[1, 0, 1, 0], [0, 1, 0, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | &gt;&gt;&gt; idx.set_labels([0,0,1,1], level=&#39;bar&#39;) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 0, 1, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | &gt;&gt;&gt; idx.set_labels([[1,0,1,0], [0,0,1,1]], level=[0,1]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[1, 0, 1, 0], [0, 0, 1, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | | set_levels(self, levels, level=None, inplace=False, verify_integrity=True) | Set new levels on MultiIndex. Defaults to returning | new index. | | 【参数】 | ----------| levels : sequence or list of sequence | new level(s) to apply | level : int or level name, or sequence of int / level names (default None) | level(s) to set (None for all levels) | inplace : bool | if True, mutates in place | verify_integrity : bool (default True) | if True, checks that levels and labels are compatible | | 【返回值】 | -------| new index (of same type and class...etc) | | | 【示例】 | --------| &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), 248 | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_levels([[&#39;a&#39;,&#39;b&#39;], [1,2]]) | MultiIndex(levels=[[u&#39;a&#39;, u&#39;b&#39;], [1, 2]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | &gt;&gt;&gt; idx.set_levels([&#39;a&#39;,&#39;b&#39;], level=0) | MultiIndex(levels=[[u&#39;a&#39;, u&#39;b&#39;], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | &gt;&gt;&gt; idx.set_levels([&#39;a&#39;,&#39;b&#39;], level=&#39;bar&#39;) | MultiIndex(levels=[[1, 2], [u&#39;a&#39;, u&#39;b&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | &gt;&gt;&gt; idx.set_levels([[&#39;a&#39;,&#39;b&#39;], [1,2]], level=[0,1]) | MultiIndex(levels=[[u&#39;a&#39;, u&#39;b&#39;], [1, 2]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;foo&#39;, u&#39;bar&#39;]) | | slice_locs(self, start=None, end=None, step=None, kind=None) | For an ordered MultiIndex, compute the slice locations for input | labels. They can be tuples representing partial levels, e.g. for a | MultiIndex with 3 levels, you can pass a single value (corresponding to | the first level), or a 1-, 2-, or 3-tuple. | | 【参数】 | ----------| start : label or tuple, default None | If None, defaults to the beginning | end : label or tuple | If None, defaults to the end | step : int or None | Slice step | kind : string, optional, defaults None | | 【返回值】 | -------| (start, end) : (int, int) | | 【注意】 | -----| This function assumes that the data is sorted by the first level | | sortlevel(self, level=0, ascending=True, sort_remaining=True) | Sort MultiIndex at the requested level. The result will respect the | original ordering of the associated factor at that level. | | 【参数】 | ----------| level : list-like, int or str, default 0 | If a string is given, must be a name of the level | If list-like must be names or ints of levels. | ascending : boolean, default True | False to sort in descending order | Can also be a list to specify a directed ordering | sort_remaining : sort by the remaining levels after level. | 249 | 【返回值】 | -------| sorted_index : MultiIndex | | swaplevel(self, i, j) | Swap level i with level j. Do not change the ordering of anything | | 【参数】 | ----------| i, j : int, string (can be mixed) | Level of index to be swapped. Can pass level name as string. | | 【返回值】 | -------| swapped : MultiIndex | | take(self, indexer, axis=None) | return a new Index of the values selected by the indexer | | For internal compatibility with numpy arrays. | | # filling must always be None/nan here | # but is passed thru internally | | 【参见】 | --------| numpy.ndarray.take | | to_hierarchical(self, n_repeat, n_shuffle=1) | Return a MultiIndex reshaped to conform to the | shapes given by n_repeat and n_shuffle. | | Useful to replicate and rearrange a MultiIndex for combination | with another Index with n_repeat items. | | 【参数】 | ----------| n_repeat : int | Number of times to repeat the labels on self | n_shuffle : int | Controls the reordering of the labels. If the result is going | to be an inner level in a MultiIndex, n_shuffle will need to be | greater than one. The size of each label must divisible by | n_shuffle. | | 【返回值】 | -------| MultiIndex | | 【示例】 | --------| &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)]) | &gt;&gt;&gt; idx.to_hierarchical(3) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], | [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1]]) 250 | | truncate(self, before=None, after=None) | Slice index between two labels / tuples, return new MultiIndex | | 【参数】 | ----------| before : label or tuple, can be partial. Default None | None defaults to start | after : label or tuple, can be partial. Default None | None defaults to end | | 【返回值】 | -------| truncated : MultiIndex | | union(self, other) | Form the union of two MultiIndex objects, sorting if possible | | 【参数】 | ----------| other : MultiIndex or array / Index of tuples | | 【返回值】 | -------| Index | | &gt;&gt;&gt; index.union(index2) | | view(self, cls=None) | this is defined as a copy with the same identity | | ----------------------------------------------------------------------| Class methods defined here: | | from_arrays(arrays, sortorder=None, names=None) from builtins.type | Convert arrays to MultiIndex | | 【参数】 | ----------| arrays : list / sequence of array-likes | Each array-like gives one level&#39;s value for each data point. | len(arrays) is the number of levels. | sortorder : int or None | Level of sortedness (must be lexicographically sorted by that | level) | | 【返回值】 | -------| index : MultiIndex | | 【示例】 | --------| &gt;&gt;&gt; arrays = [[1, 1, 2, 2], [&#39;red&#39;, &#39;blue&#39;, &#39;red&#39;, &#39;blue&#39;]] | &gt;&gt;&gt; MultiIndex.from_arrays(arrays, names=(&#39;number&#39;, &#39;color&#39;)) | | 【参见】 251 | --------| MultiIndex.from_tuples : Convert list of tuples to MultiIndex | MultiIndex.from_product : Make a MultiIndex from cartesian product | of iterables | | from_product(iterables, sortorder=None, names=None) from builtins.type | Make a MultiIndex from the cartesian product of multiple iterables | | 【参数】 | ----------| iterables : list / sequence of iterables | Each iterable has unique labels for each level of the index. | sortorder : int or None | Level of sortedness (must be lexicographically sorted by that | level). | names : list / sequence of strings or None | Names for the levels in the index. | | 【返回值】 | -------| index : MultiIndex | | 【示例】 | --------| &gt;&gt;&gt; numbers = [0, 1, 2] | &gt;&gt;&gt; colors = [u&#39;green&#39;, u&#39;purple&#39;] | &gt;&gt;&gt; MultiIndex.from_product([numbers, colors], | names=[&#39;number&#39;, &#39;color&#39;]) | MultiIndex(levels=[[0, 1, 2], [u&#39;green&#39;, u&#39;purple&#39;]], | labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]], | names=[u&#39;number&#39;, u&#39;color&#39;]) | | 【参见】 | --------| MultiIndex.from_arrays : Convert list of arrays to MultiIndex | MultiIndex.from_tuples : Convert list of tuples to MultiIndex | | from_tuples(tuples, sortorder=None, names=None) from builtins.type | Convert list of tuples to MultiIndex | | 【参数】 | ----------| tuples : list / sequence of tuple-likes | Each tuple is the index of one row/column. | sortorder : int or None | Level of sortedness (must be lexicographically sorted by that | level) | | 【返回值】 | -------| index : MultiIndex | | 【示例】 | --------| &gt;&gt;&gt; tuples = [(1, u&#39;red&#39;), (1, u&#39;blue&#39;), | (2, u&#39;red&#39;), (2, u&#39;blue&#39;)] | &gt;&gt;&gt; MultiIndex.from_tuples(tuples, names=(&#39;number&#39;, &#39;color&#39;)) 252 | | 【参见】 | --------| MultiIndex.from_arrays : Convert list of arrays to MultiIndex | MultiIndex.from_product : Make a MultiIndex from cartesian product | of iterables | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, levels=None, labels=None, sortorder=None, names=None, copy=False, verify_integrity=True, _set_identity=True, name=None, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | dtype | | inferred_type | | is_all_dates | | is_unique | | labels | | levels | | levshape | | lexsort_depth | | names | Names of levels in MultiIndex | | nbytes | | nlevels | | values | return the underlying data as an ndarray | | ----------------------------------------------------------------------| Methods inherited from Index: | | __add__(self, other) | | __and__(self, other) | | __array_wrap__(self, result, context=None) | Gets called after a ufunc | | __bool__ = __nonzero__(self) | | __copy__ = copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on 253 | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | __deepcopy__(self, memo={}) | | __eq__ = _evaluate_compare(self, other) | | __ge__ = _evaluate_compare(self, other) | | __gt__ = _evaluate_compare(self, other) | | __hash__(self) | Return hash(self). | | __iadd__ = __add__(self, other) | | __iter__(self) | | __le__ = _evaluate_compare(self, other) | | __lt__ = _evaluate_compare(self, other) | | __ne__ = _evaluate_compare(self, other) | | __nonzero__(self) | | __or__(self, other) | | __radd__(self, other) | | __setitem__(self, key, value) | | __sub__(self, other) | | __unicode__(self) | Return a string representation for this object. | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | __xor__(self, other) | | asof(self, label) | For a sorted index, return the most recent label up to and including 254 | the passed label. Return NaN if not found. | | 【参见】 | --------| get_loc : asof is a thin wrapper around get_loc with method=&#39;pad&#39; | | asof_locs(self, where, mask) | where : array of timestamps | mask : array of booleans where data is not NA | | diff = wrapper(*args, **kwargs) | | drop_duplicates(self, keep=&#39;first&#39;) | Return Index with duplicate values removed | | 【参数】 | ----------| | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Drop duplicates except for the first occurrence. | - ``last`` : Drop duplicates except for the last occurrence. | - False : Drop all duplicates. | take_last : deprecated | | | 【返回值】 | -------| deduplicated : Index | | get_duplicates(self) | | get_indexer_for(self, target, **kwargs) | guaranteed return of an indexer even when non-unique | | get_indexer_non_unique(self, target) | return an indexer suitable for taking from a non unique index | return the labels in the same order as the target, and | return a missing indexer into the target (missing are marked as -1 | in the indexer); target must be an iterable | | get_values(self) | return the underlying data as an ndarray | | groupby(self, to_groupby) | Group the index labels by a given array of values. | | 【参数】 | ----------| to_groupby : array | Values used to determine the groups. | | 【返回值】 | -------| groups : dict | {group name -&gt; group labels} | | holds_integer(self) 255 | | identical(self, other) | Similar to equals, but check that other comparable attributes are | also equal | | is_(self, other) | More flexible, faster check like ``is`` but that works through views | | Note: this is *not* the same as ``Index.identical()``, which checks | that metadata is also the same. | | 【参数】 | ----------| other : object | other object to compare against. | | 【返回值】 | -------| True if both have same underlying data, False otherwise : bool | | is_boolean(self) | | is_categorical(self) | | is_floating(self) | | is_integer(self) | | is_mixed(self) | | is_numeric(self) | | is_object(self) | | is_type_compatible(self, kind) | | join(self, other, how=&#39;left&#39;, level=None, return_indexers=False) | *this is an internal non-public method* | | Compute join_index and indexers to conform data | structures to the new index. | | 【参数】 | ----------| other : Index | how : {&#39;left&#39;, &#39;right&#39;, &#39;inner&#39;, &#39;outer&#39;} | level : int or level name, default None | return_indexers : boolean, default False | | 【返回值】 | -------| join_index, (left_indexer, right_indexer) | | map(self, mapper) | | order(self, return_indexer=False, ascending=True) | Return sorted copy of Index 256 | | DEPRECATED: use :meth:Index.sort_values| | putmask(self, mask, value) | return a new Index of the values set with the mask | | 【参见】 | --------| numpy.ndarray.putmask | | ravel(self, order=&#39;C&#39;) | return an ndarray of the flattened values of the underlying data | | 【参见】 | --------| numpy.ndarray.ravel | | set_names(self, names, level=None, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| names : str or sequence | name(s) to set | level : int or level name, or sequence of int / level names (default None) | If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels) | Otherwise level must be None | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | 【示例】 | --------| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(&#39;foo&#39;) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([&#39;foo&#39;]) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_names([&#39;baz&#39;, &#39;quz&#39;]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;quz&#39;]) | &gt;&gt;&gt; idx.set_names(&#39;baz&#39;, level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;bar&#39;]) | | set_value(self, arr, key, value) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | shift(self, periods=1, freq=None) 257 | Shift Index containing datetime objects by input number of periods and | DateOffset | | 【返回值】 | -------| shifted : Index | | slice_indexer(self, start=None, end=None, step=None, kind=None) | For an ordered Index, compute the slice indexer for input labels and | step | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, default None | kind : string, default None | | 【返回值】 | -------| indexer : ndarray or slice | | 【注意】 | -----| This function assumes that the data is sorted, so use at your own peril | | sort(self, *args, **kwargs) | | sort_values(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | summary(self, name=None) | | sym_diff(self, other, result_name=None) | Compute the sorted symmetric difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | result_name : str | | 【返回值】 | -------| sym_diff : Index | | 【注意】 | -----| ``sym_diff`` contains elements that appear in either ``idx1`` or | ``idx2`` but not both. Equivalent to the Index created by | ``(idx1 - idx2) + (idx2 - idx1)`` with duplicates dropped. | | The sorting of a result containing ``NaN`` values is not guaranteed | across Python versions. See GitHub issue #6444. | 258 | 【示例】 | --------| &gt;&gt;&gt; idx1 = Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = Index([2, 3, 4, 5]) | &gt;&gt;&gt; idx1.sym_diff(idx2) | Int64Index([1, 5], dtype=&#39;int64&#39;) | | You can also use the ``^`` operator: | | &gt;&gt;&gt; idx1 ^ idx2 | Int64Index([1, 5], dtype=&#39;int64&#39;) | | to_datetime(self, dayfirst=False) | For an Index containing strings or datetime.datetime objects, attempt | conversion to DatetimeIndex | | to_native_types(self, slicer=None, **kwargs) | slice and dice then format | | to_series(self, **kwargs) | Create a Series with both index and values equal to the index keys | useful with map for returning an indexer based on an index | | 【返回值】 | -------| Series : dtype will be based on the type of the Index values. | | tolist(self) | return a list of the Index values | | ----------------------------------------------------------------------| Data descriptors inherited from Index: | | dtype_str | | has_duplicates | | hasnans | | is_monotonic | alias for is_monotonic_increasing (deprecated) | | is_monotonic_decreasing | return if the index is monotonic decreasing (only equal or | decreasing) values. | | is_monotonic_increasing | return if the index is monotonic increasing (only equal or | increasing) values. | | ----------------------------------------------------------------------| Data and other attributes inherited from Index: | | asi8 = None | | name = None | 259 | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.IndexOpsMixin: | | argmax(self, axis=None) | return a ndarray of the maximum argument indexer | | 【参见】 | --------| numpy.ndarray.argmax | | argmin(self, axis=None) | return a ndarray of the minimum argument indexer | | 【参见】 | --------| numpy.ndarray.argmin | | factorize(self, sort=False, na_sentinel=-1) | Encode the object as an enumerated type or categorical variable | | 【参数】 | ----------| sort : boolean, default False | Sort by values | na_sentinel: int, default -1 | Value to mark &quot;not found&quot; | | 【返回值】 | -------| labels : the indexer to the original array | uniques : the unique Index | | item(self) | return the first element of the underlying data as a python scalar | | max(self) | The maximum value of the object | | memory_usage(self, deep=False) | Memory usage of my values | | 【参数】 | ----------| deep : bool | Introspect the data deeply, interrogate |objectdtypes for system-level memory consumption | | 【返回值】 | -------| bytes used | | 【注意】 | -----| Memory usage does not include memory consumed by elements that | are not components of the array if deep=False | 260 | 【参见】 | --------| numpy.ndarray.nbytes | | min(self) | The minimum value of the object | | nunique(self, dropna=True) | Return number of unique elements in the object. | | Excludes NA values by default. | | 【参数】 | ----------| dropna : boolean, default True | Don&#39;t include NaN in the count. | | 【返回值】 | -------| nunique : int | | searchsorted(self, key, side=&#39;left&#39;) | np.ndarray searchsorted compat | | transpose(self) | return the transpose, which is by definition self | | unique(self) | Return array of unique values in the object. Significantly faster than | numpy.unique. Includes NA values. | | 【返回值】 | -------| uniques : ndarray | | value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True) | Returns object containing counts of unique values. | | The resulting object will be in descending order so that the | first element is the most frequently-occurring element. | Excludes NA values by default. | | 【参数】 | ----------| normalize : boolean, default False | If True then the object returned will contain the relative | frequencies of the unique values. | sort : boolean, default True | Sort by values | ascending : boolean, default False | Sort in ascending order | bins : integer, optional | Rather than count values, group them into half-open bins, | a convenience for pd.cut, only works with numeric data | dropna : boolean, default True | Don&#39;t include counts of NaN. | 261 | 【返回值】 | -------| counts : Series | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.IndexOpsMixin: | | T | return the transpose, which is by definition self | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | base | return the base object if the memory of the underlying data is shared | | data | return the data pointer of the underlying data | | flags | return the ndarray.flags for the underlying data | | itemsize | return the size of the dtype of the item of the underlying data | | ndim | return the number of dimensions of the underlying data, by definition 1 | | shape | return a tuple of the shape of the underlying data | | size | return the number of elements in the underlying data | | strides | return the strides of the underlying data | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.base.IndexOpsMixin: | | __array_priority__ = 1000 | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin: | | str = &lt;class &#39;pandas.core.strings.StringMethods&#39;&gt; | Vectorized string functions for Series and Index. NAs stay NA unless | handled otherwise by a particular method. Patterned after Python&#39;s string | methods, with some inspiration from R&#39;s stringr package. | | 【示例】 | --------| &gt;&gt;&gt; s.str.split(&#39;_&#39;) | &gt;&gt;&gt; s.str.replace(&#39;_&#39;, &#39;&#39;) 262 | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. NaT NaTType 模块所属：pandas.tslib object: 类定义：NaTType(_NaT) | (N)ot-(A)-(T)ime, the time equivalent of NaN | | 【方法排序】 | NaTType | _NaT | _Timestamp | datetime.datetime | datetime.date | 【内置对象】 | | 【方法定义】 | | __hash__(self) | | __int__(self) | | __long__(self) 263 | | __new__(cls) | | __reduce__(self) | | __repr__(self) | | __str__(self) | | astimezone(*args, **kwargs) | | combine(*args, **kwargs) | | ctime(*args, **kwargs) | | date(*args, **kwargs) | | dst(*args, **kwargs) | | fromordinal(*args, **kwargs) | | fromtimestamp(*args, **kwargs) | | isocalendar(*args, **kwargs) | | isoformat(*args, **kwargs) | | isoweekday(*args, **kwargs) | | now(*args, **kwargs) | | replace(*args, **kwargs) | | strftime(*args, **kwargs) | | strptime(*args, **kwargs) | | time(*args, **kwargs) | | timestamp(*args, **kwargs) | | timetuple(*args, **kwargs) | | timetz(*args, **kwargs) | | to_datetime(*args, **kwargs) | | today(*args, **kwargs) | | toordinal(*args, **kwargs) | | total_seconds(*args, **kwargs) | | tzname(*args, **kwargs) | | utcfromtimestamp(*args, **kwargs) | 264 | utcnow(*args, **kwargs) | | utcoffset(*args, **kwargs) | | utctimetuple(*args, **kwargs) | | weekday(*args, **kwargs) | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | day | | dayofweek | | dayofyear | | days_in_month | | daysinmonth | | hour | | microsecond | | millisecond | | minute | | month | | nanosecond | | quarter | | second | | week | | year | | ----------------------------------------------------------------------| Methods inherited from _NaT: | | __add__(self, value, /) | Return self+value. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) 265 | Return self&gt;=value. | | __gt__(self, value, /) | Return self&gt;value. | | __le__(self, value, /) | Return self&lt;=value. | | __lt__(self, value, /) | Return self&lt;value. | | __ne__(self, value, /) | Return self!=value. | | __radd__(self, value, /) | Return value+self. | | __rsub__(self, value, /) | Return value-self. | | __sub__(self, value, /) | Return self-value. | | ----------------------------------------------------------------------| Data and other attributes inherited from _NaT: | | __pyx_vtable__ = &lt;capsule object NULL&gt; | | ----------------------------------------------------------------------| Methods inherited from _Timestamp: | | to_datetime64(...) | Returns a numpy.datetime64 object with &#39;ns&#39; precision | | ----------------------------------------------------------------------| Data descriptors inherited from _Timestamp: | | offset | | value | | ----------------------------------------------------------------------| Methods inherited from datetime.datetime: | | __getattribute__(self, name, /) | Return getattr(self, name). | | ----------------------------------------------------------------------| Data descriptors inherited from datetime.datetime: | | tzinfo | | ----------------------------------------------------------------------| Data and other attributes inherited from datetime.datetime: | | max = datetime.datetime(9999, 12, 31, 23, 59, 59, 999999) | 266 | min = datetime.datetime(1, 1, 1, 0, 0) | | resolution = datetime.timedelta(0, 0, 1) | | ----------------------------------------------------------------------| Methods inherited from datetime.date: | | __format__(...) | Formats self with strftime. Panel Panel 模块所属：pandas.core.panel: 类定义：Panel(pandas.core.generic.NDFrame) | Represents wide format panel data, stored as 3-dimensional array | | 【参数】 | ----------| data : ndarray (items x major x minor), or dict of DataFrames | items : Index or array-like | axis=0 | major_axis : Index or array-like | axis=1 | minor_axis : Index or array-like | axis=2 | dtype : dtype, default None | Data type to force, otherwise infer | copy : boolean, default False | Copy data from inputs. Only affects DataFrame / 2d ndarray input | | 【方法排序】 | Panel | pandas.core.generic.NDFrame | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __add__(self, other) | # work only for scalars | | __and__(self, other) | # work only for scalars | | __div__ = __truediv__(self, other) | | __eq__(self, other) | Wrapper for comparison method __eq__ 267 | | __floordiv__(self, other) | # work only for scalars | | __ge__(self, other) | Wrapper for comparison method __ge__ | | __getitem__(self, key) | | __gt__(self, other) | Wrapper for comparison method __gt__ | | __iadd__ = f(self, other) | | __imul__ = f(self, other) | | __init__(self, data=None, items=None, major_axis=None, minor_axis=None, copy=False, dtype=None) | Initialize self. See help(type(self)) for accurate signature. | | __ipow__ = f(self, other) | | __isub__ = f(self, other) | | __itruediv__ = f(self, other) | | __le__(self, other) | Wrapper for comparison method __le__ | | __lt__(self, other) | Wrapper for comparison method __lt__ | | __mod__(self, other) | # work only for scalars | | __mul__(self, other) | # work only for scalars | | __ne__(self, other) | Wrapper for comparison method __ne__ | | __or__(self, other) | # work only for scalars | | __pow__(self, other) | # work only for scalars | | __radd__(self, other) | # work only for scalars | | __rand__(self, other) | # work only for scalars | | __rdiv__ = __rtruediv__(self, other) | | __rfloordiv__(self, other) | # work only for scalars | 268 | __rmod__(self, other) | # work only for scalars | | __rmul__(self, other) | # work only for scalars | | __ror__(self, other) | # work only for scalars | | __rpow__(self, other) | # work only for scalars | | __rsub__(self, other) | # work only for scalars | | __rtruediv__(self, other) | # work only for scalars | | __rxor__(self, other) | # work only for scalars | | __setitem__(self, key, value) | | __sub__(self, other) | # work only for scalars | | __truediv__(self, other) | # work only for scalars | | __unicode__(self) | Return a string representation for a particular Panel | | Invoked by unicode(df) in py2 only. | Yields a Unicode String in both py2/py3. | | __xor__(self, other) | # work only for scalars | | add(self, other, axis=0) | Addition of series and other, element-wise (binary operatoradd). | Equivalent to ``panel + other``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.radd | | align(self, other, **kwargs) 269 | Align two object on their axes with the | specified join method for each axis Index | | 【参数】 | ----------| other : DataFrame or Series | join : {&#39;outer&#39;, &#39;inner&#39;, &#39;left&#39;, &#39;right&#39;}, default &#39;outer&#39; | axis : allowed axis of the other object, default None | Align on index (0), columns (1), or both (None) | level : int or level name, default None | Broadcast across a level, matching Index values on the | passed MultiIndex level | copy : boolean, default True | Always returns new objects. If copy=False and no reindexing is | required then original objects are returned. | fill_value : scalar, default np.NaN | Value to use for missing values. Defaults to NaN, but can be any | &quot;compatible&quot; value | method : str, default None | limit : int, default None | fill_axis : int or labels for object, default 0 | Filling axis, method and limit | broadcast_axis : int or labels for object, default None | Broadcast values along this axis, if aligning two objects of | different dimensions | | .. versionadded:: 0.17.0 | | 【返回值】 | -------| (left, right) : (NDFrame, type of other) | Aligned objects | | all(self, axis=None, bool_only=None, skipna=None, level=None, **kwargs) | Return whether all elements are True over requested axis | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | bool_only : boolean, default None | Include only boolean data. If None, will attempt to use everything, | then use only boolean data | | 【返回值】 | -------| all : DataFrame or Panel (if level specified) | | any(self, axis=None, bool_only=None, skipna=None, level=None, **kwargs) | Return whether any element is True over requested axis | | 【参数】 270 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | bool_only : boolean, default None | Include only boolean data. If None, will attempt to use everything, | then use only boolean data | | 【返回值】 | -------| any : DataFrame or Panel (if level specified) | | apply(self, func, axis=&#39;major&#39;, **kwargs) | Applies function along axis (or axes) of the Panel | | 【参数】 | ----------| func : function | Function to apply to each combination of &#39;other&#39; axes | e.g. if axis = &#39;items&#39;, the combination of major_axis/minor_axis | will each be passed as a Series; if axis = (&#39;items&#39;, &#39;major&#39;), DataFrames | of items &amp; major axis will be passed | axis : {&#39;items&#39;, &#39;minor&#39;, &#39;major&#39;}, or {0, 1, 2}, or a tuple with two axes | Additional keyword arguments will be passed as keywords to the function | | 【示例】 | --------| | Returns a Panel with the square root of each element | | &gt;&gt;&gt; p = pd.Panel(np.random.rand(4,3,2)) | &gt;&gt;&gt; p.apply(np.sqrt) | | Equivalent to p.sum(1), returning a DataFrame | | &gt;&gt;&gt; p.apply(lambda x: x.sum(), axis=1) | | Equivalent to previous: | | &gt;&gt;&gt; p.apply(lambda x: x.sum(), axis=&#39;minor&#39;) | | Return the shapes of each DataFrame over axis 2 (i.e the shapes of items x major), as a Series | | &gt;&gt;&gt; p.apply(lambda x: x.shape, axis=(0,1)) | | 【返回值】 | -------| result : Panel, DataFrame, or Series | | as_matrix(self) | Convert the frame to its Numpy-array representation. | | 【参数】 271 | ----------| columns: list, optional, default:None | If None, return all columns, otherwise, returns specified columns. | | 【返回值】 | -------| values : ndarray | If the caller is heterogeneous and contains booleans or objects, | the result will be of dtype=object. See Notes. | | | 【注意】 | -----| Return is NOT a Numpy-matrix, rather, a Numpy-array. | | The dtype will be a lower-common-denominator dtype (implicit | upcasting); that is to say if the dtypes (even of numeric types) | are mixed, the one that accommodates all will be chosen. Use this | with care if you are not dealing with the blocks. | | e.g. If the dtypes are float16 and float32, dtype will be upcast to | float32. If dtypes are int32 and uint8, dtype will be upcase to | int32. | | This method is provided for backwards compatibility. Generally, | it is recommended to use &#39;.values&#39;. | | 【参见】 | --------| pandas.DataFrame.values | | compound(self, axis=None, skipna=None, level=None) | Return the compound percentage of the values for the requested axis | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| compounded : DataFrame or Panel (if level specified) | | conform(self, frame, axis=&#39;items&#39;) | Conform input DataFrame to align with chosen axis pair. | | 【参数】 | ----------| frame : DataFrame 272 | axis : {&#39;items&#39;, &#39;major&#39;, &#39;minor&#39;} | | Axis the input corresponds to. E.g., if axis=&#39;major&#39;, then | the frame&#39;s columns would be items, and the index would be | values of the minor axis | | 【返回值】 | -------| DataFrame | | count(self, axis=&#39;major&#39;) | Return number of observations over requested axis. | | 【参数】 | ----------| axis : {&#39;items&#39;, &#39;major&#39;, &#39;minor&#39;} or {0, 1, 2} | | 【返回值】 | -------| count : DataFrame | | cummax = max(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative max over requested axis. | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| max : DataFrame | | cummin = min(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative min over requested axis. | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| min : DataFrame | | cumprod = prod(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative prod over requested axis. | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True 273 | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| prod : DataFrame | | cumsum = sum(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative sum over requested axis. | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| sum : DataFrame | | div = truediv(self, other, axis=0) | | divide = truediv(self, other, axis=0) | | dropna(self, axis=0, how=&#39;any&#39;, inplace=False) | Drop 2D from panel, holding passed axis constant | | 【参数】 | ----------| axis : int, default 0 | Axis to hold constant. E.g. axis=1 will drop major_axis entries | having a certain amount of NA data | how : {&#39;all&#39;, &#39;any&#39;}, default &#39;any&#39; | &#39;any&#39;: one or more values are NA in the DataFrame along the | axis. For &#39;all&#39; they all must be. | inplace : bool, default False | If True, do operation inplace and return None. | | 【返回值】 | -------| dropped : Panel | | eq(self, other) | Wrapper for comparison method eq | | fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs) | Fill NA/NaN values using the specified method | | 【参数】 | ----------| value : scalar, dict, Series, or DataFrame | Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of | values specifying which value to use for each index (for a Series) or | column (for a DataFrame). (values not in the dict/Series/DataFrame will not be | filled). This value cannot be a list. | method : {&#39;backfill&#39;, &#39;bfill&#39;, &#39;pad&#39;, &#39;ffill&#39;, None}, default None 274 | Method to use for filling holes in reindexed Series | pad / ffill: propagate last valid observation forward to next valid | backfill / bfill: use NEXT valid observation to fill gap | axis : {0, 1, 2, &#39;items&#39;, &#39;major_axis&#39;, &#39;minor_axis&#39;} | inplace : boolean, default False | If True, fill in place. Note: this will modify any | other views on this object, (e.g. a no-copy slice for a column in a | DataFrame). | limit : int, default None | If method is specified, this is the maximum number of consecutive | NaN values to forward/backward fill. In other words, if there is | a gap with more than this number of consecutive NaNs, it will only | be partially filled. If method is not specified, this is the | maximum number of entries along the entire axis where NaNs will be | filled. | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | | 【参见】 | --------| reindex, asfreq | | 【返回值】 | -------| filled : Panel | | floordiv(self, other, axis=0) | Integer division of series and other, element-wise (binary operatorfloordiv). | Equivalent to ``panel // other``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.rfloordiv | | ge(self, other) | Wrapper for comparison method ge | | get_value(self, *args, **kwargs) | Quickly retrieve single value at (item, major, minor) location | | 【参数】 | ----------| item : item label (panel item) | major : major axis label (panel item row) | minor : minor axis label (panel item column) 275 | takeable : interpret the passed labels as indexers, default False | | 【返回值】 | -------| value : scalar value | | groupby(self, function, axis=&#39;major&#39;) | Group data on given axis, returning GroupBy object | | 【参数】 | ----------| function : callable | Mapping function for chosen access | axis : {&#39;major&#39;, &#39;minor&#39;, &#39;items&#39;}, default &#39;major&#39; | | 【返回值】 | -------| grouped : PanelGroupBy | | gt(self, other) | Wrapper for comparison method gt | | head(self, n=5) | Returns first n rows | | join(self, other, how=&#39;left&#39;, lsuffix=&#39;&#39;, rsuffix=&#39;&#39;) | Join items with other Panel either on major and minor axes column | | 【参数】 | ----------| other : Panel or list of Panels | Index should be similar to one of the columns in this one | how : {&#39;left&#39;, &#39;right&#39;, &#39;outer&#39;, &#39;inner&#39;} | How to handle indexes of the two objects. Default: &#39;left&#39; | for joining on index, None otherwise | * left: use calling frame&#39;s index | * right: use input frame&#39;s index | * outer: form union of indexes | * inner: use intersection of indexes | lsuffix : string | Suffix to use from left frame&#39;s overlapping columns | rsuffix : string | Suffix to use from right frame&#39;s overlapping columns | | 【返回值】 | -------| joined : Panel | | kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return unbiased kurtosis over requested axis using Fishers definition of | kurtosis (kurtosis of normal == 0.0). Normalized by N-1 | | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True 276 | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| kurt : DataFrame or Panel (if level specified) | | kurtosis = kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | | le(self, other) | Wrapper for comparison method le | | lt(self, other) | Wrapper for comparison method lt | | mad(self, axis=None, skipna=None, level=None) | Return the mean absolute deviation of the values for the requested axis | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| mad : DataFrame or Panel (if level specified) | | major_xs(self, key, copy=None) | Return slice of panel along major axis | | 【参数】 | ----------| key : object | Major axis label | copy : boolean [deprecated] | Whether to make a copy of the data | | 【返回值】 | -------| y : DataFrame | index -&gt; minor axis, columns -&gt; items | | 【注意】 277 | -----| major_xs is only for getting, not setting values. | | MultiIndex Slicers is a generic way to get/set values on any level or levels | it is a superset of major_xs functionality, see :ref:MultiIndex Slicers | | max(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | This method returns the maximum of the values in the object. If you | want the *index* of the maximum, use ``idxmax``. This is the | equivalent of the ``numpy.ndarray`` method ``argmax``. | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| max : DataFrame or Panel (if level specified) | | mean(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the mean of the values for the requested axis | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| mean : DataFrame or Panel (if level specified) | | median(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the median of the values for the requested axis | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA 278 | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| median : DataFrame or Panel (if level specified) | | min(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | This method returns the minimum of the values in the object. If you | want the *index* of the minimum, use ``idxmin``. This is the | equivalent of the ``numpy.ndarray`` method ``argmin``. | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| min : DataFrame or Panel (if level specified) | | minor_xs(self, key, copy=None) | Return slice of panel along minor axis | | 【参数】 | ----------| key : object | Minor axis label | copy : boolean [deprecated] | Whether to make a copy of the data | | 【返回值】 | -------| y : DataFrame | index -&gt; major axis, columns -&gt; items | | 【注意】 | -----| minor_xs is only for getting, not setting values. | | MultiIndex Slicers is a generic way to get/set values on any level or levels | it is a superset of minor_xs functionality, see :ref:MultiIndex Slicers | | mod(self, other, axis=0) | Modulo of series and other, element-wise (binary operatormod). 279 | Equivalent to ``panel % other``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.rmod | | mul(self, other, axis=0) | Multiplication of series and other, element-wise (binary operatormul). | Equivalent to ``panel * other``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.rmul | | multiply = mul(self, other, axis=0) | | ne(self, other) | Wrapper for comparison method ne | | pow(self, other, axis=0) | Exponential power of series and other, element-wise (binary operatorpow). | Equivalent to ``panel ** other``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.rpow 280 | | prod(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the product of the values for the requested axis | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| prod : DataFrame or Panel (if level specified) | | product = prod(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | | radd(self, other, axis=0) | Addition of series and other, element-wise (binary operatorradd). | Equivalent to ``other + panel``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.add | | rdiv = rtruediv(self, other, axis=0) | | reindex(self, items=None, major_axis=None, minor_axis=None, **kwargs) | Conform Panel to new index with optional filling logic, placing | NA/NaN in locations having no value in the previous index. A new object | is produced unless the new index is equivalent to the current one and | copy=False | | 【参数】 | ----------| items, major_axis, minor_axis : array-like, optional (can be specified in order, or as | keywords) | New labels / index to conform to. Preferably an Index object to | avoid duplicating data | method : {None, &#39;backfill&#39;/&#39;bfill&#39;, &#39;pad&#39;/&#39;ffill&#39;, &#39;nearest&#39;}, optional | method to use for filling holes in reindexed DataFrame. 281 | Please note: this is only applicable to DataFrames/Series with a | monotonically increasing/decreasing index. | * default: don&#39;t fill gaps | * pad / ffill: propagate last valid observation forward to next valid | * backfill / bfill: use next valid observation to fill gap | * nearest: use nearest valid observations to fill gap | copy : boolean, default True | Return a new object, even if the passed indexes are the same | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | fill_value : scalar, default np.NaN | Value to use for missing values. Defaults to NaN, but can be any | &quot;compatible&quot; value | limit : int, default None | Maximum number of consecutive elements to forward or backward fill | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【示例】 | --------| | Create a dataframe with some fictional data. | | &gt;&gt;&gt; index = [&#39;Firefox&#39;, &#39;Chrome&#39;, &#39;Safari&#39;, &#39;IE10&#39;, &#39;Konqueror&#39;] | &gt;&gt;&gt; df = pd.DataFrame({ | ... &#39;http_status&#39;: [200,200,404,404,301], | ... &#39;response_time&#39;: [0.04, 0.02, 0.07, 0.08, 1.0]}, | ... index=index) | &gt;&gt;&gt; df | http_status response_time | Firefox 200 0.04 | Chrome 200 0.02 | Safari 404 0.07 | IE10 404 0.08 | Konqueror 301 1.00 | | Create a new index and reindex the dataframe. By default | values in the new index that do not have corresponding | records in the dataframe are assigned ``NaN``. | | &gt;&gt;&gt; new_index= [&#39;Safari&#39;, &#39;Iceweasel&#39;, &#39;Comodo Dragon&#39;, &#39;IE10&#39;, | ... &#39;Chrome&#39;] | &gt;&gt;&gt; df.reindex(new_index) | http_status response_time | Safari 404 0.07 | Iceweasel NaN NaN | Comodo Dragon NaN NaN | IE10 404 0.08 | Chrome 200 0.02 | | We can fill in the missing values by passing a value to | the keyword ``fill_value``. Because the index is not monotonically 282 | increasing or decreasing, we cannot use arguments to the keyword | ``method`` to fill the ``NaN`` values. | | &gt;&gt;&gt; df.reindex(new_index, fill_value=0) | http_status response_time | Safari 404 0.07 | Iceweasel 0 0.00 | Comodo Dragon 0 0.00 | IE10 404 0.08 | Chrome 200 0.02 | | &gt;&gt;&gt; df.reindex(new_index, fill_value=&#39;missing&#39;) | http_status response_time | Safari 404 0.07 | Iceweasel missing missing | Comodo Dragon missing missing | IE10 404 0.08 | Chrome 200 0.02 | | To further illustrate the filling functionality in | ``reindex``, we will create a dataframe with a | monotonically increasing index (for example, a sequence | of dates). | | &gt;&gt;&gt; date_index = pd.date_range(&#39;1/1/2010&#39;, periods=6, freq=&#39;D&#39;) | &gt;&gt;&gt; df2 = pd.DataFrame({&quot;prices&quot;: [100, 101, np.nan, 100, 89, 88]}, | index=date_index) | &gt;&gt;&gt; df2 | prices | 2010-01-01 100 | 2010-01-02 101 | 2010-01-03 NaN | 2010-01-04 100 | 2010-01-05 89 | 2010-01-06 88 | | Suppose we decide to expand the dataframe to cover a wider | date range. | | &gt;&gt;&gt; date_index2 = pd.date_range(&#39;12/29/2009&#39;, periods=10, freq=&#39;D&#39;) | &gt;&gt;&gt; df2.reindex(date_index2) | prices | 2009-12-29 NaN | 2009-12-30 NaN | 2009-12-31 NaN | 2010-01-01 100 | 2010-01-02 101 | 2010-01-03 NaN | 2010-01-04 100 | 2010-01-05 89 | 2010-01-06 88 | 2010-01-07 NaN | | The index entries that did not have a value in the original data frame | (for example, &#39;2009-12-29&#39;) are by default filled with ``NaN``. | If desired, we can fill in the missing values using one of several | options. 283 | | For example, to backpropagate the last valid value to fill the ``NaN`` | values, pass ``bfill`` as an argument to the ``method`` keyword. | | &gt;&gt;&gt; df2.reindex(date_index2, method=&#39;bfill&#39;) | prices | 2009-12-29 100 | 2009-12-30 100 | 2009-12-31 100 | 2010-01-01 100 | 2010-01-02 101 | 2010-01-03 NaN | 2010-01-04 100 | 2010-01-05 89 | 2010-01-06 88 | 2010-01-07 NaN | | Please note that the ``NaN`` value present in the original dataframe | (at index value 2010-01-03) will not be filled by any of the | value propagation schemes. This is because filling while reindexing | does not look at dataframe values, but only compares the original and | desired indexes. If you do want to fill in the ``NaN`` values present | in the original dataframe, use the ``fillna()`` method. | | 【返回值】 | -------| reindexed : Panel | | reindex_axis(self, labels, axis=0, method=None, level=None, copy=True, limit=None, fill_value=nan) | Conform input object to new index with optional filling logic, | placing NA/NaN in locations having no value in the previous index. A | new object is produced unless the new index is equivalent to the | current one and copy=False | | 【参数】 | ----------| labels : array-like | New labels / index to conform to. Preferably an Index object to | avoid duplicating data | axis : {0, 1, 2, &#39;items&#39;, &#39;major_axis&#39;, &#39;minor_axis&#39;} | method : {None, &#39;backfill&#39;/&#39;bfill&#39;, &#39;pad&#39;/&#39;ffill&#39;, &#39;nearest&#39;}, optional | Method to use for filling holes in reindexed DataFrame: | * default: don&#39;t fill gaps | * pad / ffill: propagate last valid observation forward to next valid | * backfill / bfill: use next valid observation to fill gap | * nearest: use nearest valid observations to fill gap | copy : boolean, default True | Return a new object, even if the passed indexes are the same | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | limit : int, default None | Maximum number of consecutive elements to forward or backward fill | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. 284 | | .. versionadded:: 0.17.0 | | 【示例】 | --------| &gt;&gt;&gt; df.reindex_axis([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;], axis=1) | | 【参见】 | --------| reindex, reindex_like | | 【返回值】 | -------| reindexed : Panel | | rename(self, items=None, major_axis=None, minor_axis=None, **kwargs) | Alter axes input function or functions. Function / dict values must be | unique (1-to-1). Labels not contained in a dict / Series will be left | as-is. | | 【参数】 | ----------| items, major_axis, minor_axis : dict-like or function, optional | Transformation to apply to that axis values | | copy : boolean, default True | Also copy underlying data | inplace : boolean, default False | Whether to return a new Panel. If True then value of copy is | ignored. | | 【返回值】 | -------| renamed : Panel (new object) | | rfloordiv(self, other, axis=0) | Integer division of series and other, element-wise (binary operatorrfloordiv). | Equivalent to ``other // panel``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.floordiv | | rmod(self, other, axis=0) | Modulo of series and other, element-wise (binary operatorrmod). | Equivalent to ``other % panel``. 285 | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.mod | | rmul(self, other, axis=0) | Multiplication of series and other, element-wise (binary operatorrmul). | Equivalent to ``other * panel``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.mul | | rpow(self, other, axis=0) | Exponential power of series and other, element-wise (binary operatorrpow). | Equivalent to ``other ** panel``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.pow | | rsub(self, other, axis=0) | Subtraction of series and other, element-wise (binary operatorrsub). | Equivalent to ``other - panel``. | | 【参数】 286 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.sub | | rtruediv(self, other, axis=0) | Floating division of series and other, element-wise (binary operatorrtruediv). | Equivalent to ``other / panel``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.truediv | | sem(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs) | Return unbiased standard error of the mean over requested axis. | | Normalized by N-1 by default. This can be changed using the ddof argument | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | ddof : int, default 1 | degrees of freedom | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| sem : DataFrame or Panel (if level specified) | | set_value(self, *args, **kwargs) | Quickly set single value at (item, major, minor) location 287 | | 【参数】 | ----------| item : item label (panel item) | major : major axis label (panel item row) | minor : minor axis label (panel item column) | value : scalar | takeable : interpret the passed labels as indexers, default False | | 【返回值】 | -------| panel : Panel | If label combo is contained, will be reference to calling Panel, | otherwise a new object | | shift(self, periods=1, freq=None, axis=&#39;major&#39;) | Shift index by desired number of periods with an optional time freq. | The shifted data will not include the dropped periods and the | shifted axis will be smaller than the original. This is different | from the behavior of DataFrame.shift() | | 【参数】 | ----------| periods : int | Number of periods to move, can be positive or negative | freq : DateOffset, timedelta, or time rule string, optional | axis : {&#39;items&#39;, &#39;major&#39;, &#39;minor&#39;} or {0, 1, 2} | | 【返回值】 | -------| shifted : Panel | | skew(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return unbiased skew over requested axis | Normalized by N-1 | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| skew : DataFrame or Panel (if level specified) | | std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs) | Return unbiased standard deviation over requested axis. | | Normalized by N-1 by default. This can be changed using the ddof argument 288 | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | ddof : int, default 1 | degrees of freedom | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| std : DataFrame or Panel (if level specified) | | sub(self, other, axis=0) | Subtraction of series and other, element-wise (binary operatorsub). | Equivalent to ``panel - other``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.rsub | | subtract = sub(self, other, axis=0) | | sum(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the sum of the values for the requested axis | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 289 | -------| sum : DataFrame or Panel (if level specified) | | tail(self, n=5) | Returns last n rows | | toLong = wrapper(*args, **kwargs) | | to_excel(self, path, na_rep=&#39;&#39;, engine=None, **kwargs) | Write each DataFrame in Panel to a separate excel sheet | | 【参数】 | ----------| path : string or ExcelWriter object | File path or existing ExcelWriter | na_rep : string, default &#39;&#39; | Missing data representation | engine : string, default None | write engine to use - you can also set this via the options | ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and | ``io.excel.xlsm.writer``. | | Other【参数】 | ----------------| float_format : string, default None | Format string for floating point numbers | cols : sequence, optional | Columns to write | header : boolean or list of string, default True | Write out column names. If a list of string is given it is | assumed to be aliases for the column names | index : boolean, default True | Write row names (index) | index_label : string or sequence, default None | Column label for index column(s) if desired. If None is given, and |headerandindexare True, then the index names are used. A | sequence should be given if the DataFrame uses MultiIndex. | startrow : upper left cell row to dump data frame | startcol : upper left cell column to dump data frame | | 【注意】 | -----| Keyword arguments (and na_rep) are passed to the ``to_excel`` method | for each DataFrame written. | | to_frame(self, filter_observations=True) | Transform wide format into long (stacked) format as DataFrame whose | columns are the Panel&#39;s items and whose index is a MultiIndex formed | of the Panel&#39;s major and minor axes. | | 【参数】 | ----------| filter_observations : boolean, default True | Drop (major, minor) pairs without a complete set of observations | across all the items | | 【返回值】 290 | -------| y : DataFrame | | to_long = wrapper(*args, **kwargs) | | to_sparse(self, fill_value=None, kind=&#39;block&#39;) | Convert to SparsePanel | | 【参数】 | ----------| fill_value : float, default NaN | kind : {&#39;block&#39;, &#39;integer&#39;} | | 【返回值】 | -------| y : SparseDataFrame | | transpose(self, *args, **kwargs) | Permute the dimensions of the Panel | | 【参数】 | ----------| args : three positional arguments: each oneof | {0, 1, 2, &#39;items&#39;, &#39;major_axis&#39;, &#39;minor_axis&#39;} | copy : boolean, default False | Make a copy of the underlying data. Mixed-dtype data will | always result in a copy | | 【示例】 | --------| &gt;&gt;&gt; p.transpose(2, 0, 1) | &gt;&gt;&gt; p.transpose(2, 0, 1, copy=True) | | 【返回值】 | -------| y : same as input | | truediv(self, other, axis=0) | Floating division of series and other, element-wise (binary operatortruediv). | Equivalent to ``panel / other``. | | 【参数】 | ----------| other : DataFrame or Panel | axis : {items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel | | 【参见】 | --------| Panel.rtruediv | | tshift(self, periods=1, freq=None, axis=&#39;major&#39;) 291 | Shift the time index, using the index&#39;s frequency if available | | 【参数】 | ----------| periods : int | Number of periods to move, can be positive or negative | freq : DateOffset, timedelta, or time rule string, default None | Increment to use from datetools module or time rule (e.g. &#39;EOM&#39;) | axis : int or basestring | Corresponds to the axis that contains the Index | | 【注意】 | -----| If freq is not specified then tries to use the freq or inferred_freq | attributes of the index. If neither of those attributes exist, a | ValueError is thrown | | 【返回值】 | -------| shifted : NDFrame | | update(self, other, join=&#39;left&#39;, overwrite=True, filter_func=None, raise_conflict=False) | Modify Panel in place using non-NA values from passed | Panel, or object coercible to Panel. Aligns on items | | 【参数】 | ----------| other : Panel, or object coercible to Panel | join : How to join individual DataFrames | {&#39;left&#39;, &#39;right&#39;, &#39;outer&#39;, &#39;inner&#39;}, default &#39;left&#39; | overwrite : boolean, default True | If True then overwrite values for common keys in the calling panel | filter_func : callable(1d-array) -&gt; 1d-array&lt;boolean&gt;, default None | Can choose to replace values other than NA. Return True for values | that should be updated | raise_conflict : bool | If True, will raise an error if a DataFrame and other both | contain data in the same place. | | var(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs) | Return unbiased variance over requested axis. | | Normalized by N-1 by default. This can be changed using the ddof argument | | 【参数】 | ----------| axis : {items (0), major_axis (1), minor_axis (2)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a DataFrame | ddof : int, default 1 | degrees of freedom | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use 292 | everything, then use only numeric data | | 【返回值】 | -------| var : DataFrame or Panel (if level specified) | | xs(self, key, axis=1, copy=None) | Return slice of panel along selected axis | | 【参数】 | ----------| key : object | Label | axis : {&#39;items&#39;, &#39;major&#39;, &#39;minor}, default 1/&#39;major&#39; | copy : boolean [deprecated] | Whether to make a copy of the data | | 【返回值】 | -------| y : ndim(self)-1 | | 【注意】 | -----| xs is only for getting, not setting values. | | MultiIndex Slicers is a generic way to get/set values on any level or levels | it is a superset of xs functionality, see :ref:MultiIndex Slicers | | ----------------------------------------------------------------------| Class methods defined here: | | fromDict = from_dict(data, intersect=False, orient=&#39;items&#39;, dtype=None) from builtins.type | Construct Panel from dict of DataFrame objects | | 【参数】 | ----------| data : dict | {field : DataFrame} | intersect : boolean | Intersect indexes of input DataFrames | orient : {&#39;items&#39;, &#39;minor&#39;}, default &#39;items&#39; | The &quot;orientation&quot; of the data. If the keys of the passed dict | should be the items of the result panel, pass &#39;items&#39; | (default). Otherwise if the columns of the values of the passed | DataFrame objects should be the items (which in the case of | mixed-dtype data you should do), instead pass &#39;minor&#39; | dtype : dtype, default None | Data type to force, otherwise infer | | 【返回值】 | -------| Panel | | from_dict(data, intersect=False, orient=&#39;items&#39;, dtype=None) from builtins.type | Construct Panel from dict of DataFrame objects | | 【参数】 293 | ----------| data : dict | {field : DataFrame} | intersect : boolean | Intersect indexes of input DataFrames | orient : {&#39;items&#39;, &#39;minor&#39;}, default &#39;items&#39; | The &quot;orientation&quot; of the data. If the keys of the passed dict | should be the items of the result panel, pass &#39;items&#39; | (default). Otherwise if the columns of the values of the passed | DataFrame objects should be the items (which in the case of | mixed-dtype data you should do), instead pass &#39;minor&#39; | dtype : dtype, default None | Data type to force, otherwise infer | | 【返回值】 | -------| Panel | | ----------------------------------------------------------------------| Data descriptors defined here: | | items | | major_axis | | minor_axis | | ----------------------------------------------------------------------| Methods inherited from pandas.core.generic.NDFrame: | | __abs__(self) | | __array__(self, dtype=None) | | __array_wrap__(self, result, context=None) | | __bool__ = __nonzero__(self) | | __contains__(self, key) | True if the key is in the info axis | | __delitem__(self, key) | Delete item | | __finalize__(self, other, method=None, **kwargs) | propagate metadata from other to self | | 【参数】 | ----------| other : the object from which to get the attributes that we are going | to propagate | method : optional, a passed method name ; possibly to take different | types of propagation actions based on this | | __getattr__(self, name) | After regular attribute access, try looking up the name | This allows simpler access to columns for interactive use. 294 | | __getstate__(self) | | __hash__(self) | Return hash(self). | | __invert__(self) | | __iter__(self) | Iterate over infor axis | | __len__(self) | Returns length of info axis | | __neg__(self) | | __nonzero__(self) | | __setattr__(self, name, value) | After regular attribute access, try setting the name | This allows simpler access to columns for interactive use. | | __setstate__(self, state) | | abs(self) | Return an object with absolute value taken. Only applicable to objects | that are all numeric | | 【返回值】 | -------| abs: type of caller | | add_prefix(self, prefix) | Concatenate prefix string with panel items names. | | 【参数】 | ----------| prefix : string | | 【返回值】 | -------| with_prefix : type of caller | | add_suffix(self, suffix) | Concatenate suffix string with panel items names | | 【参数】 | ----------| suffix : string | | 【返回值】 | -------| with_suffix : type of caller | | as_blocks(self, copy=True) | Convert the frame to a dict of dtype -&gt; Constructor Types that each has | a homogeneous dtype. 295 | | NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in | as_matrix) | | 【参数】 | ----------| copy : boolean, default True | | .. versionadded: 0.16.1 | | 【返回值】 | -------| values : a dict of dtype -&gt; Constructor Types | | asfreq(self, freq, method=None, how=None, normalize=False) | Convert all TimeSeries inside to specified frequency using DateOffset | objects. Optionally provide fill method to pad/backfill missing values. | | 【参数】 | ----------| freq : DateOffset object, or string | method : {&#39;backfill&#39;, &#39;bfill&#39;, &#39;pad&#39;, &#39;ffill&#39;, None} | Method to use for filling holes in reindexed Series | pad / ffill: propagate last valid observation forward to next valid | backfill / bfill: use NEXT valid observation to fill method | how : {&#39;start&#39;, &#39;end&#39;}, default end | For PeriodIndex only, see PeriodIndex.asfreq | normalize : bool, default False | Whether to reset output index to midnight | | 【返回值】 | -------| converted : type of caller | | astype(self, dtype, copy=True, raise_on_error=True, **kwargs) | Cast object to input numpy.dtype | Return a copy when copy = True (be really careful with this!) | | 【参数】 | ----------| dtype : numpy.dtype or Python type | raise_on_error : raise on invalid input | kwargs : keyword arguments to pass on to the constructor | | 【返回值】 | -------| casted : type of caller | | at_time(self, time, asof=False) | Select values at particular time of day (e.g. 9:30AM) | | 【参数】 | ----------| time : datetime.time or string | | 【返回值】 296 | -------| values_at_time : type of caller | | between_time(self, start_time, end_time, include_start=True, include_end=True) | Select values between particular times of the day (e.g., 9:00-9:30 AM) | | 【参数】 | ----------| start_time : datetime.time or string | end_time : datetime.time or string | include_start : boolean, default True | include_end : boolean, default True | | 【返回值】 | -------| values_between_time : type of caller | | bfill(self, axis=None, inplace=False, limit=None, downcast=None) | Synonym for NDFrame.fillna(method=&#39;bfill&#39;) | | bool(self) | Return the bool of a single element PandasObject | This must be a boolean scalar value, either True or False | | Raise a ValueError if the PandasObject does not have exactly | 1 element, or that element is not boolean | | clip(self, lower=None, upper=None, out=None, axis=None) | Trim values at input threshold(s) | | 【参数】 | ----------| lower : float or array_like, default None | upper : float or array_like, default None | axis : int or string axis name, optional | Align object with lower and upper along the given axis. | | 【返回值】 | -------| clipped : Series | | 【示例】 | --------| &gt;&gt;&gt; df | 0 1 | 0 0.335232 -1.256177 | 1 -1.367855 0.746646 | 2 0.027753 -1.176076 | 3 0.230930 -0.679613 | 4 1.261967 0.570967 | &gt;&gt;&gt; df.clip(-1.0, 0.5) | 0 1 | 0 0.335232 -1.000000 | 1 -1.000000 0.500000 | 2 0.027753 -1.000000 | 3 0.230930 -0.679613 | 4 0.500000 0.500000 297 | &gt;&gt;&gt; t | 0 -0.3 | 1 -0.2 | 2 -0.1 | 3 0.0 | 4 0.1 | dtype: float64 | &gt;&gt;&gt; df.clip(t, t + 1, axis=0) | 0 1 | 0 0.335232 -0.300000 | 1 -0.200000 0.746646 | 2 0.027753 -0.100000 | 3 0.230930 0.000000 | 4 1.100000 0.570967 | | clip_lower(self, threshold, axis=None) | Return copy of the input with values below given value(s) truncated | | 【参数】 | ----------| threshold : float or array_like | axis : int or string axis name, optional | Align object with threshold along the given axis. | | 【参见】 | --------| clip | | 【返回值】 | -------| clipped : same type as input | | clip_upper(self, threshold, axis=None) | Return copy of input with values above given value(s) truncated | | 【参数】 | ----------| threshold : float or array_like | axis : int or string axis name, optional | Align object with threshold along the given axis. | | 【参见】 | --------| clip | | 【返回值】 | -------| clipped : same type as input | | consolidate(self, inplace=False) | Compute NDFrame with &quot;consolidated&quot; internals (data of each dtype | grouped together in a single ndarray). Mainly an internal API function, | but available here to the savvy user | | 【参数】 | ----------| inplace : boolean, default False 298 | If False return new object, otherwise modify existing object | | 【返回值】 | -------| consolidated : type of caller | | convert_objects(self, convert_dates=True, convert_numeric=False, convert_timedeltas=True, copy=True) | Attempt to infer better dtype for object columns | | 【参数】 | ----------| convert_dates : boolean, default True | If True, convert to date where possible. If &#39;coerce&#39;, force | conversion, with unconvertible values becoming NaT. | convert_numeric : boolean, default False | If True, attempt to coerce to numbers (including strings), with | unconvertible values becoming NaN. | convert_timedeltas : boolean, default True | If True, convert to timedelta where possible. If &#39;coerce&#39;, force | conversion, with unconvertible values becoming NaT. | copy : boolean, default True | If True, return a copy even if no copy is necessary (e.g. no | conversion was done). Note: This is meant for internal use, and | should not be confused with inplace. | | 【返回值】 | -------| converted : same as input object | | copy(self, deep=True) | Make a copy of this object | | 【参数】 | ----------| deep : boolean or string, default True | Make a deep copy, i.e. also copy data | | 【返回值】 | -------| copy : type of caller | | describe(self, percentiles=None, include=None, exclude=None) | Generate various summary statistics, excluding NaN values. | | 【参数】 | ----------| percentiles : array-like, optional | The percentiles to include in the output. Should all | be in the interval [0, 1]. By defaultpercentilesis | [.25, .5, .75], returning the 25th, 50th, and 75th percentiles. | include, exclude : list-like, &#39;all&#39;, or None (default) | Specify the form of the returned result. Either: | | - None to both (default). The result will include only numeric-typed | columns or, if none are, only categorical columns. | - A list of dtypes or strings to be included/excluded. | To select all numeric types use numpy numpy.number. To select 299 | categorical objects use type object. 参见：the select_dtypes | documentation. eg. df.describe(include=[&#39;O&#39;]) | - If include is the string &#39;all&#39;, the output column-set will | match the input one. | | 【返回值】 | -------| summary: NDFrame of summary statistics | | 【注意】 | -----| The output DataFrame index depends on the requested dtypes: | | For numeric dtypes, it will include: count, mean, std, min, | max, and lower, 50, and upper percentiles. | | For object dtypes (e.g. timestamps or strings), the index | will include the count, unique, most common, and frequency of the | most common. Timestamps also include the first and last items. | | For mixed dtypes, the index will be the union of the corresponding | output types. Non-applicable entries will be filled with NaN. | Note that mixed-dtype outputs can only be returned from mixed-dtype | inputs and appropriate use of the include/exclude arguments. | | If multiple values have the highest count, then the |countandmost commonpair will be arbitrarily chosen from | among those with the highest count. | | The include, exclude arguments are ignored for Series. | | 【参见】 | --------| DataFrame.select_dtypes | | drop(self, labels, axis=0, level=None, inplace=False, errors=&#39;raise&#39;) | Return new object with labels in requested axis removed | | 【参数】 | ----------| labels : single label or list-like | axis : int or axis name | level : int or level name, default None | For MultiIndex | inplace : bool, default False | If True, do operation inplace and return None. | errors : {&#39;ignore&#39;, &#39;raise&#39;}, default &#39;raise&#39; | If &#39;ignore&#39;, suppress error and existing labels are dropped. | | .. versionadded:: 0.16.1 | | 【返回值】 | -------| dropped : type of caller | | equals(self, other) | Determines if two NDFrame objects contain the same elements. NaNs in the 300 | same location are considered equal. | | ffill(self, axis=None, inplace=False, limit=None, downcast=None) | Synonym for NDFrame.fillna(method=&#39;ffill&#39;) | | filter(self, items=None, like=None, regex=None, axis=None) | Restrict the info axis to set of items or wildcard | | 【参数】 | ----------| items : list-like | List of info axis to restrict to (must not all be present) | like : string | Keep info axis where &quot;arg in col == True&quot; | regex : string (regular expression) | Keep info axis with re.search(regex, col) == True | axis : int or None | The axis to filter on. By default this is the info axis. The &quot;info | axis&quot; is the axis that is used when indexing with ``[]``. For | example, ``df = DataFrame({&#39;a&#39;: [1, 2, 3, 4]]}); df[&#39;a&#39;]``. So, | the ``DataFrame`` columns are the info axis. | | 【注意】 | -----| Arguments are mutually exclusive, but this is not checked for | | first(self, offset) | Convenience method for subsetting initial periods of time series data | based on a date offset | | 【参数】 | ----------| offset : string, DateOffset, dateutil.relativedelta | | 【示例】 | --------| ts.last(&#39;10D&#39;) -&gt; First 10 days | | 【返回值】 | -------| subset : type of caller | | get(self, key, default=None) | Get item from object for given key (DataFrame column, Panel slice, | etc.). Returns default value if not found | | 【参数】 | ----------| key : object | | 【返回值】 | -------| value : type of items contained in object | | get_dtype_counts(self) | Return the counts of dtypes in this object | 301 | get_ftype_counts(self) | Return the counts of ftypes in this object | | get_values(self) | same as values (but handles sparseness conversions) | | interpolate(self, method=&#39;linear&#39;, axis=0, limit=None, inplace=False, limit_direction=&#39;forward&#39;, downcast=None, **kwargs) | Interpolate values according to different methods. | | Please note that only ``method=&#39;linear&#39;`` is supported for DataFrames/Series | with a MultiIndex. | | 【参数】 | ----------| method : {&#39;linear&#39;, &#39;time&#39;, &#39;index&#39;, &#39;values&#39;, &#39;nearest&#39;, &#39;zero&#39;, | &#39;slinear&#39;, &#39;quadratic&#39;, &#39;cubic&#39;, &#39;barycentric&#39;, &#39;krogh&#39;, | &#39;polynomial&#39;, &#39;spline&#39; &#39;piecewise_polynomial&#39;, &#39;pchip&#39;} | | * &#39;linear&#39;: ignore the index and treat the values as equally | spaced. This is the only method supported on MultiIndexes. | default | * &#39;time&#39;: interpolation works on daily and higher resolution | data to interpolate given length of interval | * &#39;index&#39;, &#39;values&#39;: use the actual numerical values of the index | * &#39;nearest&#39;, &#39;zero&#39;, &#39;slinear&#39;, &#39;quadratic&#39;, &#39;cubic&#39;, | &#39;barycentric&#39;, &#39;polynomial&#39; is passed to | ``scipy.interpolate.interp1d``. Both &#39;polynomial&#39; and &#39;spline&#39; | require that you also specify anorder(int), | e.g. df.interpolate(method=&#39;polynomial&#39;, order=4). | These use the actual numerical values of the index. | * &#39;krogh&#39;, &#39;piecewise_polynomial&#39;, &#39;spline&#39;, and &#39;pchip&#39; are all | wrappers around the scipy interpolation methods of similar | names. These use the actual numerical values of the index. See | the scipy documentation for more on their behavior |here http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation__ |and here http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html__ | | axis : {0, 1}, default 0 | * 0: fill column-by-column | * 1: fill row-by-row | limit : int, default None. | Maximum number of consecutive NaNs to fill. | limit_direction : {&#39;forward&#39;, &#39;backward&#39;, &#39;both&#39;}, defaults to &#39;forward&#39; | If limit is specified, consecutive NaNs will be filled in this | direction. | | .. versionadded:: 0.17.0 | | inplace : bool, default False | Update the NDFrame in place if possible. | downcast : optional, &#39;infer&#39; or None, defaults to None | Downcast dtypes if possible. | kwargs : keyword arguments to pass on to the interpolating function. | | 【返回值】 | -------| Series or DataFrame of same shape interpolated at the NaNs 302 | | 【参见】 | --------| reindex, replace, fillna | | 【示例】 | --------| | Filling in NaNs | | &gt;&gt;&gt; s = pd.Series([0, 1, np.nan, 3]) | &gt;&gt;&gt; s.interpolate() | 0 0 | 1 1 | 2 2 | 3 3 | dtype: float64 | | isnull(self) | Return a boolean same-sized object indicating if the values are null | | 【参见】 | --------| notnull : boolean inverse of isnull | | iteritems(self) | Iterate over (label, values) on info axis | | This is index for Series, columns for DataFrame, major_axis for Panel, | and so on. | | iterkv(self, *args, **kwargs) | iteritems alias used to get around 2to3. Deprecated | | keys(self) | Get the &#39;info axis&#39; (see Indexing for more) | | This is index for Series, columns for DataFrame and major_axis for | Panel. | | last(self, offset) | Convenience method for subsetting final periods of time series data | based on a date offset | | 【参数】 | ----------| offset : string, DateOffset, dateutil.relativedelta | | 【示例】 | --------| ts.last(&#39;5M&#39;) -&gt; Last 5 months | | 【返回值】 | -------| subset : type of caller | | mask(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True) 303 | Return an object of same shape as self and whose corresponding | entries are from self where cond is False and otherwise are from other. | | 【参数】 | ----------| cond : boolean NDFrame or array | other : scalar or NDFrame | inplace : boolean, default False | Whether to perform the operation in place on the data | axis : alignment axis if needed, default None | level : alignment level if needed, default None | try_cast : boolean, default False | try to cast the result back to the input type (if possible), | raise_on_error : boolean, default True | Whether to raise on invalid data types (e.g. trying to where on | strings) | | 【返回值】 | -------| wh : same type as caller | | notnull(self) | Return a boolean same-sized object indicating if the values are | not null | | 【参见】 | --------| isnull : boolean inverse of notnull | | pct_change(self, periods=1, fill_method=&#39;pad&#39;, limit=None, freq=None, **kwargs) | Percent change over given number of periods. | | 【参数】 | ----------| periods : int, default 1 | Periods to shift for forming percent change | fill_method : str, default &#39;pad&#39; | How to handle NAs before computing percent changes | limit : int, default None | The number of consecutive NAs to fill before stopping | freq : DateOffset, timedelta, or offset alias string, optional | Increment to use from time series API (e.g. &#39;M&#39; or BDay()) | | 【返回值】 | -------| chg : NDFrame | | 【注意】 | -----| | By default, the percentage change is calculated along the stat | axis: 0, or ``Index``, for ``DataFrame`` and 1, or ``minor`` for | ``Panel``. You can change this with the ``axis`` keyword argument. | | pipe(self, func, *args, **kwargs) | Apply func(self, \\*args, \\*\\*kwargs) | 304 | .. versionadded:: 0.16.2 | | 【参数】 | ----------| func : function | function to apply to the NDFrame. | ``args``, and ``kwargs`` are passed into ``func``. | Alternatively a ``(callable, data_keyword)`` tuple where | ``data_keyword`` is a string indicating the keyword of | ``callable`` that expects the NDFrame. | args : positional arguments passed into ``func``. | kwargs : a dictionary of keyword arguments passed into ``func``. | | 【返回值】 | -------| object : the return type of ``func``. | | 【注意】 | -----| | Use ``.pipe`` when chaining together functions that expect | on Series or DataFrames. Instead of writing | | &gt;&gt;&gt; f(g(h(df), arg1=a), arg2=b, arg3=c) | | You can write | | &gt;&gt;&gt; (df.pipe(h) | ... .pipe(g, arg1=a) | ... .pipe(f, arg2=b, arg3=c) | ... ) | | If you have a function that takes the data as (say) the second | argument, pass a tuple indicating which keyword expects the | data. For example, suppose ``f`` takes its data as ``arg2``: | | &gt;&gt;&gt; (df.pipe(h) | ... .pipe(g, arg1=a) | ... .pipe((f, &#39;arg2&#39;), arg1=a, arg3=c) | ... ) | | 【参见】 | --------| pandas.DataFrame.apply | pandas.DataFrame.applymap | pandas.Series.map | | pop(self, item) | Return item and drop from frame. Raise KeyError if not found. | | reindex_like(self, other, method=None, copy=True, limit=None, tolerance=None) | return an object with matching indicies to myself | | 【参数】 | ----------| other : Object | method : string or None 305 | copy : boolean, default True | limit : int, default None | Maximum number of consecutive labels to fill for inexact matches. | tolerance : optional | Maximum distance between labels of the other object and this | object for inexact matches. | | .. versionadded:: 0.17.0 | | 【注意】 | -----| Like calling s.reindex(index=other.index, columns=other.columns, | method=...) | | 【返回值】 | -------| reindexed : same as input | | rename_axis(self, mapper, axis=0, copy=True, inplace=False) | Alter index and / or columns using input function or functions. | Function / dict values must be unique (1-to-1). Labels not contained in | a dict / Series will be left as-is. | | 【参数】 | ----------| mapper : dict-like or function, optional | axis : int or string, default 0 | copy : boolean, default True | Also copy underlying data | inplace : boolean, default False | | 【返回值】 | -------| renamed : type of caller | | replace(self, to_replace=None, value=None, inplace=False, limit=None, regex=False, method=&#39;pad&#39;, axis=None) | Replace values given in &#39;to_replace&#39; with &#39;value&#39;. | | 【参数】 | ----------| to_replace : str, regex, list, dict, Series, numeric, or None | | * str or regex: | | - str: string exactly matchingto_replacewill be replaced | withvalue| - regex: regexs matchingto_replacewill be replaced with |value| | * list of str, regex, or numeric: | | - First, ifto_replaceandvalueare both lists, they | **must** be the same length. | - Second, if ``regex=True`` then all of the strings in **both** | lists will be interpreted as regexs otherwise they will match | directly. This doesn&#39;t matter much forvaluesince there | are only a few possible substitution regexes you can use. 306 | - str and regex rules apply as above. | | * dict: | | - Nested dictionaries, e.g., {&#39;a&#39;: {&#39;b&#39;: nan}}, are read as | follows: look in column &#39;a&#39; for the value &#39;b&#39; and replace it | with nan. You can nest regular expressions as well. Note that | column names (the top-level dictionary keys in a nested | dictionary) **cannot** be regular expressions. | - Keys map to column names and values map to substitution | values. You can treat this as a special case of passing two | lists except that you are specifying the column to search in. | | * None: | | - This means that the ``regex`` argument must be a string, | compiled regular expression, or list, dict, ndarray or Series | of such elements. Ifvalueis also ``None`` then this | **must** be a nested dictionary or ``Series``. | | See the examples section for examples of each of these. | value : scalar, dict, list, str, regex, default None | Value to use to fill holes (e.g. 0), alternately a dict of values | specifying which value to use for each column (columns not in the | dict will not be filled). Regular expressions, strings and lists or | dicts of such objects are also allowed. | inplace : boolean, default False | If True, in place. Note: this will modify any | other views on this object (e.g. a column form a DataFrame). | Returns the caller if this is True. | limit : int, default None | Maximum size gap to forward or backward fill | regex : bool or same types asto_replace, default False | Whether to interpretto_replaceand/orvalueas regular | expressions. If this is ``True`` thento_replace*must* be a | string. Otherwise,to_replacemust be ``None`` because this | parameter will be interpreted as a regular expression or a list, | dict, or array of regular expressions. | method : string, optional, {&#39;pad&#39;, &#39;ffill&#39;, &#39;bfill&#39;} | The method to use when for replacement, when ``to_replace`` is a | ``list``. | | 【参见】 | --------| NDFrame.reindex | NDFrame.asfreq | NDFrame.fillna | | 【返回值】 | -------| filled : NDFrame | | 【Raises 引发错误】 | ------| AssertionError | * Ifregexis not a ``bool`` andto_replaceis not ``None``. | TypeError 307 | * Ifto_replaceis a ``dict`` andvalueis not a ``list``, | ``dict``, ``ndarray``, or ``Series`` | * Ifto_replaceis ``None`` andregexis not compilable into a | regular expression or is a list, dict, ndarray, or Series. | ValueError | * Ifto_replaceandvalueare ``list`` s or ``ndarray`` s, but | they are not the same length. | | 【注意】 | -----| * Regex substitution is performed under the hood with ``re.sub``. The | rules for substitution for ``re.sub`` are the same. | * Regular expressions will only substitute on strings, meaning you | cannot provide, for example, a regular expression matching floating | point numbers and expect the columns in your frame that have a | numeric dtype to be matched. However, if those floating point numbers | *are* strings, then you can do this. | * This method has *a lot* of options. You are encouraged to experiment | and play with this method to gain intuition about how it works. | | resample(self, rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention=&#39;start&#39;, kind=None, loffset=None, limit=None, base=0) | Convenience method for frequency conversion and resampling of regular | time-series data. | | 【参数】 | ----------| rule : string | the offset string or object representing target conversion | how : string | method for down- or re-sampling, default to &#39;mean&#39; for | downsampling | axis : int, optional, default 0 | fill_method : string, default None | fill_method for upsampling | closed : {&#39;right&#39;, &#39;left&#39;} | Which side of bin interval is closed | label : {&#39;right&#39;, &#39;left&#39;} | Which bin edge label to label bucket with | convention : {&#39;start&#39;, &#39;end&#39;, &#39;s&#39;, &#39;e&#39;} | kind : &quot;period&quot;/&quot;timestamp&quot; | loffset : timedelta | Adjust the resampled time labels | limit : int, default None | Maximum size gap to when reindexing with fill_method | base : int, default 0 | For frequencies that evenly subdivide 1 day, the &quot;origin&quot; of the | aggregated intervals. For example, for &#39;5min&#39; frequency, base could | range from 0 through 4. Defaults to 0 | | | 【示例】 | --------| | Start by creating a series with 9 one minute timestamps. | | &gt;&gt;&gt; index = pd.date_range(&#39;1/1/2000&#39;, periods=9, freq=&#39;T&#39;) 308 | &gt;&gt;&gt; series = pd.Series(range(9), index=index) | &gt;&gt;&gt; series | 2000-01-01 00:00:00 0 | 2000-01-01 00:01:00 1 | 2000-01-01 00:02:00 2 | 2000-01-01 00:03:00 3 | 2000-01-01 00:04:00 4 | 2000-01-01 00:05:00 5 | 2000-01-01 00:06:00 6 | 2000-01-01 00:07:00 7 | 2000-01-01 00:08:00 8 | Freq: T, dtype: int64 | | Downsample the series into 3 minute bins and sum the values | of the timestamps falling into a bin. | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=&#39;sum&#39;) | 2000-01-01 00:00:00 3 | 2000-01-01 00:03:00 12 | 2000-01-01 00:06:00 21 | Freq: 3T, dtype: int64 | | Downsample the series into 3 minute bins as above, but label each | bin using the right edge instead of the left. Please note that the | value in the bucket used as the label is not included in the bucket, | which it labels. For example, in the original series the | bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed | value in the resampled bucket with the label``2000-01-01 00:03:00`` | does not include 3 (if it did, the summed value would be 6, not 3). | To include this value close the right side of the bin interval as | illustrated in the example below this one. | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=&#39;sum&#39;, label=&#39;right&#39;) | 2000-01-01 00:03:00 3 | 2000-01-01 00:06:00 12 | 2000-01-01 00:09:00 21 | Freq: 3T, dtype: int64 | | Downsample the series into 3 minute bins as above, but close the right | side of the bin interval. | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=&#39;sum&#39;, label=&#39;right&#39;, closed=&#39;right&#39;) | 2000-01-01 00:00:00 0 | 2000-01-01 00:03:00 6 | 2000-01-01 00:06:00 15 | 2000-01-01 00:09:00 15 | Freq: 3T, dtype: int64 | | Upsample the series into 30 second bins. | | &gt;&gt;&gt; series.resample(&#39;30S&#39;)[0:5] #select first 5 rows | 2000-01-01 00:00:00 0 | 2000-01-01 00:00:30 NaN | 2000-01-01 00:01:00 1 | 2000-01-01 00:01:30 NaN | 2000-01-01 00:02:00 2 | Freq: 30S, dtype: float64 309 | | Upsample the series into 30 second bins and fill the ``NaN`` | values using the ``pad`` method. | | &gt;&gt;&gt; series.resample(&#39;30S&#39;, fill_method=&#39;pad&#39;)[0:5] | 2000-01-01 00:00:00 0 | 2000-01-01 00:00:30 0 | 2000-01-01 00:01:00 1 | 2000-01-01 00:01:30 1 | 2000-01-01 00:02:00 2 | Freq: 30S, dtype: int64 | | Upsample the series into 30 second bins and fill the | ``NaN`` values using the ``bfill`` method. | | &gt;&gt;&gt; series.resample(&#39;30S&#39;, fill_method=&#39;bfill&#39;)[0:5] | 2000-01-01 00:00:00 0 | 2000-01-01 00:00:30 1 | 2000-01-01 00:01:00 1 | 2000-01-01 00:01:30 2 | 2000-01-01 00:02:00 2 | Freq: 30S, dtype: int64 | | Pass a custom function to ``how``. | | &gt;&gt;&gt; def custom_resampler(array_like): | ... return np.sum(array_like)+5 | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=custom_resampler) | 2000-01-01 00:00:00 8 | 2000-01-01 00:03:00 17 | 2000-01-01 00:06:00 26 | Freq: 3T, dtype: int64 | | sample(self, n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) | Returns a random sample of items from an axis of object. | | .. versionadded:: 0.16.1 | | 【参数】 | ----------| n : int, optional | Number of items from axis to return. Cannot be used withfrac. | Default = 1 iffrac= None. | frac : float, optional | Fraction of axis items to return. Cannot be used withn. | replace : boolean, optional | Sample with or without replacement. Default = False. | weights : str or ndarray-like, optional | Default &#39;None&#39; results in equal probability weighting. | If passed a Series, will align with target object on index. Index | values in weights not found in sampled object will be ignored and | index values in sampled object not in weights will be assigned | weights of zero. | If called on a DataFrame, will accept the name of a column | when axis = 0. | Unless weights are a Series, weights must be same length as axis 310 | being sampled. | If weights do not sum to 1, they will be normalized to sum to 1. | Missing values in the weights column will be treated as zero. | inf and -inf values not allowed. | random_state : int or numpy.random.RandomState, optional | Seed for the random number generator (if int), or numpy RandomState | object. | axis : int or string, optional | Axis to sample. Accepts axis number or name. Default is stat axis | for given data type (0 for Series and DataFrames, 1 for Panels). | | 【返回值】 | -------| A new object of same type as caller. | | select(self, crit, axis=0) | Return data corresponding to axis labels matching criteria | | 【参数】 | ----------| crit : function | To be called on each index (label). Should return True or False | axis : int | | 【返回值】 | -------| selection : type of caller | | set_axis(self, axis, labels) | public verson of axis assignment | | slice_shift(self, periods=1, axis=0) | Equivalent toshiftwithout copying data. The shifted data will | not include the dropped periods and the shifted axis will be smaller | than the original. | | 【参数】 | ----------| periods : int | Number of periods to move, can be positive or negative | | 【注意】 | -----| While theslice_shiftis faster thanshift, you may pay for it | later during alignment. | | 【返回值】 | -------| shifted : same type as caller | | sort_index(self, axis=0, level=None, ascending=True, inplace=False, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;, sort_remaining=True) | Sort object by labels (along an axis) | | 【参数】 | ----------| axis : axes to direct sorting 311 | level : int or level name or list of ints or list of level names | if not None, sort on values in specified index level(s) | ascending : boolean, default True | Sort ascending vs. descending | inplace : bool | if True, perform operation in-place | kind : {quicksort,mergesort,heapsort} | Choice of sorting algorithm. 参见：ndarray.np.sort for more information. |mergesortis the only stable algorithm. For DataFrames, this option is | only applied when sorting on a single column or label. | na_position : {&#39;first&#39;, &#39;last&#39;} |firstputs NaNs at the beginning,lastputs NaNs at the end | sort_remaining : bool | if true and sorting by level and index is multilevel, sort by other levels | too (in order) after sorting by specified level | | 【返回值】 | -------| sorted_obj : NDFrame | | sort_values(self, by, axis=0, ascending=True, inplace=False, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;) | | squeeze(self) | squeeze length 1 dimensions | | swapaxes(self, axis1, axis2, copy=True) | Interchange axes and swap values axes appropriately | | 【返回值】 | -------| y : same as input | | swaplevel(self, i, j, axis=0) | Swap levels i and j in a MultiIndex on a particular axis | | 【参数】 | ----------| i, j : int, string (can be mixed) | Level of index to be swapped. Can pass level name as string. | | 【返回值】 | -------| swapped : type of caller (new object) | | take(self, indices, axis=0, convert=True, is_copy=True) | Analogous to ndarray.take | | 【参数】 | ----------| indices : list / array of ints | axis : int, default 0 | convert : translate neg to pos indices (default) | is_copy : mark the returned frame as a copy | | 【返回值】 | -------| taken : type of caller 312 | | to_clipboard(self, excel=None, sep=None, **kwargs) | Attempt to write text representation of object to the system clipboard | This can be pasted into Excel, for example. | | 【参数】 | ----------| excel : boolean, defaults to True | if True, use the provided separator, writing in a csv | format for allowing easy pasting into excel. | if False, write a string representation of the object | to the clipboard | sep : optional, defaults to tab | other keywords are passed to to_csv | | 【注意】 | -----| Requirements for your platform | - Linux: xclip, or xsel (with gtk or PyQt4 modules) | - Windows: none | - OS X: none | | to_dense(self) | Return dense representation of NDFrame (as opposed to sparse) | | to_hdf(self, path_or_buf, key, **kwargs) | activate the HDFStore | | 【参数】 | ----------| path_or_buf : the path (string) or HDFStore object | key : string | indentifier for the group in the store | mode : optional, {&#39;a&#39;, &#39;w&#39;, &#39;r&#39;, &#39;r+&#39;}, default &#39;a&#39; | | ``&#39;r&#39;`` | Read-only; no data can be modified. | ``&#39;w&#39;`` | Write; a new file is created (an existing file with the same | name would be deleted). | ``&#39;a&#39;`` | Append; an existing file is opened for reading and writing, | and if the file does not exist it is created. | ``&#39;r+&#39;`` | It is similar to ``&#39;a&#39;``, but the file must already exist. | format : &#39;fixed(f)|table(t)&#39;, default is &#39;fixed&#39; | fixed(f) : Fixed format | Fast writing/reading. Not-appendable, nor searchable | table(t) : Table format | Write as a PyTables Table structure which may perform | worse but allow more flexible operations like searching | / selecting subsets of the data | append : boolean, default False | For Table formats, append the input data to the existing | complevel : int, 1-9, default 0 | If a complib is specified compression will be applied | where possible 313 | complib : {&#39;zlib&#39;, &#39;bzip2&#39;, &#39;lzo&#39;, &#39;blosc&#39;, None}, default None | If complevel is &gt; 0 apply compression to objects written | in the store wherever possible | fletcher32 : bool, default False | If applying compression use the fletcher32 checksum | dropna : boolean, default False. | If true, ALL nan rows will not be written to store. | | to_json(self, path_or_buf=None, orient=None, date_format=&#39;epoch&#39;, double_precision=10, force_ascii=True, date_unit=&#39;ms&#39;, default_handler=None) | Convert the object to a JSON string. | | Note NaN&#39;s and None will be converted to null and datetime objects | will be converted to UNIX timestamps. | | 【参数】 | ----------| path_or_buf : the path or buffer to write the result string | if this is None, return a StringIO of the converted string | orient : string | | * Series | | - default is &#39;index&#39; | - allowed values are: {&#39;split&#39;,&#39;records&#39;,&#39;index&#39;} | | * DataFrame | | - default is &#39;columns&#39; | - allowed values are: | {&#39;split&#39;,&#39;records&#39;,&#39;index&#39;,&#39;columns&#39;,&#39;values&#39;} | | * The format of the JSON string | | - split : dict like | {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]} | - records : list like | [{column -&gt; value}, ... , {column -&gt; value}] | - index : dict like {index -&gt; {column -&gt; value}} | - columns : dict like {column -&gt; {index -&gt; value}} | - values : just the values array | | date_format : {&#39;epoch&#39;, &#39;iso&#39;} | Type of date conversion.epoch= epoch milliseconds, |iso= ISO8601, default is epoch. | double_precision : The number of decimal places to use when encoding | floating point values, default 10. | force_ascii : force encoded string to be ASCII, default True. | date_unit : string, default &#39;ms&#39; (milliseconds) | The time unit to encode to, governs timestamp and ISO8601 | precision. One of &#39;s&#39;, &#39;ms&#39;, &#39;us&#39;, &#39;ns&#39; for second, millisecond, | microsecond, and nanosecond respectively. | default_handler : callable, default None | Handler to call if object cannot otherwise be converted to a | suitable format for JSON. Should receive a single argument which is | the object to convert and return a serialisable object. | 314 | 【返回值】 | -------| same type as input object with filtered info axis | | to_msgpack(self, path_or_buf=None, **kwargs) | msgpack (serialize) object to input file path | | THIS IS AN EXPERIMENTAL LIBRARY and the storage format | may not be stable until a future release. | | 【参数】 | ----------| path : string File path, buffer-like, or None | if None, return generated string | append : boolean whether to append to an existing msgpack | (default is False) | compress : type of compressor (zlib or blosc), default to None (no | compression) | | to_pickle(self, path) | Pickle (serialize) object to input file path | | 【参数】 | ----------| path : string | File path | | to_sql(self, name, con, flavor=&#39;sqlite&#39;, schema=None, if_exists=&#39;fail&#39;, index=True, index_label=None, chunksize=None, dtype=None) | Write records stored in a DataFrame to a SQL database. | | 【参数】 | ----------| name : string | Name of SQL table | con : SQLAlchemy engine or DBAPI2 connection (legacy mode) | Using SQLAlchemy makes it possible to use any DB supported by that | library. | If a DBAPI2 object, only sqlite3 is supported. | flavor : {&#39;sqlite&#39;, &#39;mysql&#39;}, default &#39;sqlite&#39; | The flavor of SQL to use. Ignored when using SQLAlchemy engine. | &#39;mysql&#39; is deprecated and will be removed in future versions, but it | will be further supported through SQLAlchemy engines. | schema : string, default None | Specify the schema (if database flavor supports this). If None, use | default schema. | if_exists : {&#39;fail&#39;, &#39;replace&#39;, &#39;append&#39;}, default &#39;fail&#39; | - fail: If table exists, do nothing. | - replace: If table exists, drop it, recreate it, and insert data. | - append: If table exists, insert data. Create if does not exist. | index : boolean, default True | Write DataFrame index as a column. | index_label : string or sequence, default None | Column label for index column(s). If None is given (default) and | `index` is True, then the index names are used. | A sequence should be given if the DataFrame uses MultiIndex. | chunksize : int, default None 315 | If not None, then rows will be written in batches of this size at a | time. If None, all rows will be written at once. | dtype : dict of column name to SQL type, default None | Optional specifying the datatype for columns. The SQL type should | be a SQLAlchemy type, or a string for sqlite3 fallback connection. | | truncate(self, before=None, after=None, axis=None, copy=True) | Truncates a sorted NDFrame before and/or after some particular | dates. | | 【参数】 | ----------| before : date | Truncate before date | after : date | Truncate after date | axis : the truncation axis, defaults to the stat axis | copy : boolean, default is True, | return a copy of the truncated section | | 【返回值】 | -------| truncated : type of caller | | tz_convert(self, tz, axis=0, level=None, copy=True) | Convert tz-aware axis to target time zone. | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to convert | level : int, str, default None | If axis ia a MultiIndex, convert a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data | | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError | If the axis is tz-naive. | | tz_localize(self, tz, axis=0, level=None, copy=True, ambiguous=&#39;raise&#39;) | Localize tz-naive TimeSeries to target time zone | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to localize | level : int, str, default None | If axis ia a MultiIndex, localize a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data 316 | ambiguous : &#39;infer&#39;, bool-ndarray, &#39;NaT&#39;, default &#39;raise&#39; | - &#39;infer&#39; will attempt to infer fall dst-transition hours based on order | - bool-ndarray where True signifies a DST time, False designates | a non-DST time (note that this flag is only applicable for ambiguous times) | - &#39;NaT&#39; will return NaT where there are ambiguous times | - &#39;raise&#39; will raise an AmbiguousTimeError if there are ambiguous times | infer_dst : boolean, default False (DEPRECATED) | Attempt to infer fall dst-transition hours based on order | | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError | If the TimeSeries is tz-aware and tz is not None. | | where(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True) | Return an object of same shape as self and whose corresponding | entries are from self where cond is True and otherwise are from other. | | 【参数】 | ----------| cond : boolean NDFrame or array | other : scalar or NDFrame | inplace : boolean, default False | Whether to perform the operation in place on the data | axis : alignment axis if needed, default None | level : alignment level if needed, default None | try_cast : boolean, default False | try to cast the result back to the input type (if possible), | raise_on_error : boolean, default True | Whether to raise on invalid data types (e.g. trying to where on | strings) | | 【返回值】 | -------| wh : same type as caller | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.generic.NDFrame: | | at | Fast label-based scalar accessor | | Similarly toloc,atprovides **label** based scalar lookups. | You can also set using these indexers. | | axes | Return index label(s) of the internal NDFrame | | blocks | Internal property, property synonym for as_blocks() | | dtypes | Return the dtypes in this object | 317 | empty | True if NDFrame is entirely empty [no items] | | ftypes | Return the ftypes (indication of sparse/dense and dtype) | in this object. | | iat | Fast integer location scalar accessor. | | Similarly toiloc,iatprovides **integer** based lookups. | You can also set using these indexers. | | iloc | Purely integer-location based indexing for selection by position. | |.iloc[]is primarily integer position based (from0to |length-1of the axis), but may also be used with a boolean | array. | | Allowed inputs are: | | - An integer, e.g.5. | - A list or array of integers, e.g.[4, 3, 0]. | - A slice object with ints, e.g.1:7. | - A boolean array. | |.ilocwill raiseIndexErrorif a requested indexer is | out-of-bounds, except *slice* indexers which allow out-of-bounds | indexing (this conforms with python/numpy *slice* semantics). | | See more at :ref:`Selection by Position &lt;indexing.integer&gt;` | | ix | A primarily label-location based indexer, with integer position | fallback. | |.ix[]supports mixed integer and label based access. It is | primarily label based, but will fall back to integer positional | access unless the corresponding axis is of integer type. | |.ixis the most general indexer and will support any of the | inputs in.locand.iloc..ixalso supports floating | point label schemes..ixis exceptionally useful when dealing | with mixed positional and label based hierachical indexes. | | However, when an axis is integer based, ONLY label based access | and not positional access is supported. Thus, in such cases, it&#39;s | usually better to be explicit and use.ilocor.loc. | | See more at :ref:`Advanced Indexing &lt;advanced&gt;`. | | loc | Purely label-location based indexer for selection by label. | |.loc[]is primarily label based, but may also be used with a | boolean array. 318 | | Allowed inputs are: | | - A single label, e.g.5or‘a’, (note that5is | interpreted as a *label* of the index, and **never** as an | integer position along the index). | - A list or array of labels, e.g.[‘a’, ‘b’, ‘c’]. | - A slice object with labels, e.g.‘a’:’f’(note that contrary | to usual python slices, **both** the start and the stop are included!). | - A boolean array. | |.locwill raise aKeyErrorwhen the items are not found. | | See more at :ref:`Selection by Label &lt;indexing.label&gt;` | | ndim | Number of axes / array dimensions | | shape | Return a tuple of axis dimensions | | size | number of elements in the NDFrame | | values | Numpy representation of NDFrame | | 【注意】 | -----| The dtype will be a lower-common-denominator dtype (implicit | upcasting); that is to say if the dtypes (even of numeric types) | are mixed, the one that accommodates all will be chosen. Use this | with care if you are not dealing with the blocks. | | e.g. If the dtypes are float16 and float32, dtype will be upcast to | float32. If dtypes are int32 and uint8, dtype will be upcase to | int32. | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.generic.NDFrame: | | is_copy = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. 319 | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.StringMixin: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) Panel4D Panel4D 模块所属：pandas.core.panelnd: 类定义：Panel4D(pandas.core.panel.Panel) | Panel4D is a 4-Dimensional named container very much like a Panel, but | having 4 named dimensions. It is intended as a test bed for more | N-Dimensional named containers. | | 【参数】 | ----------| data : ndarray (labels x items x major x minor), or dict of Panels | | labels : Index or array-like : axis=0 | items : Index or array-like : axis=1 | major_axis : Index or array-like: axis=2 | minor_axis : Index or array-like: axis=3 | | dtype : dtype, default None | Data type to force, otherwise infer | copy : boolean, default False | Copy data from inputs. Only affects DataFrame / 2d ndarray input | | 【方法排序】 | Panel4D | pandas.core.panel.Panel | pandas.core.generic.NDFrame | pandas.core.base.PandasObject 320 | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __init__ = panel4d_init(self, data=None, labels=None, items=None, major_axis=None, minor_axis=None, copy=False, dtype=None) | | add(self, other, axis=0) | Addition of series and other, element-wise (binary operator `add`). | Equivalent topanel + other. | | 【参数】 | ----------| other : Panel or Panel4D | axis : {labels, items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel4D | | 【参见】 | --------| Panel4D.radd | | all(self, axis=None, bool_only=None, skipna=None, level=None, **kwargs) | Return whether all elements are True over requested axis | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | bool_only : boolean, default None | Include only boolean data. If None, will attempt to use everything, | then use only boolean data | | 【返回值】 | -------| all : Panel or Panel4D (if level specified) | | any(self, axis=None, bool_only=None, skipna=None, level=None, **kwargs) | Return whether any element is True over requested axis | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None 321 | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | bool_only : boolean, default None | Include only boolean data. If None, will attempt to use everything, | then use only boolean data | | 【返回值】 | -------| any : Panel or Panel4D (if level specified) | | compound(self, axis=None, skipna=None, level=None) | Return the compound percentage of the values for the requested axis | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| compounded : Panel or Panel4D (if level specified) | | cummax = max(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative max over requested axis. | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| max : Panel | | cummin = min(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative min over requested axis. | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| min : Panel 322 | | cumprod = prod(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative prod over requested axis. | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| prod : Panel | | cumsum = sum(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative sum over requested axis. | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| sum : Panel | | div = truediv(self, other, axis=0) | | divide = truediv(self, other, axis=0) | | dropna = func(self, *args, **kwargs) | | eq(self, other) | Wrapper for comparison method eq | | filter = func(self, *args, **kwargs) | | floordiv(self, other, axis=0) | Integer division of series and other, element-wise (binary operator `floordiv`). | Equivalent topanel // other. | | 【参数】 | ----------| other : Panel or Panel4D | axis : {labels, items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel4D | | 【参见】 | --------| Panel4D.rfloordiv 323 | | ge(self, other) | Wrapper for comparison method ge | | groupby = func(self, *args, **kwargs) | | gt(self, other) | Wrapper for comparison method gt | | join = func(self, *args, **kwargs) | | kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return unbiased kurtosis over requested axis using Fishers definition of | kurtosis (kurtosis of normal == 0.0). Normalized by N-1 | | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| kurt : Panel or Panel4D (if level specified) | | kurtosis = kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | | le(self, other) | Wrapper for comparison method le | | lt(self, other) | Wrapper for comparison method lt | | mad(self, axis=None, skipna=None, level=None) | Return the mean absolute deviation of the values for the requested axis | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | 324 | 【返回值】 | -------| mad : Panel or Panel4D (if level specified) | | max(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | This method returns the maximum of the values in the object. If you | want the *index* of the maximum, useidxmax. This is the | equivalent of thenumpy.ndarraymethodargmax. | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| max : Panel or Panel4D (if level specified) | | mean(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the mean of the values for the requested axis | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| mean : Panel or Panel4D (if level specified) | | median(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the median of the values for the requested axis | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a 325 | particular level, collapsing into a Panel | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| median : Panel or Panel4D (if level specified) | | min(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | This method returns the minimum of the values in the object. If you | want the *index* of the minimum, useidxmin. This is the | equivalent of thenumpy.ndarraymethodargmin. | | 【参数】 | ----------| axis : {labels (0), items (1), major_axis (2), minor_axis (3)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a Panel | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| min : Panel or Panel4D (if level specified) | | mod(self, other, axis=0) | Modulo of series and other, element-wise (binary operator `mod`). | Equivalent topanel % other. | | 【参数】 | ----------| other : Panel or Panel4D | axis : {labels, items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel4D | | 【参见】 | --------| Panel4D.rmod | | mul(self, other, axis=0) | Multiplication of series and other, element-wise (binary operator `mul`). | Equivalent topanel other. | | 【参数】 | ----------| other : Panel or Panel4D | axis : {labels, items, major_axis, minor_axis} 326 | Axis to broadcast over | | 【返回值】 | -------| Panel4D | | 【参见】 | --------| Panel4D.rmul | | multiply = mul(self, other, axis=0) | | ne(self, other) | Wrapper for comparison method ne | | pow(self, other, axis=0) | Exponential power of series and other, element-wise (binary operator `pow`). | Equivalent topanel other``.|| 【参数】| ———-| other : Panel or Panel4D| axis : {labels, items, major_axis, minor_axis}| Axis to broadcast over|| 【返回值】| ——-| Panel4D|| 【参见】| ——–| Panel4D.rpow|| prod(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return the product of the values for the requested axis|| 【参数】| ———-| axis : {labels (0), items (1), major_axis (2), minor_axis (3)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Panel| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| prod : Panel or Panel4D (if level specified)|| product = prod(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)|| radd(self, other, axis=0)| Addition of series and other, element-wise (binary operator radd).327| Equivalent to other + panel.|| 【参数】| ———-| other : Panel or Panel4D| axis : {labels, items, major_axis, minor_axis}| Axis to broadcast over|| 【返回值】| ——-| Panel4D|| 【参见】| ——–| Panel4D.add|| rdiv = rtruediv(self, other, axis=0)|| rfloordiv(self, other, axis=0)| Integer division of series and other, element-wise (binary operator rfloordiv).| Equivalent to other // panel.|| 【参数】| ———-| other : Panel or Panel4D| axis : {labels, items, major_axis, minor_axis}| Axis to broadcast over|| 【返回值】| ——-| Panel4D|| 【参见】| ——–| Panel4D.floordiv|| rmod(self, other, axis=0)| Modulo of series and other, element-wise (binary operator rmod).| Equivalent to other % panel.|| 【参数】| ———-| other : Panel or Panel4D| axis : {labels, items, major_axis, minor_axis}| Axis to broadcast over|| 【返回值】| ——-| Panel4D|| 【参见】| ——–| Panel4D.mod|| rmul(self, other, axis=0)| Multiplication of series and other, element-wise (binary operator rmul).328| Equivalent to ``other panel. | | 【参数】 | ----------| other : Panel or Panel4D | axis : {labels, items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel4D | | 【参见】 | --------| Panel4D.mul | | rpow(self, other, axis=0) | Exponential power of series and other, element-wise (binary operator `rpow`). | Equivalent toother panel. | | 【参数】 | ----------| other : Panel or Panel4D | axis : {labels, items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel4D | | 【参见】 | --------| Panel4D.pow | | rsub(self, other, axis=0) | Subtraction of series and other, element-wise (binary operator `rsub`). | Equivalent toother - panel. | | 【参数】 | ----------| other : Panel or Panel4D | axis : {labels, items, major_axis, minor_axis} | Axis to broadcast over | | 【返回值】 | -------| Panel4D | | 【参见】 | --------| Panel4D.sub | | rtruediv(self, other, axis=0) | Floating division of series and other, element-wise (binary operator `rtruediv`). | Equivalent toother / panel``.|329| 【参数】| ———-| other : Panel or Panel4D| axis : {labels, items, major_axis, minor_axis}| Axis to broadcast over|| 【返回值】| ——-| Panel4D|| 【参见】| ——–| Panel4D.truediv|| sem(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, kwargs)| Return unbiased standard error of the mean over requested axis.|| Normalized by N-1 by default. This can be changed using the ddof argument|| 【参数】| ———-| axis : {labels (0), items (1), major_axis (2), minor_axis (3)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Panel| ddof : int, default 1| degrees of freedom| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| sem : Panel or Panel4D (if level specified)|| shift = func(self, args, kwargs)|| skew(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return unbiased skew over requested axis| Normalized by N-1|| 【参数】| ———-| axis : {labels (0), items (1), major_axis (2), minor_axis (3)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Panel| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|330| 【返回值】| ——-| skew : Panel or Panel4D (if level specified)|| std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, kwargs)| Return unbiased standard deviation over requested axis.|| Normalized by N-1 by default. This can be changed using the ddof argument|| 【参数】| ———-| axis : {labels (0), items (1), major_axis (2), minor_axis (3)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Panel| ddof : int, default 1| degrees of freedom| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| std : Panel or Panel4D (if level specified)|| sub(self, other, axis=0)| Subtraction of series and other, element-wise (binary operator sub).| Equivalent to panel - other.|| 【参数】| ———-| other : Panel or Panel4D| axis : {labels, items, major_axis, minor_axis}| Axis to broadcast over|| 【返回值】| ——-| Panel4D|| 【参见】| ——–| Panel4D.rsub|| subtract = sub(self, other, axis=0)|| sum(self, axis=None, skipna=None, level=None, numeric_only=None, kwargs)| Return the sum of the values for the requested axis|| 【参数】| ———-| axis : {labels (0), items (1), major_axis (2), minor_axis (3)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA331| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Panel| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-| sum : Panel or Panel4D (if level specified)|| to_excel = func(self, args, kwargs)|| to_frame = func(self, *args, kwargs)|| to_sparse = func(self, args, kwargs)|| truediv(self, other, axis=0)| Floating division of series and other, element-wise (binary operator truediv).| Equivalent to panel / other.|| 【参数】| ———-| other : Panel or Panel4D| axis : {labels, items, major_axis, minor_axis}| Axis to broadcast over|| 【返回值】| ——-| Panel4D|| 【参见】| ——–| Panel4D.rtruediv|| var(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, kwargs)| Return unbiased variance over requested axis.|| Normalized by N-1 by default. This can be changed using the ddof argument|| 【参数】| ———-| axis : {labels (0), items (1), major_axis (2), minor_axis (3)}| skipna : boolean, default True| Exclude NA/null values. If an entire row/column is NA, the result| will be NA| level : int or level name, default None| If the axis is a MultiIndex (hierarchical), count along a| particular level, collapsing into a Panel| ddof : int, default 1| degrees of freedom| numeric_only : boolean, default None| Include only float, int, boolean data. If None, will attempt to use| everything, then use only numeric data|| 【返回值】| ——-332| var : Panel or Panel4D (if level specified)|| ———————————————————————-| Data descriptors defined here:|| items|| labels|| major_axis|| minor_axis|| ———————————————————————-| Methods inherited from pandas.core.panel.Panel:|| add(self, other)| # work only for scalars|| and(self, other)| # work only for scalars|| div = truediv(self, other)| # work only for scalars|| eq(self, other)| Wrapper for comparison method eq|| floordiv(self, other)| # work only for scalars|| ge(self, other)| Wrapper for comparison method ge|| getitem(self, key)|| gt(self, other)| Wrapper for comparison method gt|| iadd = f(self, other)|| imul = f(self, other)|| ipow = f(self, other)|| isub = f(self, other)|| itruediv = f(self, other)|| le(self, other)| Wrapper for comparison method le|| lt(self, other)| Wrapper for comparison method lt|| mod(self, other)| # work only for scalars333|| mul(self, other)| # work only for scalars|| ne(self, other)| Wrapper for comparison method ne|| or(self, other)| # work only for scalars|| pow(self, other)| # work only for scalars|| radd(self, other)| # work only for scalars|| rand(self, other)| # work only for scalars|| rdiv = rtruediv(self, other)| # work only for scalars|| rfloordiv(self, other)| # work only for scalars|| rmod(self, other)| # work only for scalars|| rmul(self, other)| # work only for scalars|| ror(self, other)| # work only for scalars|| rpow(self, other)| # work only for scalars|| rsub(self, other)| # work only for scalars|| rtruediv(self, other)| # work only for scalars|| rxor(self, other)| # work only for scalars|| setitem(self, key, value)|| sub(self, other)| # work only for scalars|| truediv(self, other)| # work only for scalars|| unicode(self)| Return a string representation for a particular Panel|334| Invoked by unicode(df) in py2 only.| Yields a Unicode String in both py2/py3.|| xor(self, other)| # work only for scalars|| align(self, other, kwargs)| Align two object on their axes with the| specified join method for each axis Index|| 【参数】| ———-| other : DataFrame or Series| join : {‘outer’, ‘inner’, ‘left’, ‘right’}, default ‘outer’| axis : allowed axis of the other object, default None| Align on index (0), columns (1), or both (None)| level : int or level name, default None| Broadcast across a level, matching Index values on the| passed MultiIndex level| copy : boolean, default True| Always returns new objects. If copy=False and no reindexing is| required then original objects are returned.| fill_value : scalar, default np.NaN| Value to use for missing values. Defaults to NaN, but can be any| “compatible” value| method : str, default None| limit : int, default None| fill_axis : int or labels for object, default 0| Filling axis, method and limit| broadcast_axis : int or labels for object, default None| Broadcast values along this axis, if aligning two objects of| different dimensions|| .. versionadded:: 0.17.0|| 【返回值】| ——-| (left, right) : (NDFrame, type of other)| Aligned objects|| apply(self, func, axis=’major’, kwargs)| Applies function along axis (or axes) of the Panel|| 【参数】| ———-| func : function| Function to apply to each combination of ‘other’ axes| e.g. if axis = ‘items’, the combination of major_axis/minor_axis| will each be passed as a Series; if axis = (‘items’, ‘major’), DataFrames| of items &amp; major axis will be passed| axis : {‘items’, ‘minor’, ‘major’}, or {0, 1, 2}, or a tuple with two axes| Additional keyword arguments will be passed as keywords to the function|| 【示例】| ——–|| Returns a Panel with the square root of each element335|| &gt;&gt;&gt; p = pd.Panel(np.random.rand(4,3,2))| &gt;&gt;&gt; p.apply(np.sqrt)|| Equivalent to p.sum(1), returning a DataFrame|| &gt;&gt;&gt; p.apply(lambda x: x.sum(), axis=1)|| Equivalent to previous:|| &gt;&gt;&gt; p.apply(lambda x: x.sum(), axis=’minor’)|| Return the shapes of each DataFrame over axis 2 (i.e the shapes of items x major), as a Series|| &gt;&gt;&gt; p.apply(lambda x: x.shape, axis=(0,1))|| 【返回值】| ——-| result : Panel, DataFrame, or Series|| as_matrix(self)| Convert the frame to its Numpy-array representation.|| 【参数】| ———-| columns: list, optional, default:None| If None, return all columns, otherwise, returns specified columns.|| 【返回值】| ——-| values : ndarray| If the caller is heterogeneous and contains booleans or objects,| the result will be of dtype=object. See Notes.||| 【注意】| —–| Return is NOT a Numpy-matrix, rather, a Numpy-array.|| The dtype will be a lower-common-denominator dtype (implicit| upcasting); that is to say if the dtypes (even of numeric types)| are mixed, the one that accommodates all will be chosen. Use this| with care if you are not dealing with the blocks.|| e.g. If the dtypes are float16 and float32, dtype will be upcast to| float32. If dtypes are int32 and uint8, dtype will be upcase to| int32.|| This method is provided for backwards compatibility. Generally,| it is recommended to use ‘.values’.|| 【参见】| ——–| pandas.DataFrame.values|| conform(self, frame, axis=’items’)| Conform input DataFrame to align with chosen axis pair.336|| 【参数】| ———-| frame : DataFrame| axis : {‘items’, ‘major’, ‘minor’}|| Axis the input corresponds to. E.g., if axis=’major’, then| the frame’s columns would be items, and the index would be| values of the minor axis|| 【返回值】| ——-| DataFrame|| count(self, axis=’major’)| Return number of observations over requested axis.|| 【参数】| ———-| axis : {‘items’, ‘major’, ‘minor’} or {0, 1, 2}|| 【返回值】| ——-| count : DataFrame|| fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)| Fill NA/NaN values using the specified method|| 【参数】| ———-| value : scalar, dict, Series, or DataFrame| Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of| values specifying which value to use for each index (for a Series) or| column (for a DataFrame). (values not in the dict/Series/DataFrame will not be| filled). This value cannot be a list.| method : {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None| Method to use for filling holes in reindexed Series| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use NEXT valid observation to fill gap| axis : {0, 1, 2, ‘items’, ‘major_axis’, ‘minor_axis’}| inplace : boolean, default False| If True, fill in place. Note: this will modify any| other views on this object, (e.g. a no-copy slice for a column in a| DataFrame).| limit : int, default None| If method is specified, this is the maximum number of consecutive| NaN values to forward/backward fill. In other words, if there is| a gap with more than this number of consecutive NaNs, it will only| be partially filled. If method is not specified, this is the| maximum number of entries along the entire axis where NaNs will be| filled.| downcast : dict, default is None| a dict of item-&gt;dtype of what to downcast if possible,| or the string ‘infer’ which will try to downcast to an appropriate| equal type (e.g. float64 to int64 if possible)|| 【参见】337| ——–| reindex, asfreq|| 【返回值】| ——-| filled : Panel|| get_value(self, args, kwargs)| Quickly retrieve single value at (item, major, minor) location|| 【参数】| ———-| item : item label (panel item)| major : major axis label (panel item row)| minor : minor axis label (panel item column)| takeable : interpret the passed labels as indexers, default False|| 【返回值】| ——-| value : scalar value|| head(self, n=5)| Returns first n rows|| major_xs(self, key, copy=None)| Return slice of panel along major axis|| 【参数】| ———-| key : object| Major axis label| copy : boolean [deprecated]| Whether to make a copy of the data|| 【返回值】| ——-| y : DataFrame| index -&gt; minor axis, columns -&gt; items|| 【注意】| —–| major_xs is only for getting, not setting values.|| MultiIndex Slicers is a generic way to get/set values on any level or levels| it is a superset of major_xs functionality, see :ref:MultiIndex Slicers &lt;advanced.mi_slicers&gt;|| minor_xs(self, key, copy=None)| Return slice of panel along minor axis|| 【参数】| ———-| key : object| Minor axis label| copy : boolean [deprecated]| Whether to make a copy of the data|338| 【返回值】| ——-| y : DataFrame| index -&gt; major axis, columns -&gt; items|| 【注意】| —–| minor_xs is only for getting, not setting values.|| MultiIndex Slicers is a generic way to get/set values on any level or levels| it is a superset of minor_xs functionality, see :ref:MultiIndex Slicers &lt;advanced.mi_slicers&gt;|| reindex(self, items=None, major_axis=None, minor_axis=None, kwargs)| Conform Panel to new index with optional filling logic, placing| NA/NaN in locations having no value in the previous index. A new object| is produced unless the new index is equivalent to the current one and| copy=False|| 【参数】| ———-| items, major_axis, minor_axis : array-like, optional (can be specified in order, or as| keywords)| New labels / index to conform to. Preferably an Index object to| avoid duplicating data| method : {None, ‘backfill’/‘bfill’, ‘pad’/‘ffill’, ‘nearest’}, optional| method to use for filling holes in reindexed DataFrame.| Please note: this is only applicable to DataFrames/Series with a| monotonically increasing/decreasing index.| default: don’t fill gaps| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use next valid observation to fill gap| nearest: use nearest valid observations to fill gap| copy : boolean, default True| Return a new object, even if the passed indexes are the same| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level| fill_value : scalar, default np.NaN| Value to use for missing values. Defaults to NaN, but can be any| “compatible” value| limit : int, default None| Maximum number of consecutive elements to forward or backward fill| tolerance : optional| Maximum distance between original and new labels for inexact| matches. The values of the index at the matching locations most| satisfy the equation abs(index[indexer] - target) &lt;= tolerance.|| .. versionadded:: 0.17.0|| 【示例】| ——–|| Create a dataframe with some fictional data.|| &gt;&gt;&gt; index = [‘Firefox’, ‘Chrome’, ‘Safari’, ‘IE10’, ‘Konqueror’]| &gt;&gt;&gt; df = pd.DataFrame({| … ‘http_status’: [200,200,404,404,301],339| … ‘response_time’: [0.04, 0.02, 0.07, 0.08, 1.0]},| … index=index)| &gt;&gt;&gt; df| http_status response_time| Firefox 200 0.04| Chrome 200 0.02| Safari 404 0.07| IE10 404 0.08| Konqueror 301 1.00|| Create a new index and reindex the dataframe. By default| values in the new index that do not have corresponding| records in the dataframe are assigned NaN.|| &gt;&gt;&gt; new_index= [‘Safari’, ‘Iceweasel’, ‘Comodo Dragon’, ‘IE10’,| … ‘Chrome’]| &gt;&gt;&gt; df.reindex(new_index)| http_status response_time| Safari 404 0.07| Iceweasel NaN NaN| Comodo Dragon NaN NaN| IE10 404 0.08| Chrome 200 0.02|| We can fill in the missing values by passing a value to| the keyword fill_value. Because the index is not monotonically| increasing or decreasing, we cannot use arguments to the keyword| method to fill the NaN values.|| &gt;&gt;&gt; df.reindex(new_index, fill_value=0)| http_status response_time| Safari 404 0.07| Iceweasel 0 0.00| Comodo Dragon 0 0.00| IE10 404 0.08| Chrome 200 0.02|| &gt;&gt;&gt; df.reindex(new_index, fill_value=’missing’)| http_status response_time| Safari 404 0.07| Iceweasel missing missing| Comodo Dragon missing missing| IE10 404 0.08| Chrome 200 0.02|| To further illustrate the filling functionality in| reindex, we will create a dataframe with a| monotonically increasing index (for example, a sequence| of dates).|| &gt;&gt;&gt; date_index = pd.date_range(‘1/1/2010’, periods=6, freq=’D’)| &gt;&gt;&gt; df2 = pd.DataFrame({“prices”: [100, 101, np.nan, 100, 89, 88]},| index=date_index)| &gt;&gt;&gt; df2| prices| 2010-01-01 100| 2010-01-02 101340| 2010-01-03 NaN| 2010-01-04 100| 2010-01-05 89| 2010-01-06 88|| Suppose we decide to expand the dataframe to cover a wider| date range.|| &gt;&gt;&gt; date_index2 = pd.date_range(‘12/29/2009’, periods=10, freq=’D’)| &gt;&gt;&gt; df2.reindex(date_index2)| prices| 2009-12-29 NaN| 2009-12-30 NaN| 2009-12-31 NaN| 2010-01-01 100| 2010-01-02 101| 2010-01-03 NaN| 2010-01-04 100| 2010-01-05 89| 2010-01-06 88| 2010-01-07 NaN|| The index entries that did not have a value in the original data frame| (for example, ‘2009-12-29’) are by default filled with NaN.| If desired, we can fill in the missing values using one of several| options.|| For example, to backpropagate the last valid value to fill the NaN| values, pass bfill as an argument to the method keyword.|| &gt;&gt;&gt; df2.reindex(date_index2, method=’bfill’)| prices| 2009-12-29 100| 2009-12-30 100| 2009-12-31 100| 2010-01-01 100| 2010-01-02 101| 2010-01-03 NaN| 2010-01-04 100| 2010-01-05 89| 2010-01-06 88| 2010-01-07 NaN|| Please note that the NaN value present in the original dataframe| (at index value 2010-01-03) will not be filled by any of the| value propagation schemes. This is because filling while reindexing| does not look at dataframe values, but only compares the original and| desired indexes. If you do want to fill in the NaN values present| in the original dataframe, use the fillna() method.|| 【返回值】| ——-| reindexed : Panel|| reindex_axis(self, labels, axis=0, method=None, level=None, copy=True, limit=None, fill_value=nan)| Conform input object to new index with optional filling logic,| placing NA/NaN in locations having no value in the previous index. A341| new object is produced unless the new index is equivalent to the| current one and copy=False|| 【参数】| ———-| labels : array-like| New labels / index to conform to. Preferably an Index object to| avoid duplicating data| axis : {0, 1, 2, ‘items’, ‘major_axis’, ‘minor_axis’}| method : {None, ‘backfill’/‘bfill’, ‘pad’/‘ffill’, ‘nearest’}, optional| Method to use for filling holes in reindexed DataFrame:| default: don’t fill gaps| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use next valid observation to fill gap| nearest: use nearest valid observations to fill gap| copy : boolean, default True| Return a new object, even if the passed indexes are the same| level : int or name| Broadcast across a level, matching Index values on the| passed MultiIndex level| limit : int, default None| Maximum number of consecutive elements to forward or backward fill| tolerance : optional| Maximum distance between original and new labels for inexact| matches. The values of the index at the matching locations most| satisfy the equation abs(index[indexer] - target) &lt;= tolerance.|| .. versionadded:: 0.17.0|| 【示例】| ——–| &gt;&gt;&gt; df.reindex_axis([‘A’, ‘B’, ‘C’], axis=1)|| 【参见】| ——–| reindex, reindex_like|| 【返回值】| ——-| reindexed : Panel|| rename(self, items=None, major_axis=None, minor_axis=None, kwargs)| Alter axes input function or functions. Function / dict values must be| unique (1-to-1). Labels not contained in a dict / Series will be left| as-is.|| 【参数】| ———-| items, major_axis, minor_axis : dict-like or function, optional| Transformation to apply to that axis values|| copy : boolean, default True| Also copy underlying data| inplace : boolean, default False| Whether to return a new Panel. If True then value of copy is| ignored.|342| 【返回值】| ——-| renamed : Panel (new object)|| set_value(self, *args, kwargs)| Quickly set single value at (item, major, minor) location|| 【参数】| ———-| item : item label (panel item)| major : major axis label (panel item row)| minor : minor axis label (panel item column)| value : scalar| takeable : interpret the passed labels as indexers, default False|| 【返回值】| ——-| panel : Panel| If label combo is contained, will be reference to calling Panel,| otherwise a new object|| tail(self, n=5)| Returns last n rows|| toLong = wrapper(args, **kwargs)|| to_long = wrapper(args, kwargs)|| transpose(self, *args, kwargs)| Permute the dimensions of the Panel|| 【参数】| ———-| args : three positional arguments: each oneof| {0, 1, 2, ‘items’, ‘major_axis’, ‘minor_axis’}| copy : boolean, default False| Make a copy of the underlying data. Mixed-dtype data will| always result in a copy|| 【示例】| ——–| &gt;&gt;&gt; p.transpose(2, 0, 1)| &gt;&gt;&gt; p.transpose(2, 0, 1, copy=True)|| 【返回值】| ——-| y : same as input|| tshift(self, periods=1, freq=None, axis=’major’)| Shift the time index, using the index’s frequency if available|| 【参数】| ———-| periods : int| Number of periods to move, can be positive or negative| freq : DateOffset, timedelta, or time rule string, default None| Increment to use from datetools module or time rule (e.g. ‘EOM’)343| axis : int or basestring| Corresponds to the axis that contains the Index|| 【注意】| —–| If freq is not specified then tries to use the freq or inferred_freq| attributes of the index. If neither of those attributes exist, a| ValueError is thrown|| 【返回值】| ——-| shifted : NDFrame|| update(self, other, join=’left’, overwrite=True, filter_func=None, raise_conflict=False)| Modify Panel in place using non-NA values from passed| Panel, or object coercible to Panel. Aligns on items|| 【参数】| ———-| other : Panel, or object coercible to Panel| join : How to join individual DataFrames| {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘left’| overwrite : boolean, default True| If True then overwrite values for common keys in the calling panel| filter_func : callable(1d-array) -&gt; 1d-array, default None| Can choose to replace values other than NA. Return True for values| that should be updated| raise_conflict : bool| If True, will raise an error if a DataFrame and other both| contain data in the same place.|| xs(self, key, axis=1, copy=None)| Return slice of panel along selected axis|| 【参数】| ———-| key : object| Label| axis : {‘items’, ‘major’, ‘minor}, default 1/‘major’| copy : boolean [deprecated]| Whether to make a copy of the data|| 【返回值】| ——-| y : ndim(self)-1|| 【注意】| —–| xs is only for getting, not setting values.|| MultiIndex Slicers is a generic way to get/set values on any level or levels| it is a superset of xs functionality, see :ref:MultiIndex Slicers &lt;advanced.mi_slicers&gt;|| ———————————————————————-| Class methods inherited from pandas.core.panel.Panel:|| fromDict = from_dict(data, intersect=False, orient=’items’, dtype=None) from builtins.type344| Construct Panel from dict of DataFrame objects|| 【参数】| ———-| data : dict| {field : DataFrame}| intersect : boolean| Intersect indexes of input DataFrames| orient : {‘items’, ‘minor’}, default ‘items’| The “orientation” of the data. If the keys of the passed dict| should be the items of the result panel, pass ‘items’| (default). Otherwise if the columns of the values of the passed| DataFrame objects should be the items (which in the case of| mixed-dtype data you should do), instead pass ‘minor’| dtype : dtype, default None| Data type to force, otherwise infer|| 【返回值】| ——-| Panel|| from_dict(data, intersect=False, orient=’items’, dtype=None) from builtins.type| Construct Panel from dict of DataFrame objects|| 【参数】| ———-| data : dict| {field : DataFrame}| intersect : boolean| Intersect indexes of input DataFrames| orient : {‘items’, ‘minor’}, default ‘items’| The “orientation” of the data. If the keys of the passed dict| should be the items of the result panel, pass ‘items’| (default). Otherwise if the columns of the values of the passed| DataFrame objects should be the items (which in the case of| mixed-dtype data you should do), instead pass ‘minor’| dtype : dtype, default None| Data type to force, otherwise infer|| 【返回值】| ——-| Panel|| ———————————————————————-| Methods inherited from pandas.core.generic.NDFrame:|| abs(self)|| array(self, dtype=None)|| array_wrap(self, result, context=None)|| bool = nonzero(self)|| contains(self, key)| True if the key is in the info axis|345| delitem(self, key)| Delete item|| finalize(self, other, method=None, kwargs)| propagate metadata from other to self|| 【参数】| ———-| other : the object from which to get the attributes that we are going| to propagate| method : optional, a passed method name ; possibly to take different| types of propagation actions based on this|| getattr(self, name)| After regular attribute access, try looking up the name| This allows simpler access to columns for interactive use.|| getstate(self)|| hash(self)| Return hash(self).|| invert(self)|| iter(self)| Iterate over infor axis|| len(self)| Returns length of info axis|| neg(self)|| nonzero(self)|| setattr(self, name, value)| After regular attribute access, try setting the name| This allows simpler access to columns for interactive use.|| setstate(self, state)|| abs(self)| Return an object with absolute value taken. Only applicable to objects| that are all numeric|| 【返回值】| ——-| abs: type of caller|| add_prefix(self, prefix)| Concatenate prefix string with panel items names.|| 【参数】| ———-| prefix : string|| 【返回值】| ——-346| with_prefix : type of caller|| add_suffix(self, suffix)| Concatenate suffix string with panel items names|| 【参数】| ———-| suffix : string|| 【返回值】| ——-| with_suffix : type of caller|| as_blocks(self, copy=True)| Convert the frame to a dict of dtype -&gt; Constructor Types that each has| a homogeneous dtype.|| NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in| as_matrix)|| 【参数】| ———-| copy : boolean, default True|| .. versionadded: 0.16.1|| 【返回值】| ——-| values : a dict of dtype -&gt; Constructor Types|| asfreq(self, freq, method=None, how=None, normalize=False)| Convert all TimeSeries inside to specified frequency using DateOffset| objects. Optionally provide fill method to pad/backfill missing values.|| 【参数】| ———-| freq : DateOffset object, or string| method : {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}| Method to use for filling holes in reindexed Series| pad / ffill: propagate last valid observation forward to next valid| backfill / bfill: use NEXT valid observation to fill method| how : {‘start’, ‘end’}, default end| For PeriodIndex only, see PeriodIndex.asfreq| normalize : bool, default False| Whether to reset output index to midnight|| 【返回值】| ——-| converted : type of caller|| astype(self, dtype, copy=True, raise_on_error=True, kwargs)| Cast object to input numpy.dtype| Return a copy when copy = True (be really careful with this!)|| 【参数】| ———-| dtype : numpy.dtype or Python type347| raise_on_error : raise on invalid input| kwargs : keyword arguments to pass on to the constructor|| 【返回值】| ——-| casted : type of caller|| at_time(self, time, asof=False)| Select values at particular time of day (e.g. 9:30AM)|| 【参数】| ———-| time : datetime.time or string|| 【返回值】| ——-| values_at_time : type of caller|| between_time(self, start_time, end_time, include_start=True, include_end=True)| Select values between particular times of the day (e.g., 9:00-9:30 AM)|| 【参数】| ———-| start_time : datetime.time or string| end_time : datetime.time or string| include_start : boolean, default True| include_end : boolean, default True|| 【返回值】| ——-| values_between_time : type of caller|| bfill(self, axis=None, inplace=False, limit=None, downcast=None)| Synonym for NDFrame.fillna(method=’bfill’)|| bool(self)| Return the bool of a single element PandasObject| This must be a boolean scalar value, either True or False|| Raise a ValueError if the PandasObject does not have exactly| 1 element, or that element is not boolean|| clip(self, lower=None, upper=None, out=None, axis=None)| Trim values at input threshold(s)|| 【参数】| ———-| lower : float or array_like, default None| upper : float or array_like, default None| axis : int or string axis name, optional| Align object with lower and upper along the given axis.|| 【返回值】| ——-| clipped : Series|348| 【示例】| ——–| &gt;&gt;&gt; df| 0 1| 0 0.335232 -1.256177| 1 -1.367855 0.746646| 2 0.027753 -1.176076| 3 0.230930 -0.679613| 4 1.261967 0.570967| &gt;&gt;&gt; df.clip(-1.0, 0.5)| 0 1| 0 0.335232 -1.000000| 1 -1.000000 0.500000| 2 0.027753 -1.000000| 3 0.230930 -0.679613| 4 0.500000 0.500000| &gt;&gt;&gt; t| 0 -0.3| 1 -0.2| 2 -0.1| 3 0.0| 4 0.1| dtype: float64| &gt;&gt;&gt; df.clip(t, t + 1, axis=0)| 0 1| 0 0.335232 -0.300000| 1 -0.200000 0.746646| 2 0.027753 -0.100000| 3 0.230930 0.000000| 4 1.100000 0.570967|| clip_lower(self, threshold, axis=None)| Return copy of the input with values below given value(s) truncated|| 【参数】| ———-| threshold : float or array_like| axis : int or string axis name, optional| Align object with threshold along the given axis.|| 【参见】| ——–| clip|| 【返回值】| ——-| clipped : same type as input|| clip_upper(self, threshold, axis=None)| Return copy of input with values above given value(s) truncated|| 【参数】| ———-| threshold : float or array_like| axis : int or string axis name, optional| Align object with threshold along the given axis.|349| 【参见】| ——–| clip|| 【返回值】| ——-| clipped : same type as input|| consolidate(self, inplace=False)| Compute NDFrame with “consolidated” internals (data of each dtype| grouped together in a single ndarray). Mainly an internal API function,| but available here to the savvy user|| 【参数】| ———-| inplace : boolean, default False| If False return new object, otherwise modify existing object|| 【返回值】| ——-| consolidated : type of caller|| convert_objects(self, convert_dates=True, convert_numeric=False, convert_timedeltas=True, copy=True)| Attempt to infer better dtype for object columns|| 【参数】| ———-| convert_dates : boolean, default True| If True, convert to date where possible. If ‘coerce’, force| conversion, with unconvertible values becoming NaT.| convert_numeric : boolean, default False| If True, attempt to coerce to numbers (including strings), with| unconvertible values becoming NaN.| convert_timedeltas : boolean, default True| If True, convert to timedelta where possible. If ‘coerce’, force| conversion, with unconvertible values becoming NaT.| copy : boolean, default True| If True, return a copy even if no copy is necessary (e.g. no| conversion was done). Note: This is meant for internal use, and| should not be confused with inplace.|| 【返回值】| ——-| converted : same as input object|| copy(self, deep=True)| Make a copy of this object|| 【参数】| ———-| deep : boolean or string, default True| Make a deep copy, i.e. also copy data|| 【返回值】| ——-| copy : type of caller350|| describe(self, percentiles=None, include=None, exclude=None)| Generate various summary statistics, excluding NaN values.|| 【参数】| ———-| percentiles : array-like, optional| The percentiles to include in the output. Should all| be in the interval [0, 1]. By default percentiles is| [.25, .5, .75], returning the 25th, 50th, and 75th percentiles.| include, exclude : list-like, ‘all’, or None (default)| Specify the form of the returned result. Either:|| - None to both (default). The result will include only numeric-typed| columns or, if none are, only categorical columns.| - A list of dtypes or strings to be included/excluded.| To select all numeric types use numpy numpy.number. To select| categorical objects use type object. 参见：the select_dtypes| documentation. eg. df.describe(include=[‘O’])| - If include is the string ‘all’, the output column-set will| match the input one.|| 【返回值】| ——-| summary: NDFrame of summary statistics|| 【注意】| —–| The output DataFrame index depends on the requested dtypes:|| For numeric dtypes, it will include: count, mean, std, min,| max, and lower, 50, and upper percentiles.|| For object dtypes (e.g. timestamps or strings), the index| will include the count, unique, most common, and frequency of the| most common. Timestamps also include the first and last items.|| For mixed dtypes, the index will be the union of the corresponding| output types. Non-applicable entries will be filled with NaN.| Note that mixed-dtype outputs can only be returned from mixed-dtype| inputs and appropriate use of the include/exclude arguments.|| If multiple values have the highest count, then the| count and most common pair will be arbitrarily chosen from| among those with the highest count.|| The include, exclude arguments are ignored for Series.|| 【参见】| ——–| DataFrame.select_dtypes|| drop(self, labels, axis=0, level=None, inplace=False, errors=’raise’)| Return new object with labels in requested axis removed|| 【参数】| ———-351| labels : single label or list-like| axis : int or axis name| level : int or level name, default None| For MultiIndex| inplace : bool, default False| If True, do operation inplace and return None.| errors : {‘ignore’, ‘raise’}, default ‘raise’| If ‘ignore’, suppress error and existing labels are dropped.|| .. versionadded:: 0.16.1|| 【返回值】| ——-| dropped : type of caller|| equals(self, other)| Determines if two NDFrame objects contain the same elements. NaNs in the| same location are considered equal.|| ffill(self, axis=None, inplace=False, limit=None, downcast=None)| Synonym for NDFrame.fillna(method=’ffill’)|| first(self, offset)| Convenience method for subsetting initial periods of time series data| based on a date offset|| 【参数】| ———-| offset : string, DateOffset, dateutil.relativedelta|| 【示例】| ——–| ts.last(‘10D’) -&gt; First 10 days|| 【返回值】| ——-| subset : type of caller|| get(self, key, default=None)| Get item from object for given key (DataFrame column, Panel slice,| etc.). Returns default value if not found|| 【参数】| ———-| key : object|| 【返回值】| ——-| value : type of items contained in object|| get_dtype_counts(self)| Return the counts of dtypes in this object|| get_ftype_counts(self)| Return the counts of ftypes in this object|| get_values(self)352| same as values (but handles sparseness conversions)|| interpolate(self, method=’linear’, axis=0, limit=None, inplace=False, limit_direction=’forward’, downcast=None, kwargs)| Interpolate values according to different methods.|| Please note that only method=&#39;linear&#39; is supported for DataFrames/Series| with a MultiIndex.|| 【参数】| ———-| method : {‘linear’, ‘time’, ‘index’, ‘values’, ‘nearest’, ‘zero’,| ‘slinear’, ‘quadratic’, ‘cubic’, ‘barycentric’, ‘krogh’,| ‘polynomial’, ‘spline’ ‘piecewise_polynomial’, ‘pchip’}|| ‘linear’: ignore the index and treat the values as equally| spaced. This is the only method supported on MultiIndexes.| default| ‘time’: interpolation works on daily and higher resolution| data to interpolate given length of interval| ‘index’, ‘values’: use the actual numerical values of the index| ‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’,| ‘barycentric’, ‘polynomial’ is passed to| scipy.interpolate.interp1d. Both ‘polynomial’ and ‘spline’| require that you also specify an order (int),| e.g. df.interpolate(method=’polynomial’, order=4).| These use the actual numerical values of the index.| ‘krogh’, ‘piecewise_polynomial’, ‘spline’, and ‘pchip’ are all| wrappers around the scipy interpolation methods of similar| names. These use the actual numerical values of the index. See| the scipy documentation for more on their behavior| here &lt;http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation&gt;| and here &lt;http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html&gt;|| axis : {0, 1}, default 0| 0: fill column-by-column| 1: fill row-by-row| limit : int, default None.| Maximum number of consecutive NaNs to fill.| limit_direction : {‘forward’, ‘backward’, ‘both’}, defaults to ‘forward’| If limit is specified, consecutive NaNs will be filled in this| direction.|| .. versionadded:: 0.17.0|| inplace : bool, default False| Update the NDFrame in place if possible.| downcast : optional, ‘infer’ or None, defaults to None| Downcast dtypes if possible.| kwargs : keyword arguments to pass on to the interpolating function.|| 【返回值】| ——-| Series or DataFrame of same shape interpolated at the NaNs|| 【参见】| ——–| reindex, replace, fillna353|| 【示例】| ——–|| Filling in NaNs|| &gt;&gt;&gt; s = pd.Series([0, 1, np.nan, 3])| &gt;&gt;&gt; s.interpolate()| 0 0| 1 1| 2 2| 3 3| dtype: float64|| isnull(self)| Return a boolean same-sized object indicating if the values are null|| 【参见】| ——–| notnull : boolean inverse of isnull|| iteritems(self)| Iterate over (label, values) on info axis|| This is index for Series, columns for DataFrame, major_axis for Panel,| and so on.|| iterkv(self, args, kwargs)| iteritems alias used to get around 2to3. Deprecated|| keys(self)| Get the ‘info axis’ (see Indexing for more)|| This is index for Series, columns for DataFrame and major_axis for| Panel.|| last(self, offset)| Convenience method for subsetting final periods of time series data| based on a date offset|| 【参数】| ———-| offset : string, DateOffset, dateutil.relativedelta|| 【示例】| ——–| ts.last(‘5M’) -&gt; Last 5 months|| 【返回值】| ——-| subset : type of caller|| mask(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True)| Return an object of same shape as self and whose corresponding| entries are from self where cond is False and otherwise are from other.|| 【参数】354| ———-| cond : boolean NDFrame or array| other : scalar or NDFrame| inplace : boolean, default False| Whether to perform the operation in place on the data| axis : alignment axis if needed, default None| level : alignment level if needed, default None| try_cast : boolean, default False| try to cast the result back to the input type (if possible),| raise_on_error : boolean, default True| Whether to raise on invalid data types (e.g. trying to where on| strings)|| 【返回值】| ——-| wh : same type as caller|| notnull(self)| Return a boolean same-sized object indicating if the values are| not null|| 【参见】| ——–| isnull : boolean inverse of notnull|| pct_change(self, periods=1, fill_method=’pad’, limit=None, freq=None, kwargs)| Percent change over given number of periods.|| 【参数】| ———-| periods : int, default 1| Periods to shift for forming percent change| fill_method : str, default ‘pad’| How to handle NAs before computing percent changes| limit : int, default None| The number of consecutive NAs to fill before stopping| freq : DateOffset, timedelta, or offset alias string, optional| Increment to use from time series API (e.g. ‘M’ or BDay())|| 【返回值】| ——-| chg : NDFrame|| 【注意】| —–|| By default, the percentage change is calculated along the stat| axis: 0, or Index, for DataFrame and 1, or minor for| Panel. You can change this with the axis keyword argument.|| pipe(self, func, *args, kwargs)| Apply func(self, *args, **kwargs)|| .. versionadded:: 0.16.2|| 【参数】| ———-355| func : function| function to apply to the NDFrame.| args, and kwargs are passed into func.| Alternatively a (callable, data_keyword) tuple where| data_keyword is a string indicating the keyword of| callable that expects the NDFrame.| args : positional arguments passed into func.| kwargs : a dictionary of keyword arguments passed into func.|| 【返回值】| ——-| object : the return type of func.|| 【注意】| —–|| Use .pipe when chaining together functions that expect| on Series or DataFrames. Instead of writing|| &gt;&gt;&gt; f(g(h(df), arg1=a), arg2=b, arg3=c)|| You can write|| &gt;&gt;&gt; (df.pipe(h)| … .pipe(g, arg1=a)| … .pipe(f, arg2=b, arg3=c)| … )|| If you have a function that takes the data as (say) the second| argument, pass a tuple indicating which keyword expects the| data. For example, suppose f takes its data as arg2:|| &gt;&gt;&gt; (df.pipe(h)| … .pipe(g, arg1=a)| … .pipe((f, ‘arg2’), arg1=a, arg3=c)| … )|| 【参见】| ——–| pandas.DataFrame.apply| pandas.DataFrame.applymap| pandas.Series.map|| pop(self, item)| Return item and drop from frame. Raise KeyError if not found.|| reindex_like(self, other, method=None, copy=True, limit=None, tolerance=None)| return an object with matching indicies to myself|| 【参数】| ———-| other : Object| method : string or None| copy : boolean, default True| limit : int, default None| Maximum number of consecutive labels to fill for inexact matches.| tolerance : optional356| Maximum distance between labels of the other object and this| object for inexact matches.|| .. versionadded:: 0.17.0|| 【注意】| —–| Like calling s.reindex(index=other.index, columns=other.columns,| method=…)|| 【返回值】| ——-| reindexed : same as input|| rename_axis(self, mapper, axis=0, copy=True, inplace=False)| Alter index and / or columns using input function or functions.| Function / dict values must be unique (1-to-1). Labels not contained in| a dict / Series will be left as-is.|| 【参数】| ———-| mapper : dict-like or function, optional| axis : int or string, default 0| copy : boolean, default True| Also copy underlying data| inplace : boolean, default False|| 【返回值】| ——-| renamed : type of caller|| replace(self, to_replace=None, value=None, inplace=False, limit=None, regex=False, method=’pad’, axis=None)| Replace values given in ‘to_replace’ with ‘value’.|| 【参数】| ———-| to_replace : str, regex, list, dict, Series, numeric, or None|| str or regex:|| - str: string exactly matching to_replace will be replaced| with value| - regex: regexs matching to_replace will be replaced with| value|| list of str, regex, or numeric:|| - First, if to_replace and value are both lists, they| must be the same length.| - Second, if regex=True then all of the strings in both| lists will be interpreted as regexs otherwise they will match| directly. This doesn’t matter much for value since there| are only a few possible substitution regexes you can use.| - str and regex rules apply as above.|| dict:|357| - Nested dictionaries, e.g., {‘a’: {‘b’: nan}}, are read as| follows: look in column ‘a’ for the value ‘b’ and replace it| with nan. You can nest regular expressions as well. Note that| column names (the top-level dictionary keys in a nested| dictionary) cannot be regular expressions.| - Keys map to column names and values map to substitution| values. You can treat this as a special case of passing two| lists except that you are specifying the column to search in.|| None:|| - This means that the regex argument must be a string,| compiled regular expression, or list, dict, ndarray or Series| of such elements. If value is also None then this| must be a nested dictionary or Series.|| See the examples section for examples of each of these.| value : scalar, dict, list, str, regex, default None| Value to use to fill holes (e.g. 0), alternately a dict of values| specifying which value to use for each column (columns not in the| dict will not be filled). Regular expressions, strings and lists or| dicts of such objects are also allowed.| inplace : boolean, default False| If True, in place. Note: this will modify any| other views on this object (e.g. a column form a DataFrame).| Returns the caller if this is True.| limit : int, default None| Maximum size gap to forward or backward fill| regex : bool or same types as to_replace, default False| Whether to interpret to_replace and/or value as regular| expressions. If this is True then to_replace must be a| string. Otherwise, to_replace must be None because this| parameter will be interpreted as a regular expression or a list,| dict, or array of regular expressions.| method : string, optional, {‘pad’, ‘ffill’, ‘bfill’}| The method to use when for replacement, when to_replace is a| list.|| 【参见】| ——–| NDFrame.reindex| NDFrame.asfreq| NDFrame.fillna|| 【返回值】| ——-| filled : NDFrame|| 【Raises 引发错误】| ——| AssertionError| If regex is not a bool and to_replace is not None.| TypeError| If to_replace is a dict and value is not a list,| dict, ndarray, or Series| If to_replace is None and regex is not compilable into a| regular expression or is a list, dict, ndarray, or Series.358| ValueError| If to_replace and value are list s or ndarray s, but| they are not the same length.|| 【注意】| —–| Regex substitution is performed under the hood with re.sub. The| rules for substitution for re.sub are the same.| Regular expressions will only substitute on strings, meaning you| cannot provide, for example, a regular expression matching floating| point numbers and expect the columns in your frame that have a| numeric dtype to be matched. However, if those floating point numbers| are strings, then you can do this.| This method has a lot of options. You are encouraged to experiment| and play with this method to gain intuition about how it works.|| resample(self, rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention=’start’, kind=None,loffset=None, limit=None, base=0)| Convenience method for frequency conversion and resampling of regular| time-series data.|| 【参数】| ———-| rule : string| the offset string or object representing target conversion| how : string| method for down- or re-sampling, default to ‘mean’ for| downsampling| axis : int, optional, default 0| fill_method : string, default None| fill_method for upsampling| closed : {‘right’, ‘left’}| Which side of bin interval is closed| label : {‘right’, ‘left’}| Which bin edge label to label bucket with| convention : {‘start’, ‘end’, ‘s’, ‘e’}| kind : “period”/“timestamp”| loffset : timedelta| Adjust the resampled time labels| limit : int, default None| Maximum size gap to when reindexing with fill_method| base : int, default 0| For frequencies that evenly subdivide 1 day, the “origin” of the| aggregated intervals. For example, for ‘5min’ frequency, base could| range from 0 through 4. Defaults to 0||| 【示例】| ——–|| Start by creating a series with 9 one minute timestamps.|| &gt;&gt;&gt; index = pd.date_range(‘1/1/2000’, periods=9, freq=’T’)| &gt;&gt;&gt; series = pd.Series(range(9), index=index)| &gt;&gt;&gt; series| 2000-01-01 00:00:00 0| 2000-01-01 00:01:00 1359| 2000-01-01 00:02:00 2| 2000-01-01 00:03:00 3| 2000-01-01 00:04:00 4| 2000-01-01 00:05:00 5| 2000-01-01 00:06:00 6| 2000-01-01 00:07:00 7| 2000-01-01 00:08:00 8| Freq: T, dtype: int64|| Downsample the series into 3 minute bins and sum the values| of the timestamps falling into a bin.|| &gt;&gt;&gt; series.resample(‘3T’, how=’sum’)| 2000-01-01 00:00:00 3| 2000-01-01 00:03:00 12| 2000-01-01 00:06:00 21| Freq: 3T, dtype: int64|| Downsample the series into 3 minute bins as above, but label each| bin using the right edge instead of the left. Please note that the| value in the bucket used as the label is not included in the bucket,| which it labels. For example, in the original series the| bucket 2000-01-01 00:03:00 contains the value 3, but the summed| value in the resampled bucket with the label2000-01-01 00:03:00| does not include 3 (if it did, the summed value would be 6, not 3).| To include this value close the right side of the bin interval as| illustrated in the example below this one.|| &gt;&gt;&gt; series.resample(‘3T’, how=’sum’, label=’right’)| 2000-01-01 00:03:00 3| 2000-01-01 00:06:00 12| 2000-01-01 00:09:00 21| Freq: 3T, dtype: int64|| Downsample the series into 3 minute bins as above, but close the right| side of the bin interval.|| &gt;&gt;&gt; series.resample(‘3T’, how=’sum’, label=’right’, closed=’right’)| 2000-01-01 00:00:00 0| 2000-01-01 00:03:00 6| 2000-01-01 00:06:00 15| 2000-01-01 00:09:00 15| Freq: 3T, dtype: int64|| Upsample the series into 30 second bins.|| &gt;&gt;&gt; series.resample(‘30S’)[0:5] #select first 5 rows| 2000-01-01 00:00:00 0| 2000-01-01 00:00:30 NaN| 2000-01-01 00:01:00 1| 2000-01-01 00:01:30 NaN| 2000-01-01 00:02:00 2| Freq: 30S, dtype: float64|| Upsample the series into 30 second bins and fill the NaN| values using the pad method.|360| &gt;&gt;&gt; series.resample(‘30S’, fill_method=’pad’)[0:5]| 2000-01-01 00:00:00 0| 2000-01-01 00:00:30 0| 2000-01-01 00:01:00 1| 2000-01-01 00:01:30 1| 2000-01-01 00:02:00 2| Freq: 30S, dtype: int64|| Upsample the series into 30 second bins and fill the| NaN values using the bfill method.|| &gt;&gt;&gt; series.resample(‘30S’, fill_method=’bfill’)[0:5]| 2000-01-01 00:00:00 0| 2000-01-01 00:00:30 1| 2000-01-01 00:01:00 1| 2000-01-01 00:01:30 2| 2000-01-01 00:02:00 2| Freq: 30S, dtype: int64|| Pass a custom function to how.|| &gt;&gt;&gt; def custom_resampler(array_like):| … return np.sum(array_like)+5|| &gt;&gt;&gt; series.resample(‘3T’, how=custom_resampler)| 2000-01-01 00:00:00 8| 2000-01-01 00:03:00 17| 2000-01-01 00:06:00 26| Freq: 3T, dtype: int64|| sample(self, n=None, frac=None, replace=False, weights=None, random_state=None, axis=None)| Returns a random sample of items from an axis of object.|| .. versionadded:: 0.16.1|| 【参数】| ———-| n : int, optional| Number of items from axis to return. Cannot be used with frac.| Default = 1 if frac = None.| frac : float, optional| Fraction of axis items to return. Cannot be used with n.| replace : boolean, optional| Sample with or without replacement. Default = False.| weights : str or ndarray-like, optional| Default ‘None’ results in equal probability weighting.| If passed a Series, will align with target object on index. Index| values in weights not found in sampled object will be ignored and| index values in sampled object not in weights will be assigned| weights of zero.| If called on a DataFrame, will accept the name of a column| when axis = 0.| Unless weights are a Series, weights must be same length as axis| being sampled.| If weights do not sum to 1, they will be normalized to sum to 1.| Missing values in the weights column will be treated as zero.| inf and -inf values not allowed.361| random_state : int or numpy.random.RandomState, optional| Seed for the random number generator (if int), or numpy RandomState| object.| axis : int or string, optional| Axis to sample. Accepts axis number or name. Default is stat axis| for given data type (0 for Series and DataFrames, 1 for Panels).|| 【返回值】| ——-| A new object of same type as caller.|| select(self, crit, axis=0)| Return data corresponding to axis labels matching criteria|| 【参数】| ———-| crit : function| To be called on each index (label). Should return True or False| axis : int|| 【返回值】| ——-| selection : type of caller|| set_axis(self, axis, labels)| public verson of axis assignment|| slice_shift(self, periods=1, axis=0)| Equivalent to shift without copying data. The shifted data will| not include the dropped periods and the shifted axis will be smaller| than the original.|| 【参数】| ———-| periods : int| Number of periods to move, can be positive or negative|| 【注意】| —–| While the slice_shift is faster than shift, you may pay for it| later during alignment.|| 【返回值】| ——-| shifted : same type as caller|| sort_index(self, axis=0, level=None, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’,sort_remaining=True)| Sort object by labels (along an axis)|| 【参数】| ———-| axis : axes to direct sorting| level : int or level name or list of ints or list of level names| if not None, sort on values in specified index level(s)| ascending : boolean, default True| Sort ascending vs. descending362| inplace : bool| if True, perform operation in-place| kind : {quicksort, mergesort, heapsort}| Choice of sorting algorithm. 参见：ndarray.np.sort for more information.| mergesort is the only stable algorithm. For DataFrames, this option is| only applied when sorting on a single column or label.| na_position : {‘first’, ‘last’}| first puts NaNs at the beginning, last puts NaNs at the end| sort_remaining : bool| if true and sorting by level and index is multilevel, sort by other levels| too (in order) after sorting by specified level|| 【返回值】| ——-| sorted_obj : NDFrame|| sort_values(self, by, axis=0, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’)|| squeeze(self)| squeeze length 1 dimensions|| swapaxes(self, axis1, axis2, copy=True)| Interchange axes and swap values axes appropriately|| 【返回值】| ——-| y : same as input|| swaplevel(self, i, j, axis=0)| Swap levels i and j in a MultiIndex on a particular axis|| 【参数】| ———-| i, j : int, string (can be mixed)| Level of index to be swapped. Can pass level name as string.|| 【返回值】| ——-| swapped : type of caller (new object)|| take(self, indices, axis=0, convert=True, is_copy=True)| Analogous to ndarray.take|| 【参数】| ———-| indices : list / array of ints| axis : int, default 0| convert : translate neg to pos indices (default)| is_copy : mark the returned frame as a copy|| 【返回值】| ——-| taken : type of caller|| to_clipboard(self, excel=None, sep=None, kwargs)| Attempt to write text representation of object to the system clipboard| This can be pasted into Excel, for example.363|| 【参数】| ———-| excel : boolean, defaults to True| if True, use the provided separator, writing in a csv| format for allowing easy pasting into excel.| if False, write a string representation of the object| to the clipboard| sep : optional, defaults to tab| other keywords are passed to to_csv|| 【注意】| —–| Requirements for your platform| - Linux: xclip, or xsel (with gtk or PyQt4 modules)| - Windows: none| - OS X: none|| to_dense(self)| Return dense representation of NDFrame (as opposed to sparse)|| to_hdf(self, path_or_buf, key, kwargs)| activate the HDFStore|| 【参数】| ———-| path_or_buf : the path (string) or HDFStore object| key : string| indentifier for the group in the store| mode : optional, {‘a’, ‘w’, ‘r’, ‘r+’}, default ‘a’|| &#39;r&#39;| Read-only; no data can be modified.| &#39;w&#39;| Write; a new file is created (an existing file with the same| name would be deleted).| &#39;a&#39;| Append; an existing file is opened for reading and writing,| and if the file does not exist it is created.| &#39;r+&#39;| It is similar to &#39;a&#39;, but the file must already exist.| format : ‘fixed(f)|table(t)’, default is ‘fixed’| fixed(f) : Fixed format| Fast writing/reading. Not-appendable, nor searchable| table(t) : Table format| Write as a PyTables Table structure which may perform| worse but allow more flexible operations like searching| / selecting subsets of the data| append : boolean, default False| For Table formats, append the input data to the existing| complevel : int, 1-9, default 0| If a complib is specified compression will be applied| where possible| complib : {‘zlib’, ‘bzip2’, ‘lzo’, ‘blosc’, None}, default None| If complevel is &gt; 0 apply compression to objects written| in the store wherever possible| fletcher32 : bool, default False364| If applying compression use the fletcher32 checksum| dropna : boolean, default False.| If true, ALL nan rows will not be written to store.|| to_json(self, path_or_buf=None, orient=None, date_format=’epoch’, double_precision=10, force_ascii=True, date_unit=’ms’,default_handler=None)| Convert the object to a JSON string.|| Note NaN’s and None will be converted to null and datetime objects| will be converted to UNIX timestamps.|| 【参数】| ———-| path_or_buf : the path or buffer to write the result string| if this is None, return a StringIO of the converted string| orient : string|| Series|| - default is ‘index’| - allowed values are: {‘split’,’records’,’index’}|| DataFrame|| - default is ‘columns’| - allowed values are:| {‘split’,’records’,’index’,’columns’,’values’}|| The format of the JSON string|| - split : dict like| {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]}| - records : list like| [{column -&gt; value}, … , {column -&gt; value}]| - index : dict like {index -&gt; {column -&gt; value}}| - columns : dict like {column -&gt; {index -&gt; value}}| - values : just the values array|| date_format : {‘epoch’, ‘iso’}| Type of date conversion. epoch = epoch milliseconds,| iso`` = ISO8601, default is epoch. | double_precision : The number of decimal places to use when encoding | floating point values, default 10. | force_ascii : force encoded string to be ASCII, default True. | date_unit : string, default &#39;ms&#39; (milliseconds) | The time unit to encode to, governs timestamp and ISO8601 | precision. One of &#39;s&#39;, &#39;ms&#39;, &#39;us&#39;, &#39;ns&#39; for second, millisecond, | microsecond, and nanosecond respectively. | default_handler : callable, default None | Handler to call if object cannot otherwise be converted to a | suitable format for JSON. Should receive a single argument which is | the object to convert and return a serialisable object. | | 【返回值】 | -------| same type as input object with filtered info axis | 365 | to_msgpack(self, path_or_buf=None, **kwargs) | msgpack (serialize) object to input file path | | THIS IS AN EXPERIMENTAL LIBRARY and the storage format | may not be stable until a future release. | | 【参数】 | ----------| path : string File path, buffer-like, or None | if None, return generated string | append : boolean whether to append to an existing msgpack | (default is False) | compress : type of compressor (zlib or blosc), default to None (no | compression) | | to_pickle(self, path) | Pickle (serialize) object to input file path | | 【参数】 | ----------| path : string | File path | | to_sql(self, name, con, flavor=&#39;sqlite&#39;, schema=None, if_exists=&#39;fail&#39;, index=True, index_label=None, chunksize=None, dtype=None) | Write records stored in a DataFrame to a SQL database. | | 【参数】 | ----------| name : string | Name of SQL table | con : SQLAlchemy engine or DBAPI2 connection (legacy mode) | Using SQLAlchemy makes it possible to use any DB supported by that | library. | If a DBAPI2 object, only sqlite3 is supported. | flavor : {&#39;sqlite&#39;, &#39;mysql&#39;}, default &#39;sqlite&#39; | The flavor of SQL to use. Ignored when using SQLAlchemy engine. | &#39;mysql&#39; is deprecated and will be removed in future versions, but it | will be further supported through SQLAlchemy engines. | schema : string, default None | Specify the schema (if database flavor supports this). If None, use | default schema. | if_exists : {&#39;fail&#39;, &#39;replace&#39;, &#39;append&#39;}, default &#39;fail&#39; | - fail: If table exists, do nothing. | - replace: If table exists, drop it, recreate it, and insert data. | - append: If table exists, insert data. Create if does not exist. | index : boolean, default True | Write DataFrame index as a column. | index_label : string or sequence, default None | Column label for index column(s). If None is given (default) and |indexis True, then the index names are used. | A sequence should be given if the DataFrame uses MultiIndex. | chunksize : int, default None | If not None, then rows will be written in batches of this size at a | time. If None, all rows will be written at once. | dtype : dict of column name to SQL type, default None | Optional specifying the datatype for columns. The SQL type should 366 | be a SQLAlchemy type, or a string for sqlite3 fallback connection. | | truncate(self, before=None, after=None, axis=None, copy=True) | Truncates a sorted NDFrame before and/or after some particular | dates. | | 【参数】 | ----------| before : date | Truncate before date | after : date | Truncate after date | axis : the truncation axis, defaults to the stat axis | copy : boolean, default is True, | return a copy of the truncated section | | 【返回值】 | -------| truncated : type of caller | | tz_convert(self, tz, axis=0, level=None, copy=True) | Convert tz-aware axis to target time zone. | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to convert | level : int, str, default None | If axis ia a MultiIndex, convert a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data | | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError | If the axis is tz-naive. | | tz_localize(self, tz, axis=0, level=None, copy=True, ambiguous=&#39;raise&#39;) | Localize tz-naive TimeSeries to target time zone | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to localize | level : int, str, default None | If axis ia a MultiIndex, localize a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data | ambiguous : &#39;infer&#39;, bool-ndarray, &#39;NaT&#39;, default &#39;raise&#39; | - &#39;infer&#39; will attempt to infer fall dst-transition hours based on order | - bool-ndarray where True signifies a DST time, False designates | a non-DST time (note that this flag is only applicable for ambiguous times) 367 | - &#39;NaT&#39; will return NaT where there are ambiguous times | - &#39;raise&#39; will raise an AmbiguousTimeError if there are ambiguous times | infer_dst : boolean, default False (DEPRECATED) | Attempt to infer fall dst-transition hours based on order | | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError | If the TimeSeries is tz-aware and tz is not None. | | where(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True) | Return an object of same shape as self and whose corresponding | entries are from self where cond is True and otherwise are from other. | | 【参数】 | ----------| cond : boolean NDFrame or array | other : scalar or NDFrame | inplace : boolean, default False | Whether to perform the operation in place on the data | axis : alignment axis if needed, default None | level : alignment level if needed, default None | try_cast : boolean, default False | try to cast the result back to the input type (if possible), | raise_on_error : boolean, default True | Whether to raise on invalid data types (e.g. trying to where on | strings) | | 【返回值】 | -------| wh : same type as caller | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.generic.NDFrame: | | at | Fast label-based scalar accessor | | Similarly to ``loc``, ``at`` provides **label** based scalar lookups. | You can also set using these indexers. | | axes | Return index label(s) of the internal NDFrame | | blocks | Internal property, property synonym for as_blocks() | | dtypes | Return the dtypes in this object | | empty | True if NDFrame is entirely empty [no items] | | ftypes 368 | Return the ftypes (indication of sparse/dense and dtype) | in this object. | | iat | Fast integer location scalar accessor. | | Similarly to ``iloc``, ``iat`` provides **integer** based lookups. | You can also set using these indexers. | | iloc | Purely integer-location based indexing for selection by position. | | ``.iloc[]`` is primarily integer position based (from ``0`` to | ``length-1`` of the axis), but may also be used with a boolean | array. | | Allowed inputs are: | | - An integer, e.g. ``5``. | - A list or array of integers, e.g. ``[4, 3, 0]``. | - A slice object with ints, e.g. ``1:7``. | - A boolean array. | | ``.iloc`` will raise ``IndexError`` if a requested indexer is | out-of-bounds, except *slice* indexers which allow out-of-bounds | indexing (this conforms with python/numpy *slice* semantics). | | See more at :ref:Selection by Position | | ix | A primarily label-location based indexer, with integer position | fallback. | | ``.ix[]`` supports mixed integer and label based access. It is | primarily label based, but will fall back to integer positional | access unless the corresponding axis is of integer type. | | ``.ix`` is the most general indexer and will support any of the | inputs in ``.loc`` and ``.iloc``. ``.ix`` also supports floating | point label schemes. ``.ix`` is exceptionally useful when dealing | with mixed positional and label based hierachical indexes. | | However, when an axis is integer based, ONLY label based access | and not positional access is supported. Thus, in such cases, it&#39;s | usually better to be explicit and use ``.iloc`` or ``.loc``. | | See more at :ref:Advanced Indexing . | | loc | Purely label-location based indexer for selection by label. | | ``.loc[]`` is primarily label based, but may also be used with a | boolean array. | | Allowed inputs are: | | - A single label, e.g. ``5`` or ``&#39;a&#39;``, (note that ``5`` is 369 | interpreted as a *label* of the index, and **never** as an | integer position along the index). | - A list or array of labels, e.g. ``[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]``. | - A slice object with labels, e.g. ``&#39;a&#39;:&#39;f&#39;`` (note that contrary | to usual python slices, **both** the start and the stop are included!). | - A boolean array. | | ``.loc`` will raise a ``KeyError`` when the items are not found. | | See more at :ref:Selection by Label | | ndim | Number of axes / array dimensions | | shape | Return a tuple of axis dimensions | | size | number of elements in the NDFrame | | values | Numpy representation of NDFrame | | 【注意】 | -----| The dtype will be a lower-common-denominator dtype (implicit | upcasting); that is to say if the dtypes (even of numeric types) | are mixed, the one that accommodates all will be chosen. Use this | with care if you are not dealing with the blocks. | | e.g. If the dtypes are float16 and float32, dtype will be upcast to | float32. If dtypes are int32 and uint8, dtype will be upcase to | int32. | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.generic.NDFrame: | | is_copy = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. 370 | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.StringMixin: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) Period Period 模块所属：pandas._period: 类定义：Period(builtins.object) | Represents an period of time | | 【参数】 | ----------| value : Period or compat.string_types, default None | The time period represented (e.g., &#39;4Q2005&#39;) | freq : str, default None | One of pandas period strings or corresponding objects | year : int, default None | month : int, default 1 | quarter : int, default None | day : int, default 1 | hour : int, default 0 | minute : int, default 0 | second : int, default 0 | | 【方法定义】 | | __add__(self, value, /) | Return self+value. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self&gt;=value. | 371 | __gt__(self, value, /) | Return self&gt;value. | | __hash__(self, /) | Return hash(self). | | __init__(self, /, *args, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __le__(self, value, /) | Return self&lt;=value. | | __lt__(self, value, /) | Return self&lt;value. | | __ne__(self, value, /) | Return self!=value. | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __radd__(self, value, /) | Return value+self. | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | __rsub__(self, value, /) | Return value-self. | | __setstate__(...) | | __str__(self, /) | Return str(self). | | __sub__(self, value, /) | Return self-value. | | __unicode__(...) | Return a string representation for a particular DataFrame | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | asfreq(...) | Convert Period to desired frequency, either at the start or end of the | interval | | 【参数】 | ----------| freq : string | how : {&#39;E&#39;, &#39;S&#39;, &#39;end&#39;, &#39;start&#39;}, default &#39;end&#39; | Start or end of the timespan | 372 | 【返回值】 | -------| resampled : Period | | now(...) from builtins.type | | strftime(...) | Returns the string representation of the :class:Period, depending | on the selected :keyword:format. :keyword:formatmust be a string | containing one or several directives. The method recognizes the same | directives as the :func:time.strftimefunction of the standard Python | distribution, as well as the specific additional directives ``%f``, | ``%F``, ``%q``. (formatting &amp; docs originally from scikits.timeries) | | +-----------+--------------------------------+-------+ | | Directive | Meaning | Notes | | +===========+================================+=======+ | | ``%a`` | Locale&#39;s abbreviated weekday | | | | | name. | | | +-----------+--------------------------------+-------+ | | ``%A`` | Locale&#39;s full weekday name. | | | +-----------+--------------------------------+-------+ | | ``%b`` | Locale&#39;s abbreviated month | | | | | name. | | | +-----------+--------------------------------+-------+ | | ``%B`` | Locale&#39;s full month name. | | | +-----------+--------------------------------+-------+ | | ``%c`` | Locale&#39;s appropriate date and | | | | | time representation. | | | +-----------+--------------------------------+-------+ | | ``%d`` | Day of the month as a decimal | | | | | number [01,31]. | | | +-----------+--------------------------------+-------+ | | ``%f`` | &#39;Fiscal&#39; year without a | \\(1) | | | | century as a decimal number | | | | | [00,99] | | | +-----------+--------------------------------+-------+ | | ``%F`` | &#39;Fiscal&#39; year with a century | \\(2) | | | | as a decimal number | | | +-----------+--------------------------------+-------+ | | ``%H`` | Hour (24-hour clock) as a | | | | | decimal number [00,23]. | | | +-----------+--------------------------------+-------+ | | ``%I`` | Hour (12-hour clock) as a | | | | | decimal number [01,12]. | | | +-----------+--------------------------------+-------+ | | ``%j`` | Day of the year as a decimal | | | | | number [001,366]. | | | +-----------+--------------------------------+-------+ | | ``%m`` | Month as a decimal number | | | | | [01,12]. | | | +-----------+--------------------------------+-------+ | | ``%M`` | Minute as a decimal number | | | | | [00,59]. | | | +-----------+--------------------------------+-------+ | | ``%p`` | Locale&#39;s equivalent of either | \\(3) | | | | AM or PM. | | 373 | +-----------+--------------------------------+-------+ | | ``%q`` | Quarter as a decimal number | | | | | [01,04] | | | +-----------+--------------------------------+-------+ | | ``%S`` | Second as a decimal number | \\(4) | | | | [00,61]. | | | +-----------+--------------------------------+-------+ | | ``%U`` | Week number of the year | \\(5) | | | | (Sunday as the first day of | | | | | the week) as a decimal number | | | | | [00,53]. All days in a new | | | | | year preceding the first | | | | | Sunday are considered to be in | | | | | week 0. | | | +-----------+--------------------------------+-------+ | | ``%w`` | Weekday as a decimal number | | | | | [0(Sunday),6]. | | | +-----------+--------------------------------+-------+ | | ``%W`` | Week number of the year | \\(5) | | | | (Monday as the first day of | | | | | the week) as a decimal number | | | | | [00,53]. All days in a new | | | | | year preceding the first | | | | | Monday are considered to be in | | | | | week 0. | | | +-----------+--------------------------------+-------+ | | ``%x`` | Locale&#39;s appropriate date | | | | | representation. | | | +-----------+--------------------------------+-------+ | | ``%X`` | Locale&#39;s appropriate time | | | | | representation. | | | +-----------+--------------------------------+-------+ | | ``%y`` | Year without century as a | | | | | decimal number [00,99]. | | | +-----------+--------------------------------+-------+ | | ``%Y`` | Year with century as a decimal | | | | | number. | | | +-----------+--------------------------------+-------+ | | ``%Z`` | Time zone name (no characters | | | | | if no time zone exists). | | | +-----------+--------------------------------+-------+ | | ``%%`` | A literal ``&#39;%&#39;`` character. | | | +-----------+--------------------------------+-------+ | | .. note:: | | (1) | The ``%f`` directive is the same as ``%y`` if the frequency is | not quarterly. | Otherwise, it corresponds to the &#39;fiscal&#39; year, as defined by | the :attr:qyearattribute. | | (2) | The ``%F`` directive is the same as ``%Y`` if the frequency is | not quarterly. | Otherwise, it corresponds to the &#39;fiscal&#39; year, as defined by | the :attr:qyearattribute. 374 | | (3) | The ``%p`` directive only affects the output hour field | if the ``%I`` directive is used to parse the hour. | | (4) | The range really is ``0`` to ``61``; this accounts for leap | seconds and the (very rare) double leap seconds. | | (5) | The ``%U`` and ``%W`` directives are only used in calculations | when the day of the week and the year are specified. | | .. rubric:: 【示例】 | | &gt;&gt;&gt; a = Period(freq=&#39;Q@JUL&#39;, year=2006, quarter=1) | &gt;&gt;&gt; a.strftime(&#39;%F-Q%q&#39;) | &#39;2006-Q1&#39; | &gt;&gt;&gt; # Output the last month in the quarter of this date | &gt;&gt;&gt; a.strftime(&#39;%b-%Y&#39;) | &#39;Oct-2005&#39; | &gt;&gt;&gt; | &gt;&gt;&gt; a = Period(freq=&#39;D&#39;, year=2001, month=1, day=1) | &gt;&gt;&gt; a.strftime(&#39;%d-%b-%Y&#39;) | &#39;01-Jan-2006&#39; | &gt;&gt;&gt; a.strftime(&#39;%b. %d, %Y was a %A&#39;) | &#39;Jan. 01, 2001 was a Monday&#39; | | to_timestamp(...) | Return the Timestamp representation of the Period at the target | frequency at the specified end (how) of the Period | | 【参数】 | ----------| freq : string or DateOffset, default is &#39;D&#39; if self.freq is week or | longer and &#39;S&#39; otherwise | Target frequency | how: str, default &#39;S&#39; (start) | &#39;S&#39;, &#39;E&#39;. Can be aliased as case insensitive | &#39;Start&#39;, &#39;Finish&#39;, &#39;Begin&#39;, &#39;End&#39; | | 【返回值】 | -------| Timestamp | | ----------------------------------------------------------------------| Data descriptors defined here: | | day | | dayofweek | | dayofyear | | days_in_month | | daysinmonth 375 | | end_time | | freq | | freqstr | | hour | | minute | | month | | ordinal | | quarter | | qyear | | second | | start_time | | week | | weekday | | weekofyear | | year | | ----------------------------------------------------------------------| 其他数据、属性定义： | | __pyx_vtable__ = &lt;capsule object NULL&gt; PeriodIndex PeriodIndex 模块所属：pandas.tseries.period: 类 定 义 ： PeriodIndex(pandas.tseries.base.DatelikeOps, pandas.tseries.base.DatetimeIndexOpsMixin, pandas.core.index.Int64Index) | Immutable ndarray holding ordinal values indicating regular periods in | time such as particular years, quarters, months, etc. A value of 1 is the | period containing the Gregorian proleptic datetime Jan 1, 0001 00:00:00. | This ordinal representation is from the scikits.timeseries project. | | For instance, | # construct period for day 1/1/1 and get the first second | i = Period(year=1,month=1,day=1,freq=&#39;D&#39;).asfreq(&#39;S&#39;, &#39;S&#39;) 376 | i.ordinal | ===&gt; 1 | | Index keys are boxed to Period objects which carries the metadata (eg, | frequency information). | | 【参数】 | ----------| data : array-like (1-dimensional), optional | Optional period-like data to construct index with | dtype : NumPy dtype (default: i8) | copy : bool | Make a copy of input ndarray | freq : string or period object, optional | One of pandas period strings or corresponding objects | start : starting value, period-like, optional | If data is None, used as the start point in generating regular | period data. | periods : int, optional, &gt; 0 | Number of periods to generate, if generating index. Takes precedence | over end argument | end : end value, period-like, optional | If periods is none, generated index will extend to first conforming | period on or just past end argument | year : int, array, or Series, default None | month : int, array, or Series, default None | quarter : int, array, or Series, default None | day : int, array, or Series, default None | hour : int, array, or Series, default None | minute : int, array, or Series, default None | second : int, array, or Series, default None | tz : object, default None | Timezone for converting datetime64 data to Periods | | 【示例】 | --------| &gt;&gt;&gt; idx = PeriodIndex(year=year_arr, quarter=q_arr) | | &gt;&gt;&gt; idx2 = PeriodIndex(start=&#39;2000&#39;, end=&#39;2010&#39;, freq=&#39;A&#39;) | | 【方法排序】 | PeriodIndex | pandas.tseries.base.DatelikeOps | pandas.tseries.base.DatetimeIndexOpsMixin | pandas.core.index.Int64Index | pandas.core.index.NumericIndex | pandas.core.index.Index | pandas.core.base.IndexOpsMixin | pandas.core.strings.StringAccessorMixin | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __abs__(self, other=None) | 377 | __add__(self, other) | | __array_wrap__(self, result, context=None) | Gets called after a ufunc. Needs additional handling as | PeriodIndex stores internal data as int dtype | | Replace this to __numpy_ufunc__ in future version | | __contains__(self, key) | | __eq__ = wrapper(self, other) | | __floordiv__(self, other=None) | | __ge__ = wrapper(self, other) | | __getitem__(self, key) | Override numpy.ndarray&#39;s __getitem__ method to work as desired. | | This function adds lists and Series as valid boolean indexers | (ndarrays only supports ndarray with dtype=bool). | | If resulting ndim != 1, plain ndarray is returned instead of | correspondingIndexsubclass. | | __gt__ = wrapper(self, other) | | __iadd__ = __add__(self, other) | | __inv__(self, other=None) | | __isub__ = __sub__(self, other) | | __le__ = wrapper(self, other) | | __lt__ = wrapper(self, other) | | __mul__(self, other=None) | | __ne__ = wrapper(self, other) | | __neg__(self, other=None) | | __pos__(self, other=None) | | __radd__ = __add__(self, other) | | __rfloordiv__ = __floordiv__(self, other=None) | | __rmul__ = __mul__(self, other=None) | | __rsub__(self, other) | | __rtruediv__ = __truediv__(self, other=None) | | __setstate__(self, state) | Necessary for making this object picklable 378 | | __sub__(self, other) | | __truediv__(self, other=None) | | all(self, other=None) | | any(self, other=None) | | append(self, other) | Append a collection of Index options together | | 【参数】 | ----------| other : Index or list/tuple of indices | | 【返回值】 | -------| appended : Index | | asfreq(self, freq=None, how=&#39;E&#39;) | Convert the PeriodIndex to the specified frequencyfreq. | | 【参数】 | ----------| | freq : str | a frequency | how : str {&#39;E&#39;, &#39;S&#39;} | &#39;E&#39;, &#39;END&#39;, or &#39;FINISH&#39; for end, | &#39;S&#39;, &#39;START&#39;, or &#39;BEGIN&#39; for start. | Whether the elements should be aligned to the end | or start within pa period. January 31st (&#39;END&#39;) vs. | Janury 1st (&#39;START&#39;) for example. | | 【返回值】 | -------| | new : PeriodIndex with the new frequency | | 【示例】 | --------| &gt;&gt;&gt; pidx = pd.period_range(&#39;2010-01-01&#39;, &#39;2015-01-01&#39;, freq=&#39;A&#39;) | &gt;&gt;&gt; pidx | &lt;class &#39;pandas.tseries.period.PeriodIndex&#39;&gt; | [2010, ..., 2015] | Length: 6, Freq: A-DEC | | &gt;&gt;&gt; pidx.asfreq(&#39;M&#39;) | &lt;class &#39;pandas.tseries.period.PeriodIndex&#39;&gt; | [2010-12, ..., 2015-12] | Length: 6, Freq: M | | &gt;&gt;&gt; pidx.asfreq(&#39;M&#39;, how=&#39;S&#39;) | &lt;class &#39;pandas.tseries.period.PeriodIndex&#39;&gt; | [2010-01, ..., 2015-01] | Length: 6, Freq: M 379 | | asof_locs(self, where, mask) | where : array of timestamps | mask : array of booleans where data is not NA | | astype(self, dtype) | | equals(self, other) | Determines if two Index objects contain the same elements. | | get_indexer(self, target, method=None, limit=None, tolerance=None) | Compute indexer and mask for new index given the current index. The | indexer should be then used as an input to ndarray.take to align the | current data to the new index. | | 【参数】 | ----------| target : Index | method : {None, &#39;pad&#39;/&#39;ffill&#39;, &#39;backfill&#39;/&#39;bfill&#39;, &#39;nearest&#39;}, optional | * default: exact matches only. | * pad / ffill: find the PREVIOUS index value if no exact match. | * backfill / bfill: use NEXT index value if no exact match | * nearest: use the NEAREST index value if no exact match. Tied | distances are broken by preferring the larger index value. | limit : int, optional | Maximum number of consecutive labels in ``target`` to match for | inexact matches. | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【示例】 | --------| &gt;&gt;&gt; indexer = index.get_indexer(new_index) | &gt;&gt;&gt; new_values = cur_values.take(indexer) | | 【返回值】 | -------| indexer : ndarray of int | Integers from 0 to n - 1 indicating that the index at these | positions matches the corresponding target values. Missing values | in the target are marked by -1. | | get_loc(self, key, method=None, tolerance=None) | Get integer location for requested label | | 【返回值】 | -------| loc : int | | get_value(self, series, key) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | 380 | join(self, other, how=&#39;left&#39;, level=None, return_indexers=False) | See Index.join | | repeat(self, n) | Return a new Index of the values repeated n times. | | 【参见】 | --------| numpy.ndarray.repeat | | searchsorted(self, key, side=&#39;left&#39;) | np.ndarray searchsorted compat | | shift(self, n) | Specialized shift which produces an PeriodIndex | | 【参数】 | ----------| n : int | Periods to shift by | | 【返回值】 | -------| shifted : PeriodIndex | | take(self, indices, axis=0) | Analogous to ndarray.take | | to_datetime(self, dayfirst=False) | For an Index containing strings or datetime.datetime objects, attempt | conversion to DatetimeIndex | | to_timestamp(self, freq=None, how=&#39;start&#39;) | Cast to DatetimeIndex | | 【参数】 | ----------| freq : string or DateOffset, default &#39;D&#39; for week or longer, &#39;S&#39; | otherwise | Target frequency | how : {&#39;s&#39;, &#39;e&#39;, &#39;start&#39;, &#39;end&#39;} | | 【返回值】 | -------| DatetimeIndex | | tz_convert(self, tz) | Convert tz-aware DatetimeIndex from one time zone to another (using pytz/dateutil) | | 【参数】 | ----------| tz : string, pytz.timezone, dateutil.tz.tzfile or None | Time zone for time. Corresponding timestamps would be converted to | time zone of the TimeSeries. | None will remove timezone holding UTC time. | | 【返回值】 381 | -------| normalized : DatetimeIndex | | Note | ----| Not currently implemented for PeriodIndex | | tz_localize(self, tz, infer_dst=False) | Localize tz-naive DatetimeIndex to given time zone (using pytz/dateutil), | or remove timezone from tz-aware DatetimeIndex | | 【参数】 | ----------| tz : string, pytz.timezone, dateutil.tz.tzfile or None | Time zone for time. Corresponding timestamps would be converted to | time zone of the TimeSeries. | None will remove timezone holding local time. | infer_dst : boolean, default False | Attempt to infer fall dst-transition hours based on order | | 【返回值】 | -------| localized : DatetimeIndex | | Note | ----| Not currently implemented for PeriodIndex | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, data=None, ordinal=None, freq=None, start=None, end=None, periods=None, copy=False, name=None, tz=None, **kwargs) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | day | The days of the period | | day_of_year | The ordinal day of the year | | dayofweek | The day of the week with Monday=0, Sunday=6 | | dayofyear | The ordinal day of the year | | days_in_month | The number of days in the month | | daysinmonth | The number of days in the month | | dtype_str 382 | | hour | The hour of the period | | inferred_type | | is_all_dates | Checks that all the labels are datetime objects | | is_full | Returns True if there are any missing periods from start to end | | minute | The minute of the period | | month | The month as January=1, December=12 | | quarter | The quarter of the date | | qyear | | second | The second of the period | | week | The week ordinal of the year | | weekday | The day of the week with Monday=0, Sunday=6 | | weekofyear | The week ordinal of the year | | year | The year of the period | | ----------------------------------------------------------------------| 其他数据、属性定义： | | __hash__ = None | | freq = None | | ----------------------------------------------------------------------| Methods inherited from pandas.tseries.base.DatelikeOps: | | strftime(self, date_format) | Return an array of formatted strings specified by date_format, which | supports the same string format as the python standard library. Details | of the string format can be found in thepython string format doc| https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior__ | | .. versionadded:: 0.17.0 | | 【参数】 383 | ----------| date_format : str | date format string (e.g. &quot;%Y-%m-%d&quot;) | | 【返回值】 | -------| ndarray of formatted strings | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.tseries.base.DatelikeOps: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | ----------------------------------------------------------------------| Methods inherited from pandas.tseries.base.DatetimeIndexOpsMixin: | | __iter__(self) | | argmax(self, axis=None) | return a ndarray of the maximum argument indexer | | 【参见】 | --------| numpy.ndarray.argmax | | argmin(self, axis=None) | return a ndarray of the minimum argument indexer | | 【参见】 | --------| numpy.ndarray.argmin | | get_duplicates(self) | | groupby(self, f) | | isin(self, values) | Compute boolean array of whether each index value is found in the | passed set of values | | 【参数】 | ----------| values : set or sequence of values | | 【返回值】 | -------| is_contained : ndarray (boolean dtype) | | map(self, f) | # Try to run function on index first, and then on elements of index | # Especially important for group-by functionality | | max(self, axis=None) 384 | return the maximum value of the Index | | 【参见】 | --------| numpy.ndarray.max | | min(self, axis=None) | return the minimum value of the Index | | 【参见】 | --------| numpy.ndarray.min | | sort_values(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | summary(self, name=None) | return a summarized representation | | tolist(self) | return a list of the underlying data | | unique(self) | Index.unique with handling for DatetimeIndex/PeriodIndex metadata | | 【返回值】 | -------| result : DatetimeIndex or PeriodIndex | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.tseries.base.DatetimeIndexOpsMixin: | | asobject | | freqstr | return the frequency object as a string if its set, otherwise None | | hasnans | | inferred_freq | | resolution | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.index.Int64Index: | | asi8 | | ----------------------------------------------------------------------| Methods inherited from pandas.core.index.Index: | | __and__(self, other) | | __array__(self, dtype=None) | the array interface, return my values | | __bool__ = __nonzero__(self) 385 | | __copy__ = copy(self, names=None, name=None, dtype=None, deep=False) | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | __deepcopy__(self, memo={}) | | __len__(self) | return the length of the Index | | __nonzero__(self) | | __or__(self, other) | | __reduce__(self) | helper for pickle | | __setitem__(self, key, value) | | __unicode__(self) | Return a string representation for this object. | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | __xor__(self, other) | | argsort(self, *args, **kwargs) | return an ndarray indexer of the underlying data | | 【参见】 | --------| numpy.ndarray.argsort | | asof(self, label) | For a sorted index, return the most recent label up to and including | the passed label. Return NaN if not found. | | 【参见】 | --------| get_loc : asof is a thin wrapper around get_loc with method=&#39;pad&#39; | | copy(self, names=None, name=None, dtype=None, deep=False) 386 | Make a copy of this object. Name and dtype sets those attributes on | the new object. | | 【参数】 | ----------| name : string, optional | dtype : numpy dtype or pandas type | | 【返回值】 | -------| copy : Index | | 【注意】 | -----| In most cases, there should be no functional difference from using | ``deep``, but if ``deep`` is passed it will attempt to deepcopy. | | delete(self, loc) | Make new Index with passed location(-s) deleted | | 【返回值】 | -------| new_index : Index | | diff = wrapper(*args, **kwargs) | | difference(self, other) | Return a new Index with elements from the index that are not inother. | | This is the sorted set difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| difference : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.difference(idx2) | Int64Index([1, 2], dtype=&#39;int64&#39;) | | drop(self, labels, errors=&#39;raise&#39;) | Make new Index with passed list of labels deleted | | 【参数】 | ----------| labels : array-like | errors : {&#39;ignore&#39;, &#39;raise&#39;}, default &#39;raise&#39; | If &#39;ignore&#39;, suppress error and existing labels are dropped. | 387 | 【返回值】 | -------| dropped : Index | | drop_duplicates(self, keep=&#39;first&#39;) | Return Index with duplicate values removed | | 【参数】 | ----------| | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Drop duplicates except for the first occurrence. | - ``last`` : Drop duplicates except for the last occurrence. | - False : Drop all duplicates. | take_last : deprecated | | | 【返回值】 | -------| deduplicated : Index | | duplicated(self, keep=&#39;first&#39;) | Return boolean np.array denoting duplicate values | | 【参数】 | ----------| keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Mark duplicates as ``True`` except for the first occurrence. | - ``last`` : Mark duplicates as ``True`` except for the last occurrence. | - False : Mark all duplicates as ``True``. | take_last : deprecated | | 【返回值】 | -------| duplicated : np.array | | fillna(self, value=None, downcast=None) | Fill NA/NaN values with the specified value | | 【参数】 | ----------| value : scalar | Scalar value to use to fill holes (e.g. 0). | This value cannot be a list-likes. | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | | 【返回值】 | -------| filled : Index | | format(self, name=False, formatter=None, **kwargs) | Render a string representation of the Index | | get_indexer_for(self, target, **kwargs) 388 | guaranteed return of an indexer even when non-unique | | get_indexer_non_unique(self, target) | return an indexer suitable for taking from a non unique index | return the labels in the same order as the target, and | return a missing indexer into the target (missing are marked as -1 | in the indexer); target must be an iterable | | get_level_values(self, level) | Return vector of label values for requested level, equal to the length | of the index | | 【参数】 | ----------| level : int | | 【返回值】 | -------| values : ndarray | | get_slice_bound(self, label, side, kind) | Calculate slice bound that corresponds to given label. | | Returns leftmost (one-past-the-rightmost if ``side==&#39;right&#39;``) position | of given label. | | 【参数】 | ----------| label : object | side : {&#39;left&#39;, &#39;right&#39;} | kind : string / None, the type of indexer | | get_values(self) | return the underlying data as an ndarray | | holds_integer(self) | | identical(self, other) | Similar to equals, but check that other comparable attributes are | also equal | | insert(self, loc, item) | Make new Index inserting new item at location. Follows | Python list.append semantics for negative values | | 【参数】 | ----------| loc : int | item : object | | 【返回值】 | -------| new_index : Index | | intersection(self, other) | Form the intersection of two Index objects. | 389 | This returns a new Index with elements common to the index andother. | Sortedness of the result is not guaranteed. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| intersection : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.intersection(idx2) | Int64Index([3, 4], dtype=&#39;int64&#39;) | | is_(self, other) | More flexible, faster check like ``is`` but that works through views | | Note: this is *not* the same as ``Index.identical()``, which checks | that metadata is also the same. | | 【参数】 | ----------| other : object | other object to compare against. | | 【返回值】 | -------| True if both have same underlying data, False otherwise : bool | | is_boolean(self) | | is_categorical(self) | | is_floating(self) | | is_integer(self) | | is_lexsorted_for_tuple(self, tup) | | is_mixed(self) | | is_numeric(self) | | is_object(self) | | is_type_compatible(self, kind) | | order(self, return_indexer=False, ascending=True) | Return sorted copy of Index | | DEPRECATED: use :meth:Index.sort_values| 390 | putmask(self, mask, value) | return a new Index of the values set with the mask | | 【参见】 | --------| numpy.ndarray.putmask | | ravel(self, order=&#39;C&#39;) | return an ndarray of the flattened values of the underlying data | | 【参见】 | --------| numpy.ndarray.ravel | | reindex(self, target, method=None, level=None, limit=None, tolerance=None) | Create index with target&#39;s values (move/add/delete values as necessary) | | 【参数】 | ----------| target : an iterable | | 【返回值】 | -------| new_index : pd.Index | Resulting index | indexer : np.ndarray or None | Indices of output values in original index | | rename(self, name, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| name : str or list | name to set | inplace : bool | if True, mutates in place | | 【返回值】 | -------| new index (of same type and class...etc) [if inplace, returns None] | | set_names(self, names, level=None, inplace=False) | Set new names on index. Defaults to returning new index. | | 【参数】 | ----------| names : str or sequence | name(s) to set | level : int or level name, or sequence of int / level names (default None) | If the index is a MultiIndex (hierarchical), level(s) to set (None for all levels) | Otherwise level must be None | inplace : bool | if True, mutates in place | | 【返回值】 391 | -------| new index (of same type and class...etc) [if inplace, returns None] | | 【示例】 | --------| &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names(&#39;foo&#39;) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; Index([1, 2, 3, 4]).set_names([&#39;foo&#39;]) | Int64Index([1, 2, 3, 4], dtype=&#39;int64&#39;) | &gt;&gt;&gt; idx = MultiIndex.from_tuples([(1, u&#39;one&#39;), (1, u&#39;two&#39;), | (2, u&#39;one&#39;), (2, u&#39;two&#39;)], | names=[&#39;foo&#39;, &#39;bar&#39;]) | &gt;&gt;&gt; idx.set_names([&#39;baz&#39;, &#39;quz&#39;]) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;quz&#39;]) | &gt;&gt;&gt; idx.set_names(&#39;baz&#39;, level=0) | MultiIndex(levels=[[1, 2], [u&#39;one&#39;, u&#39;two&#39;]], | labels=[[0, 0, 1, 1], [0, 1, 0, 1]], | names=[u&#39;baz&#39;, u&#39;bar&#39;]) | | set_value(self, arr, key, value) | Fast lookup of value from 1-dimensional ndarray. Only use this if you | know what you&#39;re doing | | slice_indexer(self, start=None, end=None, step=None, kind=None) | For an ordered Index, compute the slice indexer for input labels and | step | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, default None | kind : string, default None | | 【返回值】 | -------| indexer : ndarray or slice | | 【注意】 | -----| This function assumes that the data is sorted, so use at your own peril | | slice_locs(self, start=None, end=None, step=None, kind=None) | Compute slice locations for input labels. | | 【参数】 | ----------| start : label, default None | If None, defaults to the beginning | end : label, default None | If None, defaults to the end | step : int, defaults None | If None, defaults to 1 392 | kind : string, defaults None | | 【返回值】 | -------| start, end : int | | sort(self, *args, **kwargs) | | sortlevel(self, level=None, ascending=True, sort_remaining=None) | For internal compatibility with with the Index API | | Sort the Index. This is for compat with MultiIndex | | 【参数】 | ----------| ascending : boolean, default True | False to sort in descending order | | level, sort_remaining are compat paramaters | | 【返回值】 | -------| sorted_index : Index | | sym_diff(self, other, result_name=None) | Compute the sorted symmetric difference of two Index objects. | | 【参数】 | ----------| other : Index or array-like | result_name : str | | 【返回值】 | -------| sym_diff : Index | | 【注意】 | -----| ``sym_diff`` contains elements that appear in either ``idx1`` or | ``idx2`` but not both. Equivalent to the Index created by | ``(idx1 - idx2) + (idx2 - idx1)`` with duplicates dropped. | | The sorting of a result containing ``NaN`` values is not guaranteed | across Python versions. See GitHub issue #6444. | | 【示例】 | --------| &gt;&gt;&gt; idx1 = Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = Index([2, 3, 4, 5]) | &gt;&gt;&gt; idx1.sym_diff(idx2) | Int64Index([1, 5], dtype=&#39;int64&#39;) | | You can also use the ``^`` operator: | | &gt;&gt;&gt; idx1 ^ idx2 | Int64Index([1, 5], dtype=&#39;int64&#39;) | 393 | to_native_types(self, slicer=None, **kwargs) | slice and dice then format | | to_series(self, **kwargs) | Create a Series with both index and values equal to the index keys | useful with map for returning an indexer based on an index | | 【返回值】 | -------| Series : dtype will be based on the type of the Index values. | | union(self, other) | Form the union of two Index objects and sorts if possible. | | 【参数】 | ----------| other : Index or array-like | | 【返回值】 | -------| union : Index | | 【示例】 | --------| | &gt;&gt;&gt; idx1 = pd.Index([1, 2, 3, 4]) | &gt;&gt;&gt; idx2 = pd.Index([3, 4, 5, 6]) | &gt;&gt;&gt; idx1.union(idx2) | Int64Index([1, 2, 3, 4, 5, 6], dtype=&#39;int64&#39;) | | view(self, cls=None) | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.index.Index: | | dtype | | has_duplicates | | is_monotonic | alias for is_monotonic_increasing (deprecated) | | is_monotonic_decreasing | return if the index is monotonic decreasing (only equal or | decreasing) values. | | is_monotonic_increasing | return if the index is monotonic increasing (only equal or | increasing) values. | | is_unique | | names | | nlevels | | values 394 | return the underlying data as an ndarray | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.index.Index: | | name = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.IndexOpsMixin: | | factorize(self, sort=False, na_sentinel=-1) | Encode the object as an enumerated type or categorical variable | | 【参数】 | ----------| sort : boolean, default False | Sort by values | na_sentinel: int, default -1 | Value to mark &quot;not found&quot; | | 【返回值】 | -------| labels : the indexer to the original array | uniques : the unique Index | | item(self) | return the first element of the underlying data as a python scalar | | memory_usage(self, deep=False) | Memory usage of my values | | 【参数】 | ----------| deep : bool | Introspect the data deeply, interrogate |objectdtypes for system-level memory consumption | | 【返回值】 | -------| bytes used | | 【注意】 | -----| Memory usage does not include memory consumed by elements that | are not components of the array if deep=False | | 【参见】 | --------| numpy.ndarray.nbytes | | nunique(self, dropna=True) | Return number of unique elements in the object. | | Excludes NA values by default. | | 【参数】 | ---------- 395 | dropna : boolean, default True | Don&#39;t include NaN in the count. | | 【返回值】 | -------| nunique : int | | transpose(self) | return the transpose, which is by definition self | | value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True) | Returns object containing counts of unique values. | | The resulting object will be in descending order so that the | first element is the most frequently-occurring element. | Excludes NA values by default. | | 【参数】 | ----------| normalize : boolean, default False | If True then the object returned will contain the relative | frequencies of the unique values. | sort : boolean, default True | Sort by values | ascending : boolean, default False | Sort in ascending order | bins : integer, optional | Rather than count values, group them into half-open bins, | a convenience for pd.cut, only works with numeric data | dropna : boolean, default True | Don&#39;t include counts of NaN. | | 【返回值】 | -------| counts : Series | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.IndexOpsMixin: | | T | return the transpose, which is by definition self | | base | return the base object if the memory of the underlying data is shared | | data | return the data pointer of the underlying data | | flags | return the ndarray.flags for the underlying data | | itemsize | return the size of the dtype of the item of the underlying data | | nbytes | return the number of bytes in the underlying data | 396 | ndim | return the number of dimensions of the underlying data, by definition 1 | | shape | return a tuple of the shape of the underlying data | | size | return the number of elements in the underlying data | | strides | return the strides of the underlying data | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.base.IndexOpsMixin: | | __array_priority__ = 1000 | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin: | | str = &lt;class &#39;pandas.core.strings.StringMethods&#39;&gt; | Vectorized string functions for Series and Index. NAs stay NA unless | handled otherwise by a particular method. Patterned after Python&#39;s string | methods, with some inspiration from R&#39;s stringr package. | | 【示例】 | --------| &gt;&gt;&gt; s.str.split(&#39;_&#39;) | &gt;&gt;&gt; s.str.replace(&#39;_&#39;, &#39;&#39;) | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. 397 Series Series 模块所属：pandas.core.series: 类定义：Series(pandas.core.base.IndexOpsMixin, pandas.core.strings.StringAccessorMixin, pandas.core.generic.NDFrame) | One-dimensional ndarray with axis labels (including time series). | | Labels need not be unique but must be any hashable type. The object | supports both integer- and label-based indexing and provides a host of | methods for performing operations involving the index. Statistical | methods from ndarray have been overridden to automatically exclude | missing data (currently represented as NaN) | | Operations between Series (+, -, /, *, **) align values based on their | associated index values-- they need not be the same length. The result | index will be the sorted union of the two indexes. | | 【参数】 | ----------| data : array-like, dict, or scalar value | Contains data stored in Series | index : array-like or Index (1d) | Values must be unique and hashable, same length as data. Index | object (or other iterable of same length as data) Will default to | np.arange(len(data)) if not provided. If both a dict and index | sequence are used, the index will override the keys found in the | dict. | dtype : numpy.dtype or None | If None, dtype will be inferred | copy : boolean, default False | Copy input data | | 【方法排序】 | Series | pandas.core.base.IndexOpsMixin | pandas.core.strings.StringAccessorMixin | pandas.core.generic.NDFrame | pandas.core.base.PandasObject | pandas.core.base.StringMixin | 【内置对象】 | | 【方法定义】 | | __add__ = wrapper(left, right, name=&#39;__add__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C630D0&gt;) | | __and__ = wrapper(self, other) | | __array__(self, result=None) | the array interface, return my values 398 | | __array_prepare__(self, result, context=None) | Gets called prior to a ufunc | | __array_wrap__(self, result, context=None) | Gets called after a ufunc | | __div__ = wrapper(left, right, name=&#39;__truediv__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63510&gt;) | | __eq__ = wrapper(self, other, axis=None) | | __float__ = wrapper(self) | | __floordiv__ = wrapper(left, right, name=&#39;__floordiv__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63620&gt;) | | __ge__ = wrapper(self, other, axis=None) | | __getitem__(self, key) | | __gt__ = wrapper(self, other, axis=None) | | __iadd__ = f(self, other) | | __imul__ = f(self, other) | | __init__(self, data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) | Initialize self. See help(type(self)) for accurate signature. | | __int__ = wrapper(self) | | __ipow__ = f(self, other) | | __isub__ = f(self, other) | | __iter__(self) | provide iteration over the values of the Series | box values if necessary | | __itruediv__ = f(self, other) | | __le__ = wrapper(self, other, axis=None) | | __len__(self) | return the length of the Series | | __long__ = wrapper(self) | | __lt__ = wrapper(self, other, axis=None) | | __mod__ = wrapper(left, right, name=&#39;__mod__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63730&gt;) | | __mul__ = wrapper(left, right, name=&#39;__mul__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63400&gt;) | 399 | __ne__ = wrapper(self, other, axis=None) | | __or__ = wrapper(self, other) | | __pow__ = wrapper(left, right, name=&#39;__pow__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63840&gt;) | | __radd__ = wrapper(left, right, name=&#39;__radd__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C631E0&gt;) | | __rand__ = wrapper(self, other) | | __rdiv__ = wrapper(left, right, name=&#39;__rtruediv__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63C80&gt;) | | __rfloordiv__ = wrapper(left, right, name=&#39;__rfloordiv__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63E18&gt;) | | __rmod__ = wrapper(left, right, name=&#39;__rmod__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C651E0&gt;) | | __rmul__ = wrapper(left, right, name=&#39;__rmul__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63950&gt;) | | __ror__ = wrapper(self, other) | | __rpow__ = wrapper(left, right, name=&#39;__rpow__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C65048&gt;) | | __rsub__ = wrapper(left, right, name=&#39;__rsub__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63AE8&gt;) | | __rtruediv__ = wrapper(left, right, name=&#39;__rtruediv__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63C80&gt;) | | __rxor__ = wrapper(self, other) | | __setitem__(self, key, value) | | __sub__ = wrapper(left, right, name=&#39;__sub__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C632F0&gt;) | | __truediv__ = wrapper(left, right, name=&#39;__truediv__&#39;, na_op=&lt;function _arith_method_SERIES.&lt;locals&gt;.na_op at 0x0000000004C63510&gt;) | | __unicode__(self) | Return a string representation for a particular DataFrame | | Invoked by unicode(df) in py2 only. Yields a Unicode String in both | py2/py3. | | __xor__ = wrapper(self, other) | | add(self, other, level=None, fill_value=None, axis=0) | Addition of series and other, element-wise (binary operatoradd). | | Equivalent to ``series + other``, but with support to substitute a fill_value for 400 | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.radd | | align(self, other, join=&#39;outer&#39;, axis=None, level=None, copy=True, fill_value=None, method=None, limit=None, fill_axis=0, broadcast_axis=None) | Align two object on their axes with the | specified join method for each axis Index | | 【参数】 | ----------| other : DataFrame or Series | join : {&#39;outer&#39;, &#39;inner&#39;, &#39;left&#39;, &#39;right&#39;}, default &#39;outer&#39; | axis : allowed axis of the other object, default None | Align on index (0), columns (1), or both (None) | level : int or level name, default None | Broadcast across a level, matching Index values on the | passed MultiIndex level | copy : boolean, default True | Always returns new objects. If copy=False and no reindexing is | required then original objects are returned. | fill_value : scalar, default np.NaN | Value to use for missing values. Defaults to NaN, but can be any | &quot;compatible&quot; value | method : str, default None | limit : int, default None | fill_axis : {0, &#39;index&#39;}, default 0 | Filling axis, method and limit | broadcast_axis : {0, &#39;index&#39;}, default None | Broadcast values along this axis, if aligning two objects of | different dimensions | | .. versionadded:: 0.17.0 | | 【返回值】 | -------| (left, right) : (Series, type of other) | Aligned objects | | all(self, axis=None, bool_only=None, skipna=None, level=None, **kwargs) | Return whether all elements are True over requested axis 401 | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | bool_only : boolean, default None | Include only boolean data. If None, will attempt to use everything, | then use only boolean data | | 【返回值】 | -------| all : scalar or Series (if level specified) | | any(self, axis=None, bool_only=None, skipna=None, level=None, **kwargs) | Return whether any element is True over requested axis | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | bool_only : boolean, default None | Include only boolean data. If None, will attempt to use everything, | then use only boolean data | | 【返回值】 | -------| any : scalar or Series (if level specified) | | append(self, to_append, verify_integrity=False) | Concatenate two or more Series. | | 【参数】 | ----------| to_append : Series or list/tuple of Series | verify_integrity : boolean, default False | If True, raise Exception on creating index with duplicates | | 【返回值】 | -------| appended : Series | | apply(self, func, convert_dtype=True, args=(), **kwds) | Invoke function on values of Series. Can be ufunc (a NumPy function | that applies to the entire Series) or a Python function that only works | on single values | | 【参数】 402 | ----------| func : function | convert_dtype : boolean, default True | Try to find better dtype for elementwise function results. If | False, leave as dtype=object | args : tuple | Positional arguments to pass to function in addition to the value | Additional keyword arguments will be passed as keywords to the function | | 【返回值】 | -------| y : Series or DataFrame if func returns a Series | | 【参见】 | --------| Series.map: For element-wise operations | | 【示例】 | --------| | Create a series with typical summer temperatures for each city. | | &gt;&gt;&gt; import pandas as pd | &gt;&gt;&gt; import numpy as np | &gt;&gt;&gt; series = pd.Series([20, 21, 12], index=[&#39;London&#39;, | ... &#39;New York&#39;,&#39;Helsinki&#39;]) | London 20 | New York 21 | Helsinki 12 | dtype: int64 | | Square the values by defining a function and passing it as an | argument to ``apply()``. | | &gt;&gt;&gt; def square(x): | ... return x**2 | &gt;&gt;&gt; series.apply(square) | London 400 | New York 441 | Helsinki 144 | dtype: int64 | | Square the values by passing an anonymous function as an | argument to ``apply()``. | | &gt;&gt;&gt; series.apply(lambda x: x**2) | London 400 | New York 441 | Helsinki 144 | dtype: int64 | | Define a custom function that needs additional positional | arguments and pass these additional arguments using the | ``args`` keyword. | | &gt;&gt;&gt; def subtract_custom_value(x, custom_value): | ... return x-custom_value 403 | | &gt;&gt;&gt; series.apply(subtract_custom_value, args=(5,)) | London 15 | New York 16 | Helsinki 7 | dtype: int64 | | Define a custom function that takes keyword arguments | and pass these arguments to ``apply``. | | &gt;&gt;&gt; def add_custom_values(x, **kwargs): | ... for month in kwargs: | ... x+=kwargs[month] | ... return x | | &gt;&gt;&gt; series.apply(add_custom_values, june=30, july=20, august=25) | London 95 | New York 96 | Helsinki 87 | dtype: int64 | | Use a function from the Numpy library. | | &gt;&gt;&gt; series.apply(np.log) | London 2.995732 | New York 3.044522 | Helsinki 2.484907 | dtype: float64 | | argmax = idxmax(self, axis=None, out=None, skipna=True) | | argmin = idxmin(self, axis=None, out=None, skipna=True) | | argsort(self, axis=0, kind=&#39;quicksort&#39;, order=None) | Overrides ndarray.argsort. Argsorts the value, omitting NA/null values, | and places the result in the same locations as the non-NA values | | 【参数】 | ----------| axis : int (can only be zero) | kind : {&#39;mergesort&#39;, &#39;quicksort&#39;, &#39;heapsort&#39;}, default &#39;quicksort&#39; | Choice of sorting algorithm. See np.sort for more | information. &#39;mergesort&#39; is the only stable algorithm | order : ignored | | 【返回值】 | -------| argsorted : Series, with -1 indicated where nan values are present | | 【参见】 | --------| numpy.ndarray.argsort | | asof(self, where) | Return last good (non-NaN) value in Series if value is NaN for | requested date. | 404 | If there is no good value, NaN is returned. | | 【参数】 | ----------| where : date or array of dates | | 【注意】 | -----| Dates are assumed to be sorted | | 【返回值】 | -------| value or NaN | | autocorr(self, lag=1) | Lag-N autocorrelation | | 【参数】 | ----------| lag : int, default 1 | Number of lags to apply before performing autocorrelation. | | 【返回值】 | -------| autocorr : float | | between(self, left, right, inclusive=True) | Return boolean Series equivalent to left &lt;= series &lt;= right. NA values | will be treated as False | | 【参数】 | ----------| left : scalar | Left boundary | right : scalar | Right boundary | | 【返回值】 | -------| is_between : Series | | combine(self, other, func, fill_value=nan) | Perform elementwise binary operation on two Series using given function | with optional fill value when an index is missing from one Series or | the other | | 【参数】 | ----------| other : Series or scalar value | func : function | fill_value : scalar value | | 【返回值】 | -------| result : Series | 405 | combine_first(self, other) | Combine Series values, choosing the calling Series&#39;s values | first. Result index will be the union of the two indexes | | 【参数】 | ----------| other : Series | | 【返回值】 | -------| y : Series | | compound(self, axis=None, skipna=None, level=None) | Return the compound percentage of the values for the requested axis | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| compounded : scalar or Series (if level specified) | | compress(self, condition, axis=0, out=None, **kwargs) | Return selected slices of an array along given axis as a Series | | 【参见】 | --------| numpy.ndarray.compress | | corr(self, other, method=&#39;pearson&#39;, min_periods=None) | Compute correlation withotherSeries, excluding missing values | | 【参数】 | ----------| other : Series | method : {&#39;pearson&#39;, &#39;kendall&#39;, &#39;spearman&#39;} | * pearson : standard correlation coefficient | * kendall : Kendall Tau correlation coefficient | * spearman : Spearman rank correlation | min_periods : int, optional | Minimum number of observations needed to have a valid result | | | 【返回值】 | -------| correlation : float | 406 | count(self, level=None) | Return number of non-NA/null observations in the Series | | 【参数】 | ----------| level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a smaller Series | | 【返回值】 | -------| nobs : int or Series (if level specified) | | cov(self, other, min_periods=None) | Compute covariance with Series, excluding missing values | | 【参数】 | ----------| other : Series | min_periods : int, optional | Minimum number of observations needed to have a valid result | | 【返回值】 | -------| covariance : float | | Normalized by N-1 (unbiased estimator). | | cummax = max(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative max over requested axis. | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| max : scalar | | cummin = min(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative min over requested axis. | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| min : scalar | 407 | cumprod = prod(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative prod over requested axis. | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| prod : scalar | | cumsum = sum(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) | Return cumulative sum over requested axis. | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | | 【返回值】 | -------| sum : scalar | | diff(self, periods=1) | 1st discrete difference of object | | 【参数】 | ----------| periods : int, default 1 | Periods to shift for forming difference | | 【返回值】 | -------| diffed : Series | | div = truediv(self, other, level=None, fill_value=None, axis=0) | | divide = truediv(self, other, level=None, fill_value=None, axis=0) | | dot(self, other) | Matrix multiplication with DataFrame or inner-product with Series | objects | | 【参数】 | ----------| other : Series or DataFrame | | 【返回值】 | -------| dot_product : scalar or Series | 408 | drop_duplicates(self, keep=&#39;first&#39;, inplace=False) | Return Series with duplicate values removed | | 【参数】 | ----------| | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Drop duplicates except for the first occurrence. | - ``last`` : Drop duplicates except for the last occurrence. | - False : Drop all duplicates. | take_last : deprecated | inplace : boolean, default False | If True, performs operation inplace and returns None. | | 【返回值】 | -------| deduplicated : Series | | dropna(self, axis=0, inplace=False, **kwargs) | Return Series without null values | | 【返回值】 | -------| valid : Series | inplace : boolean, default False | Do operation in place. | | duplicated(self, keep=&#39;first&#39;) | Return boolean Series denoting duplicate values | | 【参数】 | ----------| keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | - ``first`` : Mark duplicates as ``True`` except for the first occurrence. | - ``last`` : Mark duplicates as ``True`` except for the last occurrence. | - False : Mark all duplicates as ``True``. | take_last : deprecated | | 【返回值】 | -------| duplicated : Series | | eq = wrapper(self, other, axis=None) | | fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs) | Fill NA/NaN values using the specified method | | 【参数】 | ----------| value : scalar, dict, Series, or DataFrame | Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of | values specifying which value to use for each index (for a Series) or | column (for a DataFrame). (values not in the dict/Series/DataFrame will not be | filled). This value cannot be a list. | method : {&#39;backfill&#39;, &#39;bfill&#39;, &#39;pad&#39;, &#39;ffill&#39;, None}, default None | Method to use for filling holes in reindexed Series | pad / ffill: propagate last valid observation forward to next valid 409 | backfill / bfill: use NEXT valid observation to fill gap | axis : {0, &#39;index&#39;} | inplace : boolean, default False | If True, fill in place. Note: this will modify any | other views on this object, (e.g. a no-copy slice for a column in a | DataFrame). | limit : int, default None | If method is specified, this is the maximum number of consecutive | NaN values to forward/backward fill. In other words, if there is | a gap with more than this number of consecutive NaNs, it will only | be partially filled. If method is not specified, this is the | maximum number of entries along the entire axis where NaNs will be | filled. | downcast : dict, default is None | a dict of item-&gt;dtype of what to downcast if possible, | or the string &#39;infer&#39; which will try to downcast to an appropriate | equal type (e.g. float64 to int64 if possible) | | 【参见】 | --------| reindex, asfreq | | 【返回值】 | -------| filled : Series | | first_valid_index(self) | Return label for first non-NA/null value | | floordiv(self, other, level=None, fill_value=None, axis=0) | Integer division of series and other, element-wise (binary operatorfloordiv). | | Equivalent to ``series // other``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.rfloordiv | | ge = wrapper(self, other, axis=None) | | get_value(self, label, takeable=False) | Quickly retrieve single value at passed index label 410 | | 【参数】 | ----------| index : label | takeable : interpret the index as indexers, default False | | 【返回值】 | -------| value : scalar value | | get_values(self) | same as values (but handles sparseness conversions); is a view | | gt = wrapper(self, other, axis=None) | | hist = hist_series(self, by=None, ax=None, grid=True, xlabelsize=None, xrot=None, ylabelsize=None, yrot=None, figsize=None, bins=10, **kwds) | Draw histogram of the input series using matplotlib | | 【参数】 | ----------| by : object, optional | If passed, then used to form histograms for separate groups | ax : matplotlib axis object | If not passed, uses gca() | grid : boolean, default True | Whether to show axis grid lines | xlabelsize : int, default None | If specified changes the x-axis label size | xrot : float, default None | rotation of x axis labels | ylabelsize : int, default None | If specified changes the y-axis label size | yrot : float, default None | rotation of y axis labels | figsize : tuple, default None | figure size in inches by default | bins: integer, default 10 | Number of histogram bins to be used | kwds : keywords | To be passed to the actual plotting function | | 【注意】 | -----| See matplotlib documentation online for more on this | | idxmax(self, axis=None, out=None, skipna=True) | Index of first occurrence of maximum of values. | | 【参数】 | ----------| skipna : boolean, default True | Exclude NA/null values | | 【返回值】 | -------| idxmax : Index of maximum of values 411 | | 【注意】 | -----| This method is the Series version of ``ndarray.argmax``. | | 【参见】 | --------| DataFrame.idxmax | numpy.ndarray.argmax | | idxmin(self, axis=None, out=None, skipna=True) | Index of first occurrence of minimum of values. | | 【参数】 | ----------| skipna : boolean, default True | Exclude NA/null values | | 【返回值】 | -------| idxmin : Index of minimum of values | | 【注意】 | -----| This method is the Series version of ``ndarray.argmin``. | | 【参见】 | --------| DataFrame.idxmin | numpy.ndarray.argmin | | iget(self, i, axis=0) | DEPRECATED. Use ``.iloc[i]`` or ``.iat[i]`` instead | | iget_value(self, i, axis=0) | DEPRECATED. Use ``.iloc[i]`` or ``.iat[i]`` instead | | irow(self, i, axis=0) | DEPRECATED. Use ``.iloc[i]`` or ``.iat[i]`` instead | | isin(self, values) | Return a boolean :class:~pandas.Seriesshowing whether each element | in the :class:~pandas.Seriesis exactly contained in the passed | sequence of ``values``. | | 【参数】 | ----------| values : list-like | The sequence of values to test. Passing in a single string will | raise a ``TypeError``. Instead, turn a single string into a | ``list`` of one element. | | 【返回值】 | -------| isin : Series (bool dtype) | 412 | 【Raises 引发错误】 | ------| TypeError | * If ``values`` is a string | | 【参见】 | --------| pandas.DataFrame.isin | | 【示例】 | --------| | &gt;&gt;&gt; s = pd.Series(list(&#39;abc&#39;)) | &gt;&gt;&gt; s.isin([&#39;a&#39;, &#39;c&#39;, &#39;e&#39;]) | 0 True | 1 False | 2 True | dtype: bool | | Passing a single string as ``s.isin(&#39;a&#39;)`` will raise an error. Use | a list of one element instead: | | &gt;&gt;&gt; s.isin([&#39;a&#39;]) | 0 True | 1 False | 2 False | dtype: bool | | items = iteritems(self) | | iteritems(self) | Lazily iterate over (index, value) tuples | | keys(self) | Alias for index | | kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return unbiased kurtosis over requested axis using Fishers definition of | kurtosis (kurtosis of normal == 0.0). Normalized by N-1 | | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| kurt : scalar or Series (if level specified) 413 | | kurtosis = kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | | last_valid_index(self) | Return label for last non-NA/null value | | le = wrapper(self, other, axis=None) | | lt = wrapper(self, other, axis=None) | | mad(self, axis=None, skipna=None, level=None) | Return the mean absolute deviation of the values for the requested axis | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| mad : scalar or Series (if level specified) | | map(self, arg, na_action=None) | Map values of Series using input correspondence (which can be | a dict, Series, or function) | | 【参数】 | ----------| arg : function, dict, or Series | na_action : {None, &#39;ignore&#39;} | If &#39;ignore&#39;, propagate NA values | | 【示例】 | --------| &gt;&gt;&gt; x | one 1 | two 2 | three 3 | | &gt;&gt;&gt; y | 1 foo | 2 bar | 3 baz | | &gt;&gt;&gt; x.map(y) | one foo | two bar | three baz | 414 | 【返回值】 | -------| y : Series | same index as caller | | max(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | This method returns the maximum of the values in the object. If you | want the *index* of the maximum, use ``idxmax``. This is the | equivalent of the ``numpy.ndarray`` method ``argmax``. | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| max : scalar or Series (if level specified) | | mean(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the mean of the values for the requested axis | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| mean : scalar or Series (if level specified) | | median(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the median of the values for the requested axis | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None 415 | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| median : scalar or Series (if level specified) | | memory_usage(self, index=False, deep=False) | Memory usage of the Series | | 【参数】 | ----------| index : bool | Specifies whether to include memory usage of Series index | deep : bool | Introspect the data deeply, interrogate |objectdtypes for system-level memory consumption | | 【返回值】 | -------| scalar bytes of memory consumed | | 【注意】 | -----| Memory usage does not include memory consumed by elements that | are not components of the array if deep=False | | 【参见】 | --------| numpy.ndarray.nbytes | | min(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | This method returns the minimum of the values in the object. If you | want the *index* of the minimum, use ``idxmin``. This is the | equivalent of the ``numpy.ndarray`` method ``argmin``. | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| min : scalar or Series (if level specified) | | mod(self, other, level=None, fill_value=None, axis=0) 416 | Modulo of series and other, element-wise (binary operatormod). | | Equivalent to ``series % other``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.rmod | | mode(self) | Returns the mode(s) of the dataset. | | Empty if nothing occurs at least 2 times. Always returns Series even | if only one value. | | 【参数】 | ----------| sort : bool, default True | If True, will lexicographically sort values, if False skips | sorting. Result ordering when ``sort=False`` is not defined. | | 【返回值】 | -------| modes : Series (sorted) | | mul(self, other, level=None, fill_value=None, axis=0) | Multiplication of series and other, element-wise (binary operatormul). | | Equivalent to ``series * other``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | ------- 417 | result : Series | | 【参见】 | --------| Series.rmul | | multiply = mul(self, other, level=None, fill_value=None, axis=0) | | ne = wrapper(self, other, axis=None) | | nlargest(self, n=5, keep=&#39;first&#39;) | Return the largestnelements. | | 【参数】 | ----------| n : int | Return this many descending sorted values | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | Where there are duplicate values: | - ``first`` : take the first occurrence. | - ``last`` : take the last occurrence. | take_last : deprecated | | 【返回值】 | -------| top_n : Series | The n largest values in the Series, in sorted order | | 【注意】 | -----| Faster than ``.sort_values(ascending=False).head(n)`` for smallnrelative | to the size of the ``Series`` object. | | 【参见】 | --------| Series.nsmallest | | 【示例】 | --------| &gt;&gt;&gt; import pandas as pd | &gt;&gt;&gt; import numpy as np | &gt;&gt;&gt; s = pd.Series(np.random.randn(1e6)) | &gt;&gt;&gt; s.nlargest(10) # only sorts up to the N requested | | nonzero(self) | Return the indices of the elements that are non-zero | | This method is equivalent to callingnumpy.nonzeroon the | series data. For compatability with NumPy, the return value is | the same (a tuple with an array of indices for each dimension), | but it will always be a one-item tuple because series only have | one dimension. | | 【示例】 | --------| &gt;&gt;&gt; s = pd.Series([0, 3, 0, 4]) | &gt;&gt;&gt; s.nonzero() 418 | (array([1, 3]),) | &gt;&gt;&gt; s.iloc[s.nonzero()[0]] | 1 3 | 3 4 | dtype: int64 | | 【参见】 | --------| numpy.nonzero | | nsmallest(self, n=5, keep=&#39;first&#39;) | Return the smallestnelements. | | 【参数】 | ----------| n : int | Return this many ascending sorted values | keep : {&#39;first&#39;, &#39;last&#39;, False}, default &#39;first&#39; | Where there are duplicate values: | - ``first`` : take the first occurrence. | - ``last`` : take the last occurrence. | take_last : deprecated | | 【返回值】 | -------| bottom_n : Series | The n smallest values in the Series, in sorted order | | 【注意】 | -----| Faster than ``.sort_values().head(n)`` for smallnrelative to | the size of the ``Series`` object. | | 【参见】 | --------| Series.nlargest | | 【示例】 | --------| &gt;&gt;&gt; import pandas as pd | &gt;&gt;&gt; import numpy as np | &gt;&gt;&gt; s = pd.Series(np.random.randn(1e6)) | &gt;&gt;&gt; s.nsmallest(10) # only sorts up to the N requested | | order(self, na_last=None, ascending=True, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;, inplace=False) | DEPRECATED: use :meth:Series.sort_values| | Sorts Series object, by value, maintaining index-value link. | This will return a new Series by default. Series.sort is the equivalent but as an inplace method. | | 【参数】 | ----------| na_last : boolean (optional, default=True) (DEPRECATED; use na_position) | Put NaN&#39;s at beginning or end | ascending : boolean, default True | Sort ascending. Passing False sorts descending | kind : {&#39;mergesort&#39;, &#39;quicksort&#39;, &#39;heapsort&#39;}, default &#39;quicksort&#39; 419 | Choice of sorting algorithm. See np.sort for more | information. &#39;mergesort&#39; is the only stable algorithm | na_position : {&#39;first&#39;, &#39;last&#39;} (optional, default=&#39;last&#39;) | &#39;first&#39; puts NaNs at the beginning | &#39;last&#39; puts NaNs at the end | inplace : boolean, default False | Do operation in place. | | 【返回值】 | -------| y : Series | | 【参见】 | --------| Series.sort_values | | pow(self, other, level=None, fill_value=None, axis=0) | Exponential power of series and other, element-wise (binary operatorpow). | | Equivalent to ``series ** other``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.rpow | | prod(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the product of the values for the requested axis | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 420 | -------| prod : scalar or Series (if level specified) | | product = prod(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | | ptp(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Returns the difference between the maximum value and the minimum | value in the object. This is the equivalent of the ``numpy.ndarray`` | method ``ptp``. | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| ptp : scalar or Series (if level specified) | | put(self, *args, **kwargs) | return a ndarray with the values put | | 【参见】 | --------| numpy.ndarray.put | | quantile(self, q=0.5) | Return value at the given quantile, a la numpy.percentile. | | 【参数】 | ----------| q : float or array-like, default 0.5 (50% quantile) | 0 &lt;= q &lt;= 1, the quantile(s) to compute | | 【返回值】 | -------| quantile : float or Series | if ``q`` is an array, a Series will be returned where the | index is ``q`` and the values are the quantiles. | | 【示例】 | --------| | &gt;&gt;&gt; s = Series([1, 2, 3, 4]) | &gt;&gt;&gt; s.quantile(.5) | 2.5 | &gt;&gt;&gt; s.quantile([.25, .5, .75]) | 0.25 1.75 | 0.50 2.50 421 | 0.75 3.25 | dtype: float64 | | radd(self, other, level=None, fill_value=None, axis=0) | Addition of series and other, element-wise (binary operatorradd). | | Equivalent to ``other + series``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.add | | rank(self, method=&#39;average&#39;, na_option=&#39;keep&#39;, ascending=True, pct=False) | Compute data ranks (1 through n). Equal values are assigned a rank that | is the average of the ranks of those values | | 【参数】 | ----------| method : {&#39;average&#39;, &#39;min&#39;, &#39;max&#39;, &#39;first&#39;, &#39;dense&#39;} | * average: average rank of group | * min: lowest rank in group | * max: highest rank in group | * first: ranks assigned in order they appear in the array | * dense: like &#39;min&#39;, but rank always increases by 1 between groups | na_option : {&#39;keep&#39;} | keep: leave NA values where they are | ascending : boolean, default True | False for ranks by high (1) to low (N) | pct : boolean, default False | Computes percentage rank of data | | 【返回值】 | -------| ranks : Series | | ravel(self, order=&#39;C&#39;) | Return the flattened underlying data as an ndarray | | 【参见】 | --------| numpy.ndarray.ravel | 422 | rdiv = rtruediv(self, other, level=None, fill_value=None, axis=0) | | reindex(self, index=None, **kwargs) | Conform Series to new index with optional filling logic, placing | NA/NaN in locations having no value in the previous index. A new object | is produced unless the new index is equivalent to the current one and | copy=False | | 【参数】 | ----------| index : array-like, optional (can be specified in order, or as | keywords) | New labels / index to conform to. Preferably an Index object to | avoid duplicating data | method : {None, &#39;backfill&#39;/&#39;bfill&#39;, &#39;pad&#39;/&#39;ffill&#39;, &#39;nearest&#39;}, optional | method to use for filling holes in reindexed DataFrame. | Please note: this is only applicable to DataFrames/Series with a | monotonically increasing/decreasing index. | * default: don&#39;t fill gaps | * pad / ffill: propagate last valid observation forward to next valid | * backfill / bfill: use next valid observation to fill gap | * nearest: use nearest valid observations to fill gap | copy : boolean, default True | Return a new object, even if the passed indexes are the same | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | fill_value : scalar, default np.NaN | Value to use for missing values. Defaults to NaN, but can be any | &quot;compatible&quot; value | limit : int, default None | Maximum number of consecutive elements to forward or backward fill | tolerance : optional | Maximum distance between original and new labels for inexact | matches. The values of the index at the matching locations most | satisfy the equation ``abs(index[indexer] - target) &lt;= tolerance``. | | .. versionadded:: 0.17.0 | | 【示例】 | --------| | Create a dataframe with some fictional data. | | &gt;&gt;&gt; index = [&#39;Firefox&#39;, &#39;Chrome&#39;, &#39;Safari&#39;, &#39;IE10&#39;, &#39;Konqueror&#39;] | &gt;&gt;&gt; df = pd.DataFrame({ | ... &#39;http_status&#39;: [200,200,404,404,301], | ... &#39;response_time&#39;: [0.04, 0.02, 0.07, 0.08, 1.0]}, | ... index=index) | &gt;&gt;&gt; df | http_status response_time | Firefox 200 0.04 | Chrome 200 0.02 | Safari 404 0.07 | IE10 404 0.08 | Konqueror 301 1.00 | 423 | Create a new index and reindex the dataframe. By default | values in the new index that do not have corresponding | records in the dataframe are assigned ``NaN``. | | &gt;&gt;&gt; new_index= [&#39;Safari&#39;, &#39;Iceweasel&#39;, &#39;Comodo Dragon&#39;, &#39;IE10&#39;, | ... &#39;Chrome&#39;] | &gt;&gt;&gt; df.reindex(new_index) | http_status response_time | Safari 404 0.07 | Iceweasel NaN NaN | Comodo Dragon NaN NaN | IE10 404 0.08 | Chrome 200 0.02 | | We can fill in the missing values by passing a value to | the keyword ``fill_value``. Because the index is not monotonically | increasing or decreasing, we cannot use arguments to the keyword | ``method`` to fill the ``NaN`` values. | | &gt;&gt;&gt; df.reindex(new_index, fill_value=0) | http_status response_time | Safari 404 0.07 | Iceweasel 0 0.00 | Comodo Dragon 0 0.00 | IE10 404 0.08 | Chrome 200 0.02 | | &gt;&gt;&gt; df.reindex(new_index, fill_value=&#39;missing&#39;) | http_status response_time | Safari 404 0.07 | Iceweasel missing missing | Comodo Dragon missing missing | IE10 404 0.08 | Chrome 200 0.02 | | To further illustrate the filling functionality in | ``reindex``, we will create a dataframe with a | monotonically increasing index (for example, a sequence | of dates). | | &gt;&gt;&gt; date_index = pd.date_range(&#39;1/1/2010&#39;, periods=6, freq=&#39;D&#39;) | &gt;&gt;&gt; df2 = pd.DataFrame({&quot;prices&quot;: [100, 101, np.nan, 100, 89, 88]}, | index=date_index) | &gt;&gt;&gt; df2 | prices | 2010-01-01 100 | 2010-01-02 101 | 2010-01-03 NaN | 2010-01-04 100 | 2010-01-05 89 | 2010-01-06 88 | | Suppose we decide to expand the dataframe to cover a wider | date range. | | &gt;&gt;&gt; date_index2 = pd.date_range(&#39;12/29/2009&#39;, periods=10, freq=&#39;D&#39;) | &gt;&gt;&gt; df2.reindex(date_index2) 424 | prices | 2009-12-29 NaN | 2009-12-30 NaN | 2009-12-31 NaN | 2010-01-01 100 | 2010-01-02 101 | 2010-01-03 NaN | 2010-01-04 100 | 2010-01-05 89 | 2010-01-06 88 | 2010-01-07 NaN | | The index entries that did not have a value in the original data frame | (for example, &#39;2009-12-29&#39;) are by default filled with ``NaN``. | If desired, we can fill in the missing values using one of several | options. | | For example, to backpropagate the last valid value to fill the ``NaN`` | values, pass ``bfill`` as an argument to the ``method`` keyword. | | &gt;&gt;&gt; df2.reindex(date_index2, method=&#39;bfill&#39;) | prices | 2009-12-29 100 | 2009-12-30 100 | 2009-12-31 100 | 2010-01-01 100 | 2010-01-02 101 | 2010-01-03 NaN | 2010-01-04 100 | 2010-01-05 89 | 2010-01-06 88 | 2010-01-07 NaN | | Please note that the ``NaN`` value present in the original dataframe | (at index value 2010-01-03) will not be filled by any of the | value propagation schemes. This is because filling while reindexing | does not look at dataframe values, but only compares the original and | desired indexes. If you do want to fill in the ``NaN`` values present | in the original dataframe, use the ``fillna()`` method. | | 【返回值】 | -------| reindexed : Series | | reindex_axis(self, labels, axis=0, **kwargs) | for compatibility with higher dims | | rename(self, index=None, **kwargs) | Alter axes input function or functions. Function / dict values must be | unique (1-to-1). Labels not contained in a dict / Series will be left | as-is. | | 【参数】 | ----------| index : dict-like or function, optional | Transformation to apply to that axis values | 425 | copy : boolean, default True | Also copy underlying data | inplace : boolean, default False | Whether to return a new Series. If True then value of copy is | ignored. | | 【返回值】 | -------| renamed : Series (new object) | | reorder_levels(self, order) | Rearrange index levels using input order. May not drop or duplicate | levels | | 【参数】 | ----------| order: list of int representing new level order. | (reference level by number or key) | axis: where to reorder levels | | 【返回值】 | -------| type of caller (new object) | | repeat(self, reps) | return a new Series with the values repeated reps times | | 【参见】 | --------| numpy.ndarray.repeat | | reset_index(self, level=None, drop=False, name=None, inplace=False) | Analogous to the :meth:pandas.DataFrame.reset_indexfunction, see | docstring there. | | 【参数】 | ----------| level : int, str, tuple, or list, default None | Only remove the given levels from the index. Removes all levels by | default | drop : boolean, default False | Do not try to insert index into dataframe columns | name : object, default None | The name of the column corresponding to the Series values | inplace : boolean, default False | Modify the Series in place (do not create a new object) | | 【返回值】 | ----------| resetted : DataFrame, or Series if drop == True | | reshape(self, *args, **kwargs) | return an ndarray with the values shape | if the specified shape matches exactly the current shape, then | return self (for compat) | | 【参见】 426 | --------| numpy.ndarray.take | | rfloordiv(self, other, level=None, fill_value=None, axis=0) | Integer division of series and other, element-wise (binary operatorrfloordiv). | | Equivalent to ``other // series``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.floordiv | | rmod(self, other, level=None, fill_value=None, axis=0) | Modulo of series and other, element-wise (binary operatorrmod). | | Equivalent to ``other % series``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.mod | | rmul(self, other, level=None, fill_value=None, axis=0) | Multiplication of series and other, element-wise (binary operatorrmul). | | Equivalent to ``other * series``, but with support to substitute a fill_value for | missing data in one of the inputs. | 427 | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.mul | | round(self, decimals=0, out=None) | a.round(decimals=0, out=None) | | Returnawith each element rounded to the given number of decimals. | | Refer tonumpy.aroundfor full documentation. | | 【参见】 | --------| numpy.around : equivalent function | | rpow(self, other, level=None, fill_value=None, axis=0) | Exponential power of series and other, element-wise (binary operatorrpow). | | Equivalent to ``other ** series``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.pow | | rsub(self, other, level=None, fill_value=None, axis=0) | Subtraction of series and other, element-wise (binary operatorrsub). | | Equivalent to ``other - series``, but with support to substitute a fill_value for 428 | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.sub | | rtruediv(self, other, level=None, fill_value=None, axis=0) | Floating division of series and other, element-wise (binary operatorrtruediv). | | Equivalent to ``other / series``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.truediv | | searchsorted(self, v, side=&#39;left&#39;, sorter=None) | Find indices where elements should be inserted to maintain order. | | Find the indices into a sorted Seriesselfsuch that, if the | corresponding elements invwere inserted before the indices, the | order ofselfwould be preserved. | | 【参数】 | ----------| v : array_like | Values to insert intoa. | side : {&#39;left&#39;, &#39;right&#39;}, optional | If &#39;left&#39;, the index of the first suitable location found is given. 429 | If &#39;right&#39;, return the last such index. If there is no suitable | index, return either 0 or N (where N is the length ofa). | sorter : 1-D array_like, optional | Optional array of integer indices that sortselfinto ascending | order. They are typically the result of ``np.argsort``. | | 【返回值】 | -------| indices : array of ints | Array of insertion points with the same shape asv. | | 【参见】 | --------| Series.sort_values | numpy.searchsorted | | 【注意】 | -----| Binary search is used to find the required insertion points. | | 【示例】 | --------| &gt;&gt;&gt; x = pd.Series([1, 2, 3]) | &gt;&gt;&gt; x | 0 1 | 1 2 | 2 3 | dtype: int64 | &gt;&gt;&gt; x.searchsorted(4) | array([3]) | &gt;&gt;&gt; x.searchsorted([0, 4]) | array([0, 3]) | &gt;&gt;&gt; x.searchsorted([1, 3], side=&#39;left&#39;) | array([0, 2]) | &gt;&gt;&gt; x.searchsorted([1, 3], side=&#39;right&#39;) | array([1, 3]) | &gt;&gt;&gt; x.searchsorted([1, 2], side=&#39;right&#39;, sorter=[0, 2, 1]) | array([1, 3]) | | sem(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs) | Return unbiased standard error of the mean over requested axis. | | Normalized by N-1 by default. This can be changed using the ddof argument | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | ddof : int, default 1 | degrees of freedom | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use 430 | everything, then use only numeric data | | 【返回值】 | -------| sem : scalar or Series (if level specified) | | set_value(self, label, value, takeable=False) | Quickly set single value at passed label. If label is not contained, a | new object is created with the label placed at the end of the result | index | | 【参数】 | ----------| label : object | Partial indexing with MultiIndex not allowed | value : object | Scalar value | takeable : interpret the index as indexers, default False | | 【返回值】 | -------| series : Series | If label is contained, will be reference to calling Series, | otherwise a new object | | shift(self, periods=1, freq=None, axis=0) | Shift index by desired number of periods with an optional time freq | | 【参数】 | ----------| periods : int | Number of periods to move, can be positive or negative | freq : DateOffset, timedelta, or time rule string, optional | Increment to use from datetools module or time rule (e.g. &#39;EOM&#39;). | See Notes. | axis : {0, &#39;index&#39;} | | 【注意】 | -----| If freq is specified then the index values are shifted but the data | is not realigned. That is, use freq if you would like to extend the | index when shifting and preserve the original data. | | 【返回值】 | -------| shifted : Series | | skew(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return unbiased skew over requested axis | Normalized by N-1 | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA 431 | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| skew : scalar or Series (if level specified) | | sort(self, axis=0, ascending=True, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;, inplace=True) | DEPRECATED: use :meth:Series.sort_values(inplace=True)for INPLACE sorting | | Sort values and index labels by value. This is an inplace sort by default. | Series.order is the equivalent but returns a new Series. | | 【参数】 | ----------| axis : int (can only be zero) | ascending : boolean, default True | Sort ascending. Passing False sorts descending | kind : {&#39;mergesort&#39;, &#39;quicksort&#39;, &#39;heapsort&#39;}, default &#39;quicksort&#39; | Choice of sorting algorithm. See np.sort for more | information. &#39;mergesort&#39; is the only stable algorithm | na_position : {&#39;first&#39;, &#39;last&#39;} (optional, default=&#39;last&#39;) | &#39;first&#39; puts NaNs at the beginning | &#39;last&#39; puts NaNs at the end | inplace : boolean, default True | Do operation in place. | | 【参见】 | --------| Series.sort_values | | sort_index(self, axis=0, level=None, ascending=True, inplace=False, sort_remaining=True) | Sort object by labels (along an axis) | | 【参数】 | ----------| axis : index to direct sorting | level : int or level name or list of ints or list of level names | if not None, sort on values in specified index level(s) | ascending : boolean, default True | Sort ascending vs. descending | inplace : bool | if True, perform operation in-place | kind : {quicksort,mergesort,heapsort} | Choice of sorting algorithm. 参见：ndarray.np.sort for more information. |mergesortis the only stable algorithm. For DataFrames, this option is | only applied when sorting on a single column or label. | na_position : {&#39;first&#39;, &#39;last&#39;} |firstputs NaNs at the beginning,lastputs NaNs at the end | sort_remaining : bool | if true and sorting by level and index is multilevel, sort by other levels | too (in order) after sorting by specified level | 432 | 【返回值】 | -------| sorted_obj : Series | | sort_values(self, axis=0, ascending=True, inplace=False, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;) | Sort by the values along either axis | | .. versionadded:: 0.17.0 | | 【参数】 | ----------| by : string name or list of names which refer to the axis items | axis : index to direct sorting | ascending : bool or list of bool | Sort ascending vs. descending. Specify list for multiple sort orders. | If this is a list of bools, must match the length of the by | inplace : bool | if True, perform operation in-place | kind : {quicksort,mergesort,heapsort} | Choice of sorting algorithm. 参见：ndarray.np.sort for more information. |mergesortis the only stable algorithm. For DataFrames, this option is | only applied when sorting on a single column or label. | na_position : {&#39;first&#39;, &#39;last&#39;} |firstputs NaNs at the beginning,lastputs NaNs at the end | | 【返回值】 | -------| sorted_obj : Series | | sortlevel(self, level=0, ascending=True, sort_remaining=True) | Sort Series with MultiIndex by chosen level. Data will be | lexicographically sorted by the chosen level followed by the other | levels (in order) | | 【参数】 | ----------| level : int or level name, default None | ascending : bool, default True | | 【返回值】 | -------| sorted : Series | | 【参见】 | --------| Series.sort_index(level=...) | | std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs) | Return unbiased standard deviation over requested axis. | | Normalized by N-1 by default. This can be changed using the ddof argument | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True 433 | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | ddof : int, default 1 | degrees of freedom | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| std : scalar or Series (if level specified) | | sub(self, other, level=None, fill_value=None, axis=0) | Subtraction of series and other, element-wise (binary operatorsub). | | Equivalent to ``series - other``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.rsub | | subtract = sub(self, other, level=None, fill_value=None, axis=0) | | sum(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs) | Return the sum of the values for the requested axis | | 【参数】 | ----------| axis : {index (0)} | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | 434 | 【返回值】 | -------| sum : scalar or Series (if level specified) | | swaplevel(self, i, j, copy=True) | Swap levels i and j in a MultiIndex | | 【参数】 | ----------| i, j : int, string (can be mixed) | Level of index to be swapped. Can pass level name as string. | | 【返回值】 | -------| swapped : Series | | take(self, indices, axis=0, convert=True, is_copy=False) | return Series corresponding to requested indices | | 【参数】 | ----------| indices : list / array of ints | convert : translate negative to positive indices (default) | | 【返回值】 | -------| taken : Series | | 【参见】 | --------| numpy.ndarray.take | | to_csv(self, path, index=True, sep=&#39;,&#39;, na_rep=&#39;&#39;, float_format=None, header=False, index_label=None, mode=&#39;w&#39;, nanRep=None, encoding=None, date_format=None, decimal=&#39;.&#39;) | Write Series to a comma-separated values (csv) file | | 【参数】 | ----------| path : string file path or file handle / StringIO. If None is provided | the result is returned as a string. | na_rep : string, default &#39;&#39; | Missing data representation | float_format : string, default None | Format string for floating point numbers | header : boolean, default False | Write out series name | index : boolean, default True | Write row names (index) | index_label : string or sequence, default None | Column label for index column(s) if desired. If None is given, and |headerandindexare True, then the index names are used. A | sequence should be given if the DataFrame uses MultiIndex. | mode : Python write mode, default &#39;w&#39; | sep : character, default &quot;,&quot; | Field delimiter for the output file. | encoding : string, optional | a string representing the encoding to use if the contents are 435 | non-ascii, for python versions prior to 3 | date_format: string, default None | Format string for datetime objects. | decimal: string, default &#39;.&#39; | Character recognized as decimal separator. E.g. use &#39;,&#39; for European data | | to_dict(self) | Convert Series to {label -&gt; value} dict | | 【返回值】 | -------| value_dict : dict | | to_frame(self, name=None) | Convert Series to DataFrame | | 【参数】 | ----------| name : object, default None | The passed name should substitute for the series name (if it has | one). | | 【返回值】 | -------| data_frame : DataFrame | | to_period(self, freq=None, copy=True) | Convert Series from DatetimeIndex to PeriodIndex with desired | frequency (inferred from index if not passed) | | 【参数】 | ----------| freq : string, default | | 【返回值】 | -------| ts : Series with PeriodIndex | | to_sparse(self, kind=&#39;block&#39;, fill_value=None) | Convert Series to SparseSeries | | 【参数】 | ----------| kind : {&#39;block&#39;, &#39;integer&#39;} | fill_value : float, defaults to NaN (missing) | | 【返回值】 | -------| sp : SparseSeries | | to_string(self, buf=None, na_rep=&#39;NaN&#39;, float_format=None, header=True, length=False, dtype=False, name=False, max_rows=None) | Render a string representation of the Series | | 【参数】 | ---------- 436 | buf : StringIO-like, optional | buffer to write to | na_rep : string, optional | string representation of NAN to use, default &#39;NaN&#39; | float_format : one-parameter function, optional | formatter function to apply to columns&#39; elements if they are floats | default None | header: boolean, default True | Add the Series header (index name) | length : boolean, default False | Add the Series length | dtype : boolean, default False | Add the Series dtype | name : boolean, default False | Add the Series name if not None | max_rows : int, optional | Maximum number of rows to show before truncating. If None, show | all. | | 【返回值】 | -------| formatted : string (if not buffer passed) | | to_timestamp(self, freq=None, how=&#39;start&#39;, copy=True) | Cast to datetimeindex of timestamps, at *beginning* of period | | 【参数】 | ----------| freq : string, default frequency of PeriodIndex | Desired frequency | how : {&#39;s&#39;, &#39;e&#39;, &#39;start&#39;, &#39;end&#39;} | Convention for converting period to timestamp; start of period | vs. end | | 【返回值】 | -------| ts : Series with DatetimeIndex | | tolist(self) | Convert Series to a nested list | | truediv(self, other, level=None, fill_value=None, axis=0) | Floating division of series and other, element-wise (binary operatortruediv). | | Equivalent to ``series / other``, but with support to substitute a fill_value for | missing data in one of the inputs. | | 【参数】 | ----------| other: Series or scalar value | fill_value : None or float value, default None (NaN) | Fill missing (NaN) values with this value. If both Series are | missing, the result will be missing | level : int or name | Broadcast across a level, matching Index values on the | passed MultiIndex level | 437 | 【返回值】 | -------| result : Series | | 【参见】 | --------| Series.rtruediv | | unstack(self, level=-1) | Unstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame. | The level involved will automatically get sorted. | | 【参数】 | ----------| level : int, string, or list of these, default last level | Level(s) to unstack, can pass level name | | 【示例】 | --------| &gt;&gt;&gt; s | one a 1. | one b 2. | two a 3. | two b 4. | | &gt;&gt;&gt; s.unstack(level=-1) | a b | one 1. 2. | two 3. 4. | | &gt;&gt;&gt; s.unstack(level=0) | one two | a 1. 2. | b 3. 4. | | 【返回值】 | -------| unstacked : DataFrame | | update(self, other) | Modify Series in place using non-NA values from passed | Series. Aligns on index | | 【参数】 | ----------| other : Series | | valid lambda self, inplace=False, **kwargs | | var(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs) | Return unbiased variance over requested axis. | | Normalized by N-1 by default. This can be changed using the ddof argument | | 【参数】 | ----------| axis : {index (0)} 438 | skipna : boolean, default True | Exclude NA/null values. If an entire row/column is NA, the result | will be NA | level : int or level name, default None | If the axis is a MultiIndex (hierarchical), count along a | particular level, collapsing into a scalar | ddof : int, default 1 | degrees of freedom | numeric_only : boolean, default None | Include only float, int, boolean data. If None, will attempt to use | everything, then use only numeric data | | 【返回值】 | -------| var : scalar or Series (if level specified) | | view(self, dtype=None) | | ----------------------------------------------------------------------| Class methods defined here: | | from_array(arr, index=None, name=None, dtype=None, copy=False, fastpath=False) from builtins.type | | from_csv(path, sep=&#39;,&#39;, parse_dates=True, header=None, index_col=0, encoding=None, infer_datetime_format=False) from builtins.type | Read CSV file (DISCOURAGED, please use :func:pandas.read_csvinstead). | | It is preferable to use the more powerful :func:pandas.read_csv| for most general purposes, but ``from_csv`` makes for an easy | roundtrip to and from a file (the exact counterpart of | ``to_csv``), especially with a time Series. | | This method only differs from :func:pandas.read_csvin some defaults: | | -index_colis ``0`` instead of ``None`` (take first column as index | by default) | -headeris ``None`` instead of ``0`` (the first row is not used as | the column names) | -parse_datesis ``True`` instead of ``False`` (try parsing the index | as datetime by default) | | With :func:pandas.read_csv, the option ``squeeze=True`` can be used | to return a Series like ``from_csv``. | | 【参数】 | ----------| path : string file path or file handle / StringIO | sep : string, default &#39;,&#39; | Field delimiter | parse_dates : boolean, default True | Parse dates. Different default from read_table | header : int, default None | Row to use as header (skip prior rows) | index_col : int or sequence, default 0 | Column to use for index. If a sequence is given, a MultiIndex | is used. Different default from read_table | encoding : string, optional 439 | a string representing the encoding to use if the contents are | non-ascii, for python versions prior to 3 | infer_datetime_format: boolean, default False | If True andparse_datesis True for a column, try to infer the | datetime format based on the first datetime string. If the format | can be inferred, there often will be a large parsing speed-up. | | 【参见】 | --------| pandas.read_csv | | 【返回值】 | -------| y : Series | | ----------------------------------------------------------------------| Data descriptors defined here: | | axes | Return a list of the row axis labels | | dtype | return the dtype object of the underlying data | | dtypes | return the dtype object of the underlying data | | ftype | return if the data is sparse|dense | | ftypes | return if the data is sparse|dense | | imag | | index | | is_time_series | | real | | values | Return Series as ndarray or ndarray-like | depending on the dtype | | 【返回值】 | -------| arr : numpy.ndarray or ndarray-like | | 【示例】 | --------| &gt;&gt;&gt; pd.Series([1, 2, 3]).values | array([1, 2, 3]) | | &gt;&gt;&gt; pd.Series(list(&#39;aabc&#39;)).values | array([&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;], dtype=object) | 440 | &gt;&gt;&gt; pd.Series(list(&#39;aabc&#39;)).astype(&#39;category&#39;).values | [a, a, b, c] | Categories (3, object): [a, b, c] | | Timezone aware datetime data is converted to UTC: | | &gt;&gt;&gt; pd.Series(pd.date_range(&#39;20130101&#39;,periods=3,tz=&#39;US/Eastern&#39;)).values | array([&#39;2013-01-01T00:00:00.000000000-0500&#39;, | &#39;2013-01-02T00:00:00.000000000-0500&#39;, | &#39;2013-01-03T00:00:00.000000000-0500&#39;], dtype=&#39;datetime64[ns]&#39;) | | ----------------------------------------------------------------------| 其他数据、属性定义： | | cat = &lt;class &#39;pandas.core.categorical.CategoricalAccessor&#39;&gt; | Accessor object for categorical properties of the Series values. | | Be aware that assigning tocategoriesis a inplace operation, while all methods return | new categorical data per default (but can be called withinplace=True). | | 【示例】 | --------| &gt;&gt;&gt; s.cat.categories | &gt;&gt;&gt; s.cat.categories = list(&#39;abc&#39;) | &gt;&gt;&gt; s.cat.rename_categories(list(&#39;cab&#39;)) | &gt;&gt;&gt; s.cat.reorder_categories(list(&#39;cab&#39;)) | &gt;&gt;&gt; s.cat.add_categories([&#39;d&#39;,&#39;e&#39;]) | &gt;&gt;&gt; s.cat.remove_categories([&#39;d&#39;]) | &gt;&gt;&gt; s.cat.remove_unused_categories() | &gt;&gt;&gt; s.cat.set_categories(list(&#39;abcde&#39;)) | &gt;&gt;&gt; s.cat.as_ordered() | &gt;&gt;&gt; s.cat.as_unordered() | | dt = &lt;class &#39;pandas.tseries.common.CombinedDatetimelikeProperties&#39;&gt; | Accessor object for datetimelike properties of the Series values. | | 【示例】 | --------| &gt;&gt;&gt; s.dt.hour | &gt;&gt;&gt; s.dt.second | &gt;&gt;&gt; s.dt.quarter | | Returns a Series indexed like the original Series. | Raises TypeError if the Series does not contain datetimelike values. | | plot = &lt;class &#39;pandas.tools.plotting.SeriesPlotMethods&#39;&gt; | Series plotting accessor and method | | 【示例】 | --------| &gt;&gt;&gt; s.plot.line() | &gt;&gt;&gt; s.plot.bar() | &gt;&gt;&gt; s.plot.hist() | | Plotting methods can also be accessed by calling the accessor as a method | with the ``kind`` argument: | ``s.plot(kind=&#39;line&#39;)`` is equivalent to ``s.plot.line()`` 441 | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.IndexOpsMixin: | | factorize(self, sort=False, na_sentinel=-1) | Encode the object as an enumerated type or categorical variable | | 【参数】 | ----------| sort : boolean, default False | Sort by values | na_sentinel: int, default -1 | Value to mark &quot;not found&quot; | | 【返回值】 | -------| labels : the indexer to the original array | uniques : the unique Index | | item(self) | return the first element of the underlying data as a python scalar | | nunique(self, dropna=True) | Return number of unique elements in the object. | | Excludes NA values by default. | | 【参数】 | ----------| dropna : boolean, default True | Don&#39;t include NaN in the count. | | 【返回值】 | -------| nunique : int | | transpose(self) | return the transpose, which is by definition self | | unique(self) | Return array of unique values in the object. Significantly faster than | numpy.unique. Includes NA values. | | 【返回值】 | -------| uniques : ndarray | | value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True) | Returns object containing counts of unique values. | | The resulting object will be in descending order so that the | first element is the most frequently-occurring element. | Excludes NA values by default. | | 【参数】 | ----------| normalize : boolean, default False 442 | If True then the object returned will contain the relative | frequencies of the unique values. | sort : boolean, default True | Sort by values | ascending : boolean, default False | Sort in ascending order | bins : integer, optional | Rather than count values, group them into half-open bins, | a convenience for pd.cut, only works with numeric data | dropna : boolean, default True | Don&#39;t include counts of NaN. | | 【返回值】 | -------| counts : Series | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.IndexOpsMixin: | | T | return the transpose, which is by definition self | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | base | return the base object if the memory of the underlying data is shared | | data | return the data pointer of the underlying data | | flags | return the ndarray.flags for the underlying data | | hasnans | | itemsize | return the size of the dtype of the item of the underlying data | | nbytes | return the number of bytes in the underlying data | | ndim | return the number of dimensions of the underlying data, by definition 1 | | shape | return a tuple of the shape of the underlying data | | size | return the number of elements in the underlying data | | strides | return the strides of the underlying data | 443 | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.base.IndexOpsMixin: | | __array_priority__ = 1000 | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.strings.StringAccessorMixin: | | str = &lt;class &#39;pandas.core.strings.StringMethods&#39;&gt; | Vectorized string functions for Series and Index. NAs stay NA unless | handled otherwise by a particular method. Patterned after Python&#39;s string | methods, with some inspiration from R&#39;s stringr package. | | 【示例】 | --------| &gt;&gt;&gt; s.str.split(&#39;_&#39;) | &gt;&gt;&gt; s.str.replace(&#39;_&#39;, &#39;&#39;) | | ----------------------------------------------------------------------| Methods inherited from pandas.core.generic.NDFrame: | | __abs__(self) | | __bool__ = __nonzero__(self) | | __contains__(self, key) | True if the key is in the info axis | | __delitem__(self, key) | Delete item | | __finalize__(self, other, method=None, **kwargs) | propagate metadata from other to self | | 【参数】 | ----------| other : the object from which to get the attributes that we are going | to propagate | method : optional, a passed method name ; possibly to take different | types of propagation actions based on this | | __getattr__(self, name) | After regular attribute access, try looking up the name | This allows simpler access to columns for interactive use. | | __getstate__(self) | | __hash__(self) | Return hash(self). | | __invert__(self) | | __neg__(self) | | __nonzero__(self) | | __setattr__(self, name, value) 444 | After regular attribute access, try setting the name | This allows simpler access to columns for interactive use. | | __setstate__(self, state) | | abs(self) | Return an object with absolute value taken. Only applicable to objects | that are all numeric | | 【返回值】 | -------| abs: type of caller | | add_prefix(self, prefix) | Concatenate prefix string with panel items names. | | 【参数】 | ----------| prefix : string | | 【返回值】 | -------| with_prefix : type of caller | | add_suffix(self, suffix) | Concatenate suffix string with panel items names | | 【参数】 | ----------| suffix : string | | 【返回值】 | -------| with_suffix : type of caller | | as_blocks(self, copy=True) | Convert the frame to a dict of dtype -&gt; Constructor Types that each has | a homogeneous dtype. | | NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in | as_matrix) | | 【参数】 | ----------| copy : boolean, default True | | .. versionadded: 0.16.1 | | 【返回值】 | -------| values : a dict of dtype -&gt; Constructor Types | | as_matrix(self, columns=None) | Convert the frame to its Numpy-array representation. | | 【参数】 445 | ----------| columns: list, optional, default:None | If None, return all columns, otherwise, returns specified columns. | | 【返回值】 | -------| values : ndarray | If the caller is heterogeneous and contains booleans or objects, | the result will be of dtype=object. See Notes. | | | 【注意】 | -----| Return is NOT a Numpy-matrix, rather, a Numpy-array. | | The dtype will be a lower-common-denominator dtype (implicit | upcasting); that is to say if the dtypes (even of numeric types) | are mixed, the one that accommodates all will be chosen. Use this | with care if you are not dealing with the blocks. | | e.g. If the dtypes are float16 and float32, dtype will be upcast to | float32. If dtypes are int32 and uint8, dtype will be upcase to | int32. | | This method is provided for backwards compatibility. Generally, | it is recommended to use &#39;.values&#39;. | | 【参见】 | --------| pandas.DataFrame.values | | asfreq(self, freq, method=None, how=None, normalize=False) | Convert all TimeSeries inside to specified frequency using DateOffset | objects. Optionally provide fill method to pad/backfill missing values. | | 【参数】 | ----------| freq : DateOffset object, or string | method : {&#39;backfill&#39;, &#39;bfill&#39;, &#39;pad&#39;, &#39;ffill&#39;, None} | Method to use for filling holes in reindexed Series | pad / ffill: propagate last valid observation forward to next valid | backfill / bfill: use NEXT valid observation to fill method | how : {&#39;start&#39;, &#39;end&#39;}, default end | For PeriodIndex only, see PeriodIndex.asfreq | normalize : bool, default False | Whether to reset output index to midnight | | 【返回值】 | -------| converted : type of caller | | astype(self, dtype, copy=True, raise_on_error=True, **kwargs) | Cast object to input numpy.dtype | Return a copy when copy = True (be really careful with this!) | | 【参数】 | ---------- 446 | dtype : numpy.dtype or Python type | raise_on_error : raise on invalid input | kwargs : keyword arguments to pass on to the constructor | | 【返回值】 | -------| casted : type of caller | | at_time(self, time, asof=False) | Select values at particular time of day (e.g. 9:30AM) | | 【参数】 | ----------| time : datetime.time or string | | 【返回值】 | -------| values_at_time : type of caller | | between_time(self, start_time, end_time, include_start=True, include_end=True) | Select values between particular times of the day (e.g., 9:00-9:30 AM) | | 【参数】 | ----------| start_time : datetime.time or string | end_time : datetime.time or string | include_start : boolean, default True | include_end : boolean, default True | | 【返回值】 | -------| values_between_time : type of caller | | bfill(self, axis=None, inplace=False, limit=None, downcast=None) | Synonym for NDFrame.fillna(method=&#39;bfill&#39;) | | bool(self) | Return the bool of a single element PandasObject | This must be a boolean scalar value, either True or False | | Raise a ValueError if the PandasObject does not have exactly | 1 element, or that element is not boolean | | clip(self, lower=None, upper=None, out=None, axis=None) | Trim values at input threshold(s) | | 【参数】 | ----------| lower : float or array_like, default None | upper : float or array_like, default None | axis : int or string axis name, optional | Align object with lower and upper along the given axis. | | 【返回值】 | -------| clipped : Series | 447 | 【示例】 | --------| &gt;&gt;&gt; df | 0 1 | 0 0.335232 -1.256177 | 1 -1.367855 0.746646 | 2 0.027753 -1.176076 | 3 0.230930 -0.679613 | 4 1.261967 0.570967 | &gt;&gt;&gt; df.clip(-1.0, 0.5) | 0 1 | 0 0.335232 -1.000000 | 1 -1.000000 0.500000 | 2 0.027753 -1.000000 | 3 0.230930 -0.679613 | 4 0.500000 0.500000 | &gt;&gt;&gt; t | 0 -0.3 | 1 -0.2 | 2 -0.1 | 3 0.0 | 4 0.1 | dtype: float64 | &gt;&gt;&gt; df.clip(t, t + 1, axis=0) | 0 1 | 0 0.335232 -0.300000 | 1 -0.200000 0.746646 | 2 0.027753 -0.100000 | 3 0.230930 0.000000 | 4 1.100000 0.570967 | | clip_lower(self, threshold, axis=None) | Return copy of the input with values below given value(s) truncated | | 【参数】 | ----------| threshold : float or array_like | axis : int or string axis name, optional | Align object with threshold along the given axis. | | 【参见】 | --------| clip | | 【返回值】 | -------| clipped : same type as input | | clip_upper(self, threshold, axis=None) | Return copy of input with values above given value(s) truncated | | 【参数】 | ----------| threshold : float or array_like | axis : int or string axis name, optional | Align object with threshold along the given axis. | 448 | 【参见】 | --------| clip | | 【返回值】 | -------| clipped : same type as input | | consolidate(self, inplace=False) | Compute NDFrame with &quot;consolidated&quot; internals (data of each dtype | grouped together in a single ndarray). Mainly an internal API function, | but available here to the savvy user | | 【参数】 | ----------| inplace : boolean, default False | If False return new object, otherwise modify existing object | | 【返回值】 | -------| consolidated : type of caller | | convert_objects(self, convert_dates=True, convert_numeric=False, convert_timedeltas=True, copy=True) | Attempt to infer better dtype for object columns | | 【参数】 | ----------| convert_dates : boolean, default True | If True, convert to date where possible. If &#39;coerce&#39;, force | conversion, with unconvertible values becoming NaT. | convert_numeric : boolean, default False | If True, attempt to coerce to numbers (including strings), with | unconvertible values becoming NaN. | convert_timedeltas : boolean, default True | If True, convert to timedelta where possible. If &#39;coerce&#39;, force | conversion, with unconvertible values becoming NaT. | copy : boolean, default True | If True, return a copy even if no copy is necessary (e.g. no | conversion was done). Note: This is meant for internal use, and | should not be confused with inplace. | | 【返回值】 | -------| converted : same as input object | | copy(self, deep=True) | Make a copy of this object | | 【参数】 | ----------| deep : boolean or string, default True | Make a deep copy, i.e. also copy data | | 【返回值】 | -------| copy : type of caller 449 | | describe(self, percentiles=None, include=None, exclude=None) | Generate various summary statistics, excluding NaN values. | | 【参数】 | ----------| percentiles : array-like, optional | The percentiles to include in the output. Should all | be in the interval [0, 1]. By defaultpercentilesis | [.25, .5, .75], returning the 25th, 50th, and 75th percentiles. | include, exclude : list-like, &#39;all&#39;, or None (default) | Specify the form of the returned result. Either: | | - None to both (default). The result will include only numeric-typed | columns or, if none are, only categorical columns. | - A list of dtypes or strings to be included/excluded. | To select all numeric types use numpy numpy.number. To select | categorical objects use type object. 参见：the select_dtypes | documentation. eg. df.describe(include=[&#39;O&#39;]) | - If include is the string &#39;all&#39;, the output column-set will | match the input one. | | 【返回值】 | -------| summary: NDFrame of summary statistics | | 【注意】 | -----| The output DataFrame index depends on the requested dtypes: | | For numeric dtypes, it will include: count, mean, std, min, | max, and lower, 50, and upper percentiles. | | For object dtypes (e.g. timestamps or strings), the index | will include the count, unique, most common, and frequency of the | most common. Timestamps also include the first and last items. | | For mixed dtypes, the index will be the union of the corresponding | output types. Non-applicable entries will be filled with NaN. | Note that mixed-dtype outputs can only be returned from mixed-dtype | inputs and appropriate use of the include/exclude arguments. | | If multiple values have the highest count, then the |countandmost commonpair will be arbitrarily chosen from | among those with the highest count. | | The include, exclude arguments are ignored for Series. | | 【参见】 | --------| DataFrame.select_dtypes | | drop(self, labels, axis=0, level=None, inplace=False, errors=&#39;raise&#39;) | Return new object with labels in requested axis removed | | 【参数】 | ---------- 450 | labels : single label or list-like | axis : int or axis name | level : int or level name, default None | For MultiIndex | inplace : bool, default False | If True, do operation inplace and return None. | errors : {&#39;ignore&#39;, &#39;raise&#39;}, default &#39;raise&#39; | If &#39;ignore&#39;, suppress error and existing labels are dropped. | | .. versionadded:: 0.16.1 | | 【返回值】 | -------| dropped : type of caller | | equals(self, other) | Determines if two NDFrame objects contain the same elements. NaNs in the | same location are considered equal. | | ffill(self, axis=None, inplace=False, limit=None, downcast=None) | Synonym for NDFrame.fillna(method=&#39;ffill&#39;) | | filter(self, items=None, like=None, regex=None, axis=None) | Restrict the info axis to set of items or wildcard | | 【参数】 | ----------| items : list-like | List of info axis to restrict to (must not all be present) | like : string | Keep info axis where &quot;arg in col == True&quot; | regex : string (regular expression) | Keep info axis with re.search(regex, col) == True | axis : int or None | The axis to filter on. By default this is the info axis. The &quot;info | axis&quot; is the axis that is used when indexing with ``[]``. For | example, ``df = DataFrame({&#39;a&#39;: [1, 2, 3, 4]]}); df[&#39;a&#39;]``. So, | the ``DataFrame`` columns are the info axis. | | 【注意】 | -----| Arguments are mutually exclusive, but this is not checked for | | first(self, offset) | Convenience method for subsetting initial periods of time series data | based on a date offset | | 【参数】 | ----------| offset : string, DateOffset, dateutil.relativedelta | | 【示例】 | --------| ts.last(&#39;10D&#39;) -&gt; First 10 days | | 【返回值】 | ------- 451 | subset : type of caller | | get(self, key, default=None) | Get item from object for given key (DataFrame column, Panel slice, | etc.). Returns default value if not found | | 【参数】 | ----------| key : object | | 【返回值】 | -------| value : type of items contained in object | | get_dtype_counts(self) | Return the counts of dtypes in this object | | get_ftype_counts(self) | Return the counts of ftypes in this object | | groupby(self, by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False) | Group series using mapper (dict or key function, apply given function | to group, return result as series) or by a series of columns | | 【参数】 | ----------| by : mapping function / list of functions, dict, Series, or tuple / | list of column names. | Called on each element of the object index to determine the groups. | If a dict or Series is passed, the Series or dict VALUES will be | used to determine the groups | axis : int, default 0 | level : int, level name, or sequence of such, default None | If the axis is a MultiIndex (hierarchical), group by a particular | level or levels | as_index : boolean, default True | For aggregated output, return object with group labels as the | index. Only relevant for DataFrame input. as_index=False is | effectively &quot;SQL-style&quot; grouped output | sort : boolean, default True | Sort group keys. Get better performance by turning this off. | Note this does not influence the order of observations within each group. | groupby preserves the order of rows within each group. | group_keys : boolean, default True | When calling apply, add group keys to index to identify pieces | squeeze : boolean, default False | reduce the dimensionality of the return type if possible, | otherwise return a consistent type | | 【示例】 | --------| DataFrame results | | &gt;&gt;&gt; data.groupby(func, axis=0).mean() | &gt;&gt;&gt; data.groupby([&#39;col1&#39;, &#39;col2&#39;])[&#39;col3&#39;].mean() | | DataFrame with hierarchical index 452 | | &gt;&gt;&gt; data.groupby([&#39;col1&#39;, &#39;col2&#39;]).mean() | | 【返回值】 | -------| GroupBy object | | head(self, n=5) | Returns first n rows | | interpolate(self, method=&#39;linear&#39;, axis=0, limit=None, inplace=False, limit_direction=&#39;forward&#39;, downcast=None, **kwargs) | Interpolate values according to different methods. | | Please note that only ``method=&#39;linear&#39;`` is supported for DataFrames/Series | with a MultiIndex. | | 【参数】 | ----------| method : {&#39;linear&#39;, &#39;time&#39;, &#39;index&#39;, &#39;values&#39;, &#39;nearest&#39;, &#39;zero&#39;, | &#39;slinear&#39;, &#39;quadratic&#39;, &#39;cubic&#39;, &#39;barycentric&#39;, &#39;krogh&#39;, | &#39;polynomial&#39;, &#39;spline&#39; &#39;piecewise_polynomial&#39;, &#39;pchip&#39;} | | * &#39;linear&#39;: ignore the index and treat the values as equally | spaced. This is the only method supported on MultiIndexes. | default | * &#39;time&#39;: interpolation works on daily and higher resolution | data to interpolate given length of interval | * &#39;index&#39;, &#39;values&#39;: use the actual numerical values of the index | * &#39;nearest&#39;, &#39;zero&#39;, &#39;slinear&#39;, &#39;quadratic&#39;, &#39;cubic&#39;, | &#39;barycentric&#39;, &#39;polynomial&#39; is passed to | ``scipy.interpolate.interp1d``. Both &#39;polynomial&#39; and &#39;spline&#39; | require that you also specify anorder(int), | e.g. df.interpolate(method=&#39;polynomial&#39;, order=4). | These use the actual numerical values of the index. | * &#39;krogh&#39;, &#39;piecewise_polynomial&#39;, &#39;spline&#39;, and &#39;pchip&#39; are all | wrappers around the scipy interpolation methods of similar | names. These use the actual numerical values of the index. See | the scipy documentation for more on their behavior |here http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation__ |and here http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html__ | | axis : {0, 1}, default 0 | * 0: fill column-by-column | * 1: fill row-by-row | limit : int, default None. | Maximum number of consecutive NaNs to fill. | limit_direction : {&#39;forward&#39;, &#39;backward&#39;, &#39;both&#39;}, defaults to &#39;forward&#39; | If limit is specified, consecutive NaNs will be filled in this | direction. | | .. versionadded:: 0.17.0 | | inplace : bool, default False | Update the NDFrame in place if possible. | downcast : optional, &#39;infer&#39; or None, defaults to None | Downcast dtypes if possible. | kwargs : keyword arguments to pass on to the interpolating function. 453 | | 【返回值】 | -------| Series or DataFrame of same shape interpolated at the NaNs | | 【参见】 | --------| reindex, replace, fillna | | 【示例】 | --------| | Filling in NaNs | | &gt;&gt;&gt; s = pd.Series([0, 1, np.nan, 3]) | &gt;&gt;&gt; s.interpolate() | 0 0 | 1 1 | 2 2 | 3 3 | dtype: float64 | | isnull(self) | Return a boolean same-sized object indicating if the values are null | | 【参见】 | --------| notnull : boolean inverse of isnull | | iterkv(self, *args, **kwargs) | iteritems alias used to get around 2to3. Deprecated | | last(self, offset) | Convenience method for subsetting final periods of time series data | based on a date offset | | 【参数】 | ----------| offset : string, DateOffset, dateutil.relativedelta | | 【示例】 | --------| ts.last(&#39;5M&#39;) -&gt; Last 5 months | | 【返回值】 | -------| subset : type of caller | | mask(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True) | Return an object of same shape as self and whose corresponding | entries are from self where cond is False and otherwise are from other. | | 【参数】 | ----------| cond : boolean NDFrame or array | other : scalar or NDFrame 454 | inplace : boolean, default False | Whether to perform the operation in place on the data | axis : alignment axis if needed, default None | level : alignment level if needed, default None | try_cast : boolean, default False | try to cast the result back to the input type (if possible), | raise_on_error : boolean, default True | Whether to raise on invalid data types (e.g. trying to where on | strings) | | 【返回值】 | -------| wh : same type as caller | | notnull(self) | Return a boolean same-sized object indicating if the values are | not null | | 【参见】 | --------| isnull : boolean inverse of notnull | | pct_change(self, periods=1, fill_method=&#39;pad&#39;, limit=None, freq=None, **kwargs) | Percent change over given number of periods. | | 【参数】 | ----------| periods : int, default 1 | Periods to shift for forming percent change | fill_method : str, default &#39;pad&#39; | How to handle NAs before computing percent changes | limit : int, default None | The number of consecutive NAs to fill before stopping | freq : DateOffset, timedelta, or offset alias string, optional | Increment to use from time series API (e.g. &#39;M&#39; or BDay()) | | 【返回值】 | -------| chg : NDFrame | | 【注意】 | -----| | By default, the percentage change is calculated along the stat | axis: 0, or ``Index``, for ``DataFrame`` and 1, or ``minor`` for | ``Panel``. You can change this with the ``axis`` keyword argument. | | pipe(self, func, *args, **kwargs) | Apply func(self, \\*args, \\*\\*kwargs) | | .. versionadded:: 0.16.2 | | 【参数】 | ----------| func : function | function to apply to the NDFrame. | ``args``, and ``kwargs`` are passed into ``func``. 455 | Alternatively a ``(callable, data_keyword)`` tuple where | ``data_keyword`` is a string indicating the keyword of | ``callable`` that expects the NDFrame. | args : positional arguments passed into ``func``. | kwargs : a dictionary of keyword arguments passed into ``func``. | | 【返回值】 | -------| object : the return type of ``func``. | | 【注意】 | -----| | Use ``.pipe`` when chaining together functions that expect | on Series or DataFrames. Instead of writing | | &gt;&gt;&gt; f(g(h(df), arg1=a), arg2=b, arg3=c) | | You can write | | &gt;&gt;&gt; (df.pipe(h) | ... .pipe(g, arg1=a) | ... .pipe(f, arg2=b, arg3=c) | ... ) | | If you have a function that takes the data as (say) the second | argument, pass a tuple indicating which keyword expects the | data. For example, suppose ``f`` takes its data as ``arg2``: | | &gt;&gt;&gt; (df.pipe(h) | ... .pipe(g, arg1=a) | ... .pipe((f, &#39;arg2&#39;), arg1=a, arg3=c) | ... ) | | 【参见】 | --------| pandas.DataFrame.apply | pandas.DataFrame.applymap | pandas.Series.map | | pop(self, item) | Return item and drop from frame. Raise KeyError if not found. | | reindex_like(self, other, method=None, copy=True, limit=None, tolerance=None) | return an object with matching indicies to myself | | 【参数】 | ----------| other : Object | method : string or None | copy : boolean, default True | limit : int, default None | Maximum number of consecutive labels to fill for inexact matches. | tolerance : optional | Maximum distance between labels of the other object and this | object for inexact matches. | 456 | .. versionadded:: 0.17.0 | | 【注意】 | -----| Like calling s.reindex(index=other.index, columns=other.columns, | method=...) | | 【返回值】 | -------| reindexed : same as input | | rename_axis(self, mapper, axis=0, copy=True, inplace=False) | Alter index and / or columns using input function or functions. | Function / dict values must be unique (1-to-1). Labels not contained in | a dict / Series will be left as-is. | | 【参数】 | ----------| mapper : dict-like or function, optional | axis : int or string, default 0 | copy : boolean, default True | Also copy underlying data | inplace : boolean, default False | | 【返回值】 | -------| renamed : type of caller | | replace(self, to_replace=None, value=None, inplace=False, limit=None, regex=False, method=&#39;pad&#39;, axis=None) | Replace values given in &#39;to_replace&#39; with &#39;value&#39;. | | 【参数】 | ----------| to_replace : str, regex, list, dict, Series, numeric, or None | | * str or regex: | | - str: string exactly matchingto_replacewill be replaced | withvalue| - regex: regexs matchingto_replacewill be replaced with |value| | * list of str, regex, or numeric: | | - First, ifto_replaceandvalueare both lists, they | **must** be the same length. | - Second, if ``regex=True`` then all of the strings in **both** | lists will be interpreted as regexs otherwise they will match | directly. This doesn&#39;t matter much forvaluesince there | are only a few possible substitution regexes you can use. | - str and regex rules apply as above. | | * dict: | | - Nested dictionaries, e.g., {&#39;a&#39;: {&#39;b&#39;: nan}}, are read as | follows: look in column &#39;a&#39; for the value &#39;b&#39; and replace it | with nan. You can nest regular expressions as well. Note that 457 | column names (the top-level dictionary keys in a nested | dictionary) **cannot** be regular expressions. | - Keys map to column names and values map to substitution | values. You can treat this as a special case of passing two | lists except that you are specifying the column to search in. | | * None: | | - This means that the ``regex`` argument must be a string, | compiled regular expression, or list, dict, ndarray or Series | of such elements. Ifvalueis also ``None`` then this | **must** be a nested dictionary or ``Series``. | | See the examples section for examples of each of these. | value : scalar, dict, list, str, regex, default None | Value to use to fill holes (e.g. 0), alternately a dict of values | specifying which value to use for each column (columns not in the | dict will not be filled). Regular expressions, strings and lists or | dicts of such objects are also allowed. | inplace : boolean, default False | If True, in place. Note: this will modify any | other views on this object (e.g. a column form a DataFrame). | Returns the caller if this is True. | limit : int, default None | Maximum size gap to forward or backward fill | regex : bool or same types asto_replace, default False | Whether to interpretto_replaceand/orvalueas regular | expressions. If this is ``True`` thento_replace*must* be a | string. Otherwise,to_replacemust be ``None`` because this | parameter will be interpreted as a regular expression or a list, | dict, or array of regular expressions. | method : string, optional, {&#39;pad&#39;, &#39;ffill&#39;, &#39;bfill&#39;} | The method to use when for replacement, when ``to_replace`` is a | ``list``. | | 【参见】 | --------| NDFrame.reindex | NDFrame.asfreq | NDFrame.fillna | | 【返回值】 | -------| filled : NDFrame | | 【Raises 引发错误】 | ------| AssertionError | * Ifregexis not a ``bool`` andto_replaceis not ``None``. | TypeError | * Ifto_replaceis a ``dict`` andvalueis not a ``list``, | ``dict``, ``ndarray``, or ``Series`` | * Ifto_replaceis ``None`` andregexis not compilable into a | regular expression or is a list, dict, ndarray, or Series. | ValueError | * Ifto_replaceandvalueare ``list`` s or ``ndarray`` s, but | they are not the same length. 458 | | 【注意】 | -----| * Regex substitution is performed under the hood with ``re.sub``. The | rules for substitution for ``re.sub`` are the same. | * Regular expressions will only substitute on strings, meaning you | cannot provide, for example, a regular expression matching floating | point numbers and expect the columns in your frame that have a | numeric dtype to be matched. However, if those floating point numbers | *are* strings, then you can do this. | * This method has *a lot* of options. You are encouraged to experiment | and play with this method to gain intuition about how it works. | | resample(self, rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention=&#39;start&#39;, kind=None, loffset=None, limit=None, base=0) | Convenience method for frequency conversion and resampling of regular | time-series data. | | 【参数】 | ----------| rule : string | the offset string or object representing target conversion | how : string | method for down- or re-sampling, default to &#39;mean&#39; for | downsampling | axis : int, optional, default 0 | fill_method : string, default None | fill_method for upsampling | closed : {&#39;right&#39;, &#39;left&#39;} | Which side of bin interval is closed | label : {&#39;right&#39;, &#39;left&#39;} | Which bin edge label to label bucket with | convention : {&#39;start&#39;, &#39;end&#39;, &#39;s&#39;, &#39;e&#39;} | kind : &quot;period&quot;/&quot;timestamp&quot; | loffset : timedelta | Adjust the resampled time labels | limit : int, default None | Maximum size gap to when reindexing with fill_method | base : int, default 0 | For frequencies that evenly subdivide 1 day, the &quot;origin&quot; of the | aggregated intervals. For example, for &#39;5min&#39; frequency, base could | range from 0 through 4. Defaults to 0 | | | 【示例】 | --------| | Start by creating a series with 9 one minute timestamps. | | &gt;&gt;&gt; index = pd.date_range(&#39;1/1/2000&#39;, periods=9, freq=&#39;T&#39;) | &gt;&gt;&gt; series = pd.Series(range(9), index=index) | &gt;&gt;&gt; series | 2000-01-01 00:00:00 0 | 2000-01-01 00:01:00 1 | 2000-01-01 00:02:00 2 | 2000-01-01 00:03:00 3 | 2000-01-01 00:04:00 4 459 | 2000-01-01 00:05:00 5 | 2000-01-01 00:06:00 6 | 2000-01-01 00:07:00 7 | 2000-01-01 00:08:00 8 | Freq: T, dtype: int64 | | Downsample the series into 3 minute bins and sum the values | of the timestamps falling into a bin. | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=&#39;sum&#39;) | 2000-01-01 00:00:00 3 | 2000-01-01 00:03:00 12 | 2000-01-01 00:06:00 21 | Freq: 3T, dtype: int64 | | Downsample the series into 3 minute bins as above, but label each | bin using the right edge instead of the left. Please note that the | value in the bucket used as the label is not included in the bucket, | which it labels. For example, in the original series the | bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed | value in the resampled bucket with the label``2000-01-01 00:03:00`` | does not include 3 (if it did, the summed value would be 6, not 3). | To include this value close the right side of the bin interval as | illustrated in the example below this one. | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=&#39;sum&#39;, label=&#39;right&#39;) | 2000-01-01 00:03:00 3 | 2000-01-01 00:06:00 12 | 2000-01-01 00:09:00 21 | Freq: 3T, dtype: int64 | | Downsample the series into 3 minute bins as above, but close the right | side of the bin interval. | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=&#39;sum&#39;, label=&#39;right&#39;, closed=&#39;right&#39;) | 2000-01-01 00:00:00 0 | 2000-01-01 00:03:00 6 | 2000-01-01 00:06:00 15 | 2000-01-01 00:09:00 15 | Freq: 3T, dtype: int64 | | Upsample the series into 30 second bins. | | &gt;&gt;&gt; series.resample(&#39;30S&#39;)[0:5] #select first 5 rows | 2000-01-01 00:00:00 0 | 2000-01-01 00:00:30 NaN | 2000-01-01 00:01:00 1 | 2000-01-01 00:01:30 NaN | 2000-01-01 00:02:00 2 | Freq: 30S, dtype: float64 | | Upsample the series into 30 second bins and fill the ``NaN`` | values using the ``pad`` method. | | &gt;&gt;&gt; series.resample(&#39;30S&#39;, fill_method=&#39;pad&#39;)[0:5] | 2000-01-01 00:00:00 0 | 2000-01-01 00:00:30 0 460 | 2000-01-01 00:01:00 1 | 2000-01-01 00:01:30 1 | 2000-01-01 00:02:00 2 | Freq: 30S, dtype: int64 | | Upsample the series into 30 second bins and fill the | ``NaN`` values using the ``bfill`` method. | | &gt;&gt;&gt; series.resample(&#39;30S&#39;, fill_method=&#39;bfill&#39;)[0:5] | 2000-01-01 00:00:00 0 | 2000-01-01 00:00:30 1 | 2000-01-01 00:01:00 1 | 2000-01-01 00:01:30 2 | 2000-01-01 00:02:00 2 | Freq: 30S, dtype: int64 | | Pass a custom function to ``how``. | | &gt;&gt;&gt; def custom_resampler(array_like): | ... return np.sum(array_like)+5 | | &gt;&gt;&gt; series.resample(&#39;3T&#39;, how=custom_resampler) | 2000-01-01 00:00:00 8 | 2000-01-01 00:03:00 17 | 2000-01-01 00:06:00 26 | Freq: 3T, dtype: int64 | | sample(self, n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) | Returns a random sample of items from an axis of object. | | .. versionadded:: 0.16.1 | | 【参数】 | ----------| n : int, optional | Number of items from axis to return. Cannot be used withfrac. | Default = 1 iffrac= None. | frac : float, optional | Fraction of axis items to return. Cannot be used withn. | replace : boolean, optional | Sample with or without replacement. Default = False. | weights : str or ndarray-like, optional | Default &#39;None&#39; results in equal probability weighting. | If passed a Series, will align with target object on index. Index | values in weights not found in sampled object will be ignored and | index values in sampled object not in weights will be assigned | weights of zero. | If called on a DataFrame, will accept the name of a column | when axis = 0. | Unless weights are a Series, weights must be same length as axis | being sampled. | If weights do not sum to 1, they will be normalized to sum to 1. | Missing values in the weights column will be treated as zero. | inf and -inf values not allowed. | random_state : int or numpy.random.RandomState, optional | Seed for the random number generator (if int), or numpy RandomState | object. 461 | axis : int or string, optional | Axis to sample. Accepts axis number or name. Default is stat axis | for given data type (0 for Series and DataFrames, 1 for Panels). | | 【返回值】 | -------| A new object of same type as caller. | | select(self, crit, axis=0) | Return data corresponding to axis labels matching criteria | | 【参数】 | ----------| crit : function | To be called on each index (label). Should return True or False | axis : int | | 【返回值】 | -------| selection : type of caller | | set_axis(self, axis, labels) | public verson of axis assignment | | slice_shift(self, periods=1, axis=0) | Equivalent toshiftwithout copying data. The shifted data will | not include the dropped periods and the shifted axis will be smaller | than the original. | | 【参数】 | ----------| periods : int | Number of periods to move, can be positive or negative | | 【注意】 | -----| While theslice_shiftis faster thanshift, you may pay for it | later during alignment. | | 【返回值】 | -------| shifted : same type as caller | | squeeze(self) | squeeze length 1 dimensions | | swapaxes(self, axis1, axis2, copy=True) | Interchange axes and swap values axes appropriately | | 【返回值】 | -------| y : same as input | | tail(self, n=5) | Returns last n rows | | to_clipboard(self, excel=None, sep=None, **kwargs) 462 | Attempt to write text representation of object to the system clipboard | This can be pasted into Excel, for example. | | 【参数】 | ----------| excel : boolean, defaults to True | if True, use the provided separator, writing in a csv | format for allowing easy pasting into excel. | if False, write a string representation of the object | to the clipboard | sep : optional, defaults to tab | other keywords are passed to to_csv | | 【注意】 | -----| Requirements for your platform | - Linux: xclip, or xsel (with gtk or PyQt4 modules) | - Windows: none | - OS X: none | | to_dense(self) | Return dense representation of NDFrame (as opposed to sparse) | | to_hdf(self, path_or_buf, key, **kwargs) | activate the HDFStore | | 【参数】 | ----------| path_or_buf : the path (string) or HDFStore object | key : string | indentifier for the group in the store | mode : optional, {&#39;a&#39;, &#39;w&#39;, &#39;r&#39;, &#39;r+&#39;}, default &#39;a&#39; | | ``&#39;r&#39;`` | Read-only; no data can be modified. | ``&#39;w&#39;`` | Write; a new file is created (an existing file with the same | name would be deleted). | ``&#39;a&#39;`` | Append; an existing file is opened for reading and writing, | and if the file does not exist it is created. | ``&#39;r+&#39;`` | It is similar to ``&#39;a&#39;``, but the file must already exist. | format : &#39;fixed(f)|table(t)&#39;, default is &#39;fixed&#39; | fixed(f) : Fixed format | Fast writing/reading. Not-appendable, nor searchable | table(t) : Table format | Write as a PyTables Table structure which may perform | worse but allow more flexible operations like searching | / selecting subsets of the data | append : boolean, default False | For Table formats, append the input data to the existing | complevel : int, 1-9, default 0 | If a complib is specified compression will be applied | where possible | complib : {&#39;zlib&#39;, &#39;bzip2&#39;, &#39;lzo&#39;, &#39;blosc&#39;, None}, default None | If complevel is &gt; 0 apply compression to objects written 463 | in the store wherever possible | fletcher32 : bool, default False | If applying compression use the fletcher32 checksum | dropna : boolean, default False. | If true, ALL nan rows will not be written to store. | | to_json(self, path_or_buf=None, orient=None, date_format=&#39;epoch&#39;, double_precision=10, force_ascii=True, date_unit=&#39;ms&#39;, default_handler=None) | Convert the object to a JSON string. | | Note NaN&#39;s and None will be converted to null and datetime objects | will be converted to UNIX timestamps. | | 【参数】 | ----------| path_or_buf : the path or buffer to write the result string | if this is None, return a StringIO of the converted string | orient : string | | * Series | | - default is &#39;index&#39; | - allowed values are: {&#39;split&#39;,&#39;records&#39;,&#39;index&#39;} | | * DataFrame | | - default is &#39;columns&#39; | - allowed values are: | {&#39;split&#39;,&#39;records&#39;,&#39;index&#39;,&#39;columns&#39;,&#39;values&#39;} | | * The format of the JSON string | | - split : dict like | {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]} | - records : list like | [{column -&gt; value}, ... , {column -&gt; value}] | - index : dict like {index -&gt; {column -&gt; value}} | - columns : dict like {column -&gt; {index -&gt; value}} | - values : just the values array | | date_format : {&#39;epoch&#39;, &#39;iso&#39;} | Type of date conversion.epoch= epoch milliseconds, |iso= ISO8601, default is epoch. | double_precision : The number of decimal places to use when encoding | floating point values, default 10. | force_ascii : force encoded string to be ASCII, default True. | date_unit : string, default &#39;ms&#39; (milliseconds) | The time unit to encode to, governs timestamp and ISO8601 | precision. One of &#39;s&#39;, &#39;ms&#39;, &#39;us&#39;, &#39;ns&#39; for second, millisecond, | microsecond, and nanosecond respectively. | default_handler : callable, default None | Handler to call if object cannot otherwise be converted to a | suitable format for JSON. Should receive a single argument which is | the object to convert and return a serialisable object. | | 【返回值】 | ------- 464 | same type as input object with filtered info axis | | to_msgpack(self, path_or_buf=None, **kwargs) | msgpack (serialize) object to input file path | | THIS IS AN EXPERIMENTAL LIBRARY and the storage format | may not be stable until a future release. | | 【参数】 | ----------| path : string File path, buffer-like, or None | if None, return generated string | append : boolean whether to append to an existing msgpack | (default is False) | compress : type of compressor (zlib or blosc), default to None (no | compression) | | to_pickle(self, path) | Pickle (serialize) object to input file path | | 【参数】 | ----------| path : string | File path | | to_sql(self, name, con, flavor=&#39;sqlite&#39;, schema=None, if_exists=&#39;fail&#39;, index=True, index_label=None, chunksize=None, dtype=None) | Write records stored in a DataFrame to a SQL database. | | 【参数】 | ----------| name : string | Name of SQL table | con : SQLAlchemy engine or DBAPI2 connection (legacy mode) | Using SQLAlchemy makes it possible to use any DB supported by that | library. | If a DBAPI2 object, only sqlite3 is supported. | flavor : {&#39;sqlite&#39;, &#39;mysql&#39;}, default &#39;sqlite&#39; | The flavor of SQL to use. Ignored when using SQLAlchemy engine. | &#39;mysql&#39; is deprecated and will be removed in future versions, but it | will be further supported through SQLAlchemy engines. | schema : string, default None | Specify the schema (if database flavor supports this). If None, use | default schema. | if_exists : {&#39;fail&#39;, &#39;replace&#39;, &#39;append&#39;}, default &#39;fail&#39; | - fail: If table exists, do nothing. | - replace: If table exists, drop it, recreate it, and insert data. | - append: If table exists, insert data. Create if does not exist. | index : boolean, default True | Write DataFrame index as a column. | index_label : string or sequence, default None | Column label for index column(s). If None is given (default) and | `index` is True, then the index names are used. | A sequence should be given if the DataFrame uses MultiIndex. | chunksize : int, default None | If not None, then rows will be written in batches of this size at a | time. If None, all rows will be written at once. 465 | dtype : dict of column name to SQL type, default None | Optional specifying the datatype for columns. The SQL type should | be a SQLAlchemy type, or a string for sqlite3 fallback connection. | | truncate(self, before=None, after=None, axis=None, copy=True) | Truncates a sorted NDFrame before and/or after some particular | dates. | | 【参数】 | ----------| before : date | Truncate before date | after : date | Truncate after date | axis : the truncation axis, defaults to the stat axis | copy : boolean, default is True, | return a copy of the truncated section | | 【返回值】 | -------| truncated : type of caller | | tshift(self, periods=1, freq=None, axis=0) | Shift the time index, using the index&#39;s frequency if available | | 【参数】 | ----------| periods : int | Number of periods to move, can be positive or negative | freq : DateOffset, timedelta, or time rule string, default None | Increment to use from datetools module or time rule (e.g. &#39;EOM&#39;) | axis : int or basestring | Corresponds to the axis that contains the Index | | 【注意】 | -----| If freq is not specified then tries to use the freq or inferred_freq | attributes of the index. If neither of those attributes exist, a | ValueError is thrown | | 【返回值】 | -------| shifted : NDFrame | | tz_convert(self, tz, axis=0, level=None, copy=True) | Convert tz-aware axis to target time zone. | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to convert | level : int, str, default None | If axis ia a MultiIndex, convert a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data | 466 | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError | If the axis is tz-naive. | | tz_localize(self, tz, axis=0, level=None, copy=True, ambiguous=&#39;raise&#39;) | Localize tz-naive TimeSeries to target time zone | | 【参数】 | ----------| tz : string or pytz.timezone object | axis : the axis to localize | level : int, str, default None | If axis ia a MultiIndex, localize a specific level. Otherwise | must be None | copy : boolean, default True | Also make a copy of the underlying data | ambiguous : &#39;infer&#39;, bool-ndarray, &#39;NaT&#39;, default &#39;raise&#39; | - &#39;infer&#39; will attempt to infer fall dst-transition hours based on order | - bool-ndarray where True signifies a DST time, False designates | a non-DST time (note that this flag is only applicable for ambiguous times) | - &#39;NaT&#39; will return NaT where there are ambiguous times | - &#39;raise&#39; will raise an AmbiguousTimeError if there are ambiguous times | infer_dst : boolean, default False (DEPRECATED) | Attempt to infer fall dst-transition hours based on order | | 【返回值】 | -------| | 【Raises 引发错误】 | ------| TypeError | If the TimeSeries is tz-aware and tz is not None. | | where(self, cond, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True) | Return an object of same shape as self and whose corresponding | entries are from self where cond is True and otherwise are from other. | | 【参数】 | ----------| cond : boolean NDFrame or array | other : scalar or NDFrame | inplace : boolean, default False | Whether to perform the operation in place on the data | axis : alignment axis if needed, default None | level : alignment level if needed, default None | try_cast : boolean, default False | try to cast the result back to the input type (if possible), | raise_on_error : boolean, default True | Whether to raise on invalid data types (e.g. trying to where on | strings) | | 【返回值】 | ------- 467 | wh : same type as caller | | xs(self, key, axis=0, level=None, copy=None, drop_level=True) | Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. | Defaults to cross-section on the rows (axis=0). | | 【参数】 | ----------| key : object | Some label contained in the index, or partially in a MultiIndex | axis : int, default 0 | Axis to retrieve cross-section on | level : object, defaults to first n levels (n=1 or len(key)) | In case of a key partially contained in a MultiIndex, indicate | which levels are used. Levels can be referred by label or position. | copy : boolean [deprecated] | Whether to make a copy of the data | drop_level : boolean, default True | If False, returns object with same levels as self. | | 【示例】 | --------| &gt;&gt;&gt; df | A B C | a 4 5 2 | b 4 0 9 | c 9 7 3 | &gt;&gt;&gt; df.xs(&#39;a&#39;) | A 4 | B 5 | C 2 | Name: a | &gt;&gt;&gt; df.xs(&#39;C&#39;, axis=1) | a 2 | b 9 | c 3 | Name: C | | &gt;&gt;&gt; df | A B C D | first second third | bar one 1 4 1 8 9 | two 1 7 5 5 0 | baz one 1 6 6 8 0 | three 2 5 3 5 3 | &gt;&gt;&gt; df.xs((&#39;baz&#39;, &#39;three&#39;)) | A B C D | third | 2 5 3 5 3 | &gt;&gt;&gt; df.xs(&#39;one&#39;, level=1) | A B C D | first third | bar 1 4 1 8 9 | baz 1 6 6 8 0 | &gt;&gt;&gt; df.xs((&#39;baz&#39;, 2), level=[0, &#39;third&#39;]) | A B C D | second 468 | three 5 3 5 3 | | 【返回值】 | -------| xs : Series or DataFrame | | 【注意】 | -----| xs is only for getting, not setting values. | | MultiIndex Slicers is a generic way to get/set values on any level or levels | it is a superset of xs functionality, see :ref:`MultiIndex Slicers &lt;advanced.mi_slicers&gt;` | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.generic.NDFrame: | | at | Fast label-based scalar accessor | | Similarly toloc,atprovides **label** based scalar lookups. | You can also set using these indexers. | | blocks | Internal property, property synonym for as_blocks() | | empty | True if NDFrame is entirely empty [no items] | | iat | Fast integer location scalar accessor. | | Similarly toiloc,iatprovides **integer** based lookups. | You can also set using these indexers. | | iloc | Purely integer-location based indexing for selection by position. | |.iloc[]is primarily integer position based (from0to |length-1of the axis), but may also be used with a boolean | array. | | Allowed inputs are: | | - An integer, e.g.5. | - A list or array of integers, e.g.[4, 3, 0]. | - A slice object with ints, e.g.1:7. | - A boolean array. | |.ilocwill raiseIndexErrorif a requested indexer is | out-of-bounds, except *slice* indexers which allow out-of-bounds | indexing (this conforms with python/numpy *slice* semantics). | | See more at :ref:`Selection by Position &lt;indexing.integer&gt;` | | ix | A primarily label-location based indexer, with integer position | fallback. 469 | |.ix[]supports mixed integer and label based access. It is | primarily label based, but will fall back to integer positional | access unless the corresponding axis is of integer type. | |.ixis the most general indexer and will support any of the | inputs in.locand.iloc..ixalso supports floating | point label schemes..ixis exceptionally useful when dealing | with mixed positional and label based hierachical indexes. | | However, when an axis is integer based, ONLY label based access | and not positional access is supported. Thus, in such cases, it&#39;s | usually better to be explicit and use.ilocor.loc. | | See more at :ref:`Advanced Indexing &lt;advanced&gt;`. | | loc | Purely label-location based indexer for selection by label. | |.loc[]is primarily label based, but may also be used with a | boolean array. | | Allowed inputs are: | | - A single label, e.g.5or‘a’, (note that5is | interpreted as a *label* of the index, and **never** as an | integer position along the index). | - A list or array of labels, e.g.[‘a’, ‘b’, ‘c’]. | - A slice object with labels, e.g.‘a’:’f’(note that contrary | to usual python slices, **both** the start and the stop are included!). | - A boolean array. | |.locwill raise aKeyErrorwhen the items are not found. | | See more at :ref:`Selection by Label &lt;indexing.label&gt;` | | ----------------------------------------------------------------------| Data and other attributes inherited from pandas.core.generic.NDFrame: | | is_copy = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | 470 | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. SparseArray SparseArray 模块所属：pandas.sparse.array: 类定义：SparseArray(pandas.core.base.PandasObject, numpy.ndarray) | Data structure for labeled, sparse floating point data | | 【参数】 | ----------| data : {array-like, Series, SparseSeries, dict} | kind : {&#39;block&#39;, &#39;integer&#39;} | fill_value : float | Defaults to NaN (code for missing) | sparse_index : {BlockIndex, IntIndex}, optional | Only if you have one. Mainly used internally | |【注意】 | -----| SparseArray objects are immutable via the typical Python means. If you | must change values, convert to dense, make your changes, then convert back | to sparse | | 【方法排序】 | SparseArray | pandas.core.base.PandasObject | pandas.core.base.StringMixin | numpy.ndarray | 【内置对象】 | | 【方法定义】 | | __add__ = add(self, other) | | __array_finalize__(self, obj) | Gets called after any ufunc or other array operations, necessary | to pass on the index. | | __div__ = truediv(self, other) | 471 | __floordiv__ = floordiv(self, other) | | __getitem__(self, key) | | __getslice__(self, i, j) | | __iadd__ = disable(self, other) | | __ifloordiv__ = disable(self, other) | | __imul__ = disable(self, other) | | __ipow__ = disable(self, other) | | __isub__ = disable(self, other) | | __iter__(self) | Implement iter(self). | | __itruediv__ = disable(self, other) | | __len__(self) | Return len(self). | | __mod__ = mod(self, other) | | __mul__ = mul(self, other) | | __pow__ = pow(self, other) | | __radd__ = radd(self, other) | | __rdiv__ = rtruediv(self, other) | | __reduce__(self) | Necessary for making this object picklable | | __rfloordiv__ = rfloordiv(self, other) | | __rmod__ = rmod(self, other) | | __rmul__ = rmul(self, other) | | __rpow__ = rpow(self, other) | | __rsub__ = rsub(self, other) | | __rtruediv__ = rtruediv(self, other) | | __setitem__(self, key, value) | Set self[key] to value. | | __setslice__(self, i, j, value) | | __setstate__(self, state) | Necessary for making this object picklable | 472 | __sub__ = sub(self, other) | | __truediv__ = truediv(self, other) | | __unicode__(self) | Return a string representation for a particular object. | | Invoked by unicode(obj) in py2 only. Yields a Unicode String in both | py2/py3. | | astype(self, dtype=None) | | copy(self, deep=True) | Make a copy of the SparseSeries. Only the actual sparse values need to | be copied | | count(self) | Compute sum of non-NA/null observations in SparseSeries. If the | fill_value is not NaN, the &quot;sparse&quot; locations will be included in the | observation count | | 【返回值】 | -------| nobs : int | | cumsum(self, axis=0, dtype=None, out=None) | Cumulative sum of values. Preserves locations of NaN values | | Extra parameters are to preserve ndarray interface. | | 【返回值】 | -------| cumsum : Series | | disable(self, other) | | get_values(self, fill=None) | return a dense representation | | mean(self, axis=None, dtype=None, out=None) | Mean of non-NA/null values | | 【返回值】 | -------| mean : float | | sum(self, axis=None, dtype=None, out=None) | Sum of non-NA/null values | | 【返回值】 | -------| sum : float | | take(self, indices, axis=0) | Sparse-compatible version of ndarray.take | | 【返回值】 473 | -------| taken : ndarray | | to_dense(self, fill=None) | Convert SparseSeries to (dense) Series | | ----------------------------------------------------------------------| Static methods defined here: | | __new__(cls, data, sparse_index=None, index=None, kind=&#39;integer&#39;, fill_value=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, copy=False) | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | kind | | sp_values | | values | Dense values | | ----------------------------------------------------------------------| 其他数据、属性定义： | | __array_priority__ = 15 | | fill_value = None | | sp_index = None | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.PandasObject: | | __dir__(self) | Provide method name lookup and completion | Only provide &#39;public&#39; methods | | ----------------------------------------------------------------------| Methods inherited from pandas.core.base.StringMixin: | | __bytes__(self) | Return a string representation for a particular object. | | Invoked by bytes(obj) in py3 only. | Yields a bytestring in both py2/py3. | | __repr__(self) | Return a string representation for a particular object. | | Yields Bytestring in Py2, Unicode String in py3. | | __str__(self) 474 | Return a string representation for a particular Object | | Invoked by str(df) in both py2/py3. | Yields Bytestring in Py2, Unicode String in py3. | | ----------------------------------------------------------------------| Data descriptors inherited from pandas.core.base.StringMixin: | | __weakref__ | list of weak references to the object (if defined) | | ----------------------------------------------------------------------| Methods inherited from numpy.ndarray: | | __abs__(self, /) | abs(self) | | __and__(self, value, /) | Return self&amp;value. | | __array__(...) | a.__array__(|dtype) -&gt; reference if type unchanged, copy otherwise. | | Returns either a new reference to self if dtype is not given or a new array | of provided data type if dtype is different from the current dtype of the | array. | | __array_prepare__(...) | a.__array_prepare__(obj) -&gt; Object of same type as ndarray object obj. | | __array_wrap__(...) | a.__array_wrap__(obj) -&gt; Object of same type as ndarray object a. | | __bool__(self, /) | self != 0 | | __contains__(self, key, /) | Return key in self. | | __copy__(...) | a.__copy__([order]) | | Return a copy of the array. | | 【参数】 | ----------| order : {&#39;C&#39;, &#39;F&#39;, &#39;A&#39;}, optional | If order is &#39;C&#39; (False) then the result is contiguous (default). | If order is &#39;Fortran&#39; (True) then the result has fortran order. | If order is &#39;Any&#39; (None) then the result has fortran order | only if the array already is in fortran order. | | __deepcopy__(...) | a.__deepcopy__() -&gt; Deep copy of array. | | Used if copy.deepcopy is called on an array. | 475 | __delitem__(self, key, /) | Delete self[key]. | | __divmod__(self, value, /) | Return divmod(self, value). | | __eq__(self, value, /) | Return self==value. | | __float__(self, /) | float(self) | | __ge__(self, value, /) | Return self&gt;=value. | | __gt__(self, value, /) | Return self&gt;value. | | __iand__(self, value, /) | Return self&amp;=value. | | __ilshift__(self, value, /) | Return self&lt;&lt;=value. | | __imod__(self, value, /) | Return self%=value. | | __index__(self, /) | Return self converted to an integer, if self is suitable for use as an index into a list. | | __int__(self, /) | int(self) | | __invert__(self, /) | ~self | | __ior__(self, value, /) | Return self|=value. | | __irshift__(self, value, /) | Return self&gt;&gt;=value. | | __ixor__(self, value, /) | Return self^=value. | | __le__(self, value, /) | Return self&lt;=value. | | __lshift__(self, value, /) | Return self&lt;&lt;value. | | __lt__(self, value, /) | Return self&lt;value. | | __ne__(self, value, /) | Return self!=value. | 476 | __neg__(self, /) | -self | | __or__(self, value, /) | Return self|value. | | __pos__(self, /) | +self | | __rand__(self, value, /) | Return value&amp;self. | | __rdivmod__(self, value, /) | Return divmod(value, self). | | __rlshift__(self, value, /) | Return value&lt;&lt;self. | | __ror__(self, value, /) | Return value|self. | | __rrshift__(self, value, /) | Return value&gt;&gt;self. | | __rshift__(self, value, /) | Return self&gt;&gt;value. | | __rxor__(self, value, /) | Return value^self. | | __xor__(self, value, /) | Return self^value. | | all(...) | a.all(axis=None, out=None) | | Returns True if all elements evaluate to True. | | Refer to `numpy.all` for full documentation. | | 【参见】 | --------| numpy.all : equivalent function | | any(...) | a.any(axis=None, out=None) | | Returns True if any of the elements of `a` evaluate to True. | | Refer to `numpy.any` for full documentation. | | 【参见】 | --------| numpy.any : equivalent function | | argmax(...) | a.argmax(axis=None, out=None) 477 | | Return indices of the maximum values along the given axis. | | Refer to `numpy.argmax` for full documentation. | | 【参见】 | --------| numpy.argmax : equivalent function | | argmin(...) | a.argmin(axis=None, out=None) | | Return indices of the minimum values along the given axis of `a`. | | Refer to `numpy.argmin` for detailed documentation. | | 【参见】 | --------| numpy.argmin : equivalent function | | argpartition(...) | a.argpartition(kth, axis=-1, kind=&#39;introselect&#39;, order=None) | | Returns the indices that would partition this array. | | Refer to `numpy.argpartition` for full documentation. | | .. versionadded:: 1.8.0 | | 【参见】 | --------| numpy.argpartition : equivalent function | | argsort(...) | a.argsort(axis=-1, kind=&#39;quicksort&#39;, order=None) | | Returns the indices that would sort this array. | | Refer to `numpy.argsort` for full documentation. | | 【参见】 | --------| numpy.argsort : equivalent function | | byteswap(...) | a.byteswap(inplace) | | Swap the bytes of the array elements | | Toggle between low-endian and big-endian data representation by | returning a byteswapped array, optionally swapped in-place. | | 【参数】 | ----------| inplace : bool, optional | IfTrue, swap bytes in-place, default isFalse. | 478 | 【返回值】 | -------| out : ndarray | The byteswapped array. If `inplace` isTrue, this is | a view to self. | | 【示例】 | --------| &gt;&gt;&gt; A = np.array([1, 256, 8755], dtype=np.int16) | &gt;&gt;&gt; map(hex, A) | [&#39;0x1&#39;, &#39;0x100&#39;, &#39;0x2233&#39;] | &gt;&gt;&gt; A.byteswap(True) | array([ 256, 1, 13090], dtype=int16) | &gt;&gt;&gt; map(hex, A) | [&#39;0x100&#39;, &#39;0x1&#39;, &#39;0x3322&#39;] | | Arrays of strings are not swapped | | &gt;&gt;&gt; A = np.array([&#39;ceg&#39;, &#39;fac&#39;]) | &gt;&gt;&gt; A.byteswap() | array([&#39;ceg&#39;, &#39;fac&#39;], | dtype=&#39;|S3&#39;) | | choose(...) | a.choose(choices, out=None, mode=&#39;raise&#39;) | | Use an index array to construct a new array from a set of choices. | | Refer to `numpy.choose` for full documentation. | | 【参见】 | --------| numpy.choose : equivalent function | | clip(...) | a.clip(a_min, a_max, out=None) | | Return an array whose values are limited to[a_min, a_max]. | | Refer to `numpy.clip` for full documentation. | | 【参见】 | --------| numpy.clip : equivalent function | | compress(...) | a.compress(condition, axis=None, out=None) | | Return selected slices of this array along given axis. | | Refer to `numpy.compress` for full documentation. | | 【参见】 | --------| numpy.compress : equivalent function | | conj(...) 479 | a.conj() | | Complex-conjugate all elements. | | Refer to `numpy.conjugate` for full documentation. | | 【参见】 | --------| numpy.conjugate : equivalent function | | conjugate(...) | a.conjugate() | | Return the complex conjugate, element-wise. | | Refer to `numpy.conjugate` for full documentation. | | 【参见】 | --------| numpy.conjugate : equivalent function | | cumprod(...) | a.cumprod(axis=None, dtype=None, out=None) | | Return the cumulative product of the elements along the given axis. | | Refer to `numpy.cumprod` for full documentation. | | 【参见】 | --------| numpy.cumprod : equivalent function | | diagonal(...) | a.diagonal(offset=0, axis1=0, axis2=1) | | Return specified diagonals. In NumPy 1.9 the returned array is a | read-only view instead of a copy as in previous NumPy versions. In | NumPy 1.10 the read-only restriction will be removed. | | Refer to :func:`numpy.diagonal` for full documentation. | | 【参见】 | --------| numpy.diagonal : equivalent function | | dot(...) | a.dot(b, out=None) | | Dot product of two arrays. | | Refer to `numpy.dot` for full documentation. | | 【参见】 | --------| numpy.dot : equivalent function | | 【示例】 480 | --------| &gt;&gt;&gt; a = np.eye(2) | &gt;&gt;&gt; b = np.ones((2, 2)) * 2 | &gt;&gt;&gt; a.dot(b) | array([[ 2., 2.], | [ 2., 2.]]) | | This array method can be conveniently chained: | | &gt;&gt;&gt; a.dot(b).dot(b) | array([[ 8., 8.], | [ 8., 8.]]) | | dump(...) | a.dump(file) | | Dump a pickle of the array to the specified file. | The array can be read back with pickle.load or numpy.load. | | 【参数】 | ----------| file : str | A string naming the dump file. | | dumps(...) | a.dumps() | | Returns the pickle of the array as a string. | pickle.loads or numpy.loads will convert the string back to an array. | | 【参数】 | ----------| None | | fill(...) | a.fill(value) | | Fill the array with a scalar value. | | 【参数】 | ----------| value : scalar | All elements of `a` will be assigned this value. | | 【示例】 | --------| &gt;&gt;&gt; a = np.array([1, 2]) | &gt;&gt;&gt; a.fill(0) | &gt;&gt;&gt; a | array([0, 0]) | &gt;&gt;&gt; a = np.empty(2) | &gt;&gt;&gt; a.fill(1) | &gt;&gt;&gt; a | array([ 1., 1.]) | | flatten(...) | a.flatten(order=&#39;C&#39;) 481 | | Return a copy of the array collapsed into one dimension. | | 【参数】 | ----------| order : {&#39;C&#39;, &#39;F&#39;, &#39;A&#39;}, optional | Whether to flatten in C (row-major), Fortran (column-major) order, | or preserve the C/Fortran ordering from `a`. | The default is &#39;C&#39;. | | 【返回值】 | -------| y : ndarray | A copy of the input array, flattened to one dimension. | | 【参见】 | --------| ravel : Return a flattened array. | flat : A 1-D flat iterator over the array. | | 【示例】 | --------| &gt;&gt;&gt; a = np.array([[1,2], [3,4]]) | &gt;&gt;&gt; a.flatten() | array([1, 2, 3, 4]) | &gt;&gt;&gt; a.flatten(&#39;F&#39;) | array([1, 3, 2, 4]) | | getfield(...) | a.getfield(dtype, offset=0) | | Returns a field of the given array as a certain type. | | A field is a view of the array data with a given data-type. The values in | the view are determined by the given type and the offset into the current | array in bytes. The offset needs to be such that the view dtype fits in the | array dtype; for example an array of dtype complex128 has 16-byte elements. | If taking a view with a 32-bit integer (4 bytes), the offset needs to be | between 0 and 12 bytes. | | 【参数】 | ----------| dtype : str or dtype | The data type of the view. The dtype size of the view can not be larger | than that of the array itself. | offset : int | Number of bytes to skip before beginning the element view. | | 【示例】 | --------| &gt;&gt;&gt; x = np.diag([1.+1.j]*2) | &gt;&gt;&gt; x[1, 1] = 2 + 4.j | &gt;&gt;&gt; x | array([[ 1.+1.j, 0.+0.j], | [ 0.+0.j, 2.+4.j]]) | &gt;&gt;&gt; x.getfield(np.float64) | array([[ 1., 0.], 482 | [ 0., 2.]]) | | By choosing an offset of 8 bytes we can select the complex part of the | array for our view: | | &gt;&gt;&gt; x.getfield(np.float64, offset=8) | array([[ 1., 0.], | [ 0., 4.]]) | | item(...) | a.item(*args) | | Copy an element of an array to a standard Python scalar and return it. | | 【参数】 | ----------| \\*args : Arguments (variable number and type) | | * none: in this case, the method only works for arrays | with one element (`a.size == 1`), which element is | copied into a standard Python scalar object and returned. | | * int_type: this argument is interpreted as a flat index into | the array, specifying which element to copy and return. | | * tuple of int_types: functions as does a single int_type argument, | except that the argument is interpreted as an nd-index into the | array. | | 【返回值】 | -------| z : Standard Python scalar object | A copy of the specified element of the array as a suitable | Python scalar | | 【注意】 | -----| When the data type of `a` is longdouble or clongdouble, item() returns | a scalar array object because there is no available Python scalar that | would not lose information. Void arrays return a buffer object for item(), | unless fields are defined, in which case a tuple is returned. | | `item` is very similar to a[args], except, instead of an array scalar, | a standard Python scalar is returned. This can be useful for speeding up | access to elements of the array and doing arithmetic on elements of the | array using Python&#39;s optimized math. | | 【示例】 | --------| &gt;&gt;&gt; x = np.random.randint(9, size=(3, 3)) | &gt;&gt;&gt; x | array([[3, 1, 7], | [2, 8, 3], | [8, 5, 3]]) | &gt;&gt;&gt; x.item(3) | 2 | &gt;&gt;&gt; x.item(7) 483 | 5 | &gt;&gt;&gt; x.item((0, 1)) | 1 | &gt;&gt;&gt; x.item((2, 2)) | 3 | | itemset(...) | a.itemset(*args) | | Insert scalar into an array (scalar is cast to array&#39;s dtype, if possible) | | There must be at least 1 argument, and define the last argument | as *item*. Then,a.itemset(args)is equivalent to but faster | thana[args] = item`. The item should be a scalar value andargs| must select a single item in the arraya`.|| 【参数】| ———-| \\args : Arguments| If one argument: a scalar, only used in case a is of size 1.| If two arguments: the last argument is the value to be set| and must be a scalar, the first argument specifies a single array| element location. It is either an int or a tuple.|| 【注意】| —–| Compared to indexing syntax, itemset provides some speed increase| for placing a scalar into a particular location in an ndarray,| if you must do this. However, generally this is discouraged:| among other problems, it complicates the appearance of the code.| Also, when using itemset (and item) inside a loop, be sure| to assign the methods to a local variable to avoid the attribute| look-up at each loop iteration.|| 【示例】| ——–| &gt;&gt;&gt; x = np.random.randint(9, size=(3, 3))| &gt;&gt;&gt; x| array([[3, 1, 7],| [2, 8, 3],| [8, 5, 3]])| &gt;&gt;&gt; x.itemset(4, 0)| &gt;&gt;&gt; x.itemset((2, 2), 9)| &gt;&gt;&gt; x| array([[3, 1, 7],| [2, 0, 3],| [8, 5, 9]])|| max(…)| a.max(axis=None, out=None)|| Return the maximum along a given axis.|| Refer to numpy.amax for full documentation.|| 【参见】| ——–484| numpy.amax : equivalent function|| min(…)| a.min(axis=None, out=None)|| Return the minimum along a given axis.|| Refer to numpy.amin for full documentation.|| 【参见】| ——–| numpy.amin : equivalent function|| newbyteorder(…)| arr.newbyteorder(new_order=’S’)|| Return the array with the same data viewed with a different byte order.|| Equivalent to::|| arr.view(arr.dtype.newbytorder(new_order))|| Changes are also made in all fields and sub-arrays of the array data| type.|||| 【参数】| ———-| new_order : string, optional| Byte order to force; a value from the byte order specifications| above. new_order codes can be any of::|| ‘S’ - swap dtype from current to opposite endian| {‘&lt;’, ‘L’} - little endian| {‘&gt;’, ‘B’} - big endian| {‘=’, ‘N’} - native order| {‘|’, ‘I’} - ignore (no change to byte order)|| The default value (‘S’) results in swapping the current| byte order. The code does a case-insensitive check on the first| letter of new_order for the alternatives above. For example,| any of ‘B’ or ‘b’ or ‘biggish’ are valid to specify big-endian.||| 【返回值】| ——-| new_arr : array| New array object with the dtype reflecting given change to the| byte order.|| nonzero(…)| a.nonzero()|| Return the indices of the elements that are non-zero.|| Refer to numpy.nonzero for full documentation.485|| 【参见】| ——–| numpy.nonzero : equivalent function|| partition(…)| a.partition(kth, axis=-1, kind=’introselect’, order=None)|| Rearranges the elements in the array in such a way that value of the| element in kth position is in the position it would be in a sorted array.| All elements smaller than the kth element are moved before this element and| all equal or greater are moved behind it. The ordering of the elements in| the two partitions is undefined.|| .. versionadded:: 1.8.0|| 【参数】| ———-| kth : int or sequence of ints| Element index to partition by. The kth element value will be in its| final sorted position and all smaller elements will be moved before it| and all equal or greater elements behind it.| The order all elements in the partitions is undefined.| If provided with a sequence of kth it will partition all elements| indexed by kth of them into their sorted position at once.| axis : int, optional| Axis along which to sort. Default is -1, which means sort along the| last axis.| kind : {‘introselect’}, optional| Selection algorithm. Default is ‘introselect’.| order : list, optional| When a is an array with fields defined, this argument specifies| which fields to compare first, second, etc. Not all fields need be| specified.|| 【参见】| ——–| numpy.partition : Return a parititioned copy of an array.| argpartition : Indirect partition.| sort : Full sort.|| 【注意】| —–| See np.partition for notes on the different algorithms.|| 【示例】| ——–| &gt;&gt;&gt; a = np.array([3, 4, 2, 1])| &gt;&gt;&gt; a.partition(a, 3)| &gt;&gt;&gt; a| array([2, 1, 3, 4])|| &gt;&gt;&gt; a.partition((1, 3))| array([1, 2, 3, 4])|| prod(…)| a.prod(axis=None, dtype=None, out=None)486|| Return the product of the array elements over the given axis|| Refer to numpy.prod for full documentation.|| 【参见】| ——–| numpy.prod : equivalent function|| ptp(…)| a.ptp(axis=None, out=None)|| Peak to peak (maximum - minimum) value along a given axis.|| Refer to numpy.ptp for full documentation.|| 【参见】| ——–| numpy.ptp : equivalent function|| put(…)| a.put(indices, values, mode=’raise’)|| Set a.flat[n] = values[n] for all n in indices.|| Refer to numpy.put for full documentation.|| 【参见】| ——–| numpy.put : equivalent function|| ravel(…)| a.ravel([order])|| Return a flattened array.|| Refer to numpy.ravel for full documentation.|| 【参见】| ——–| numpy.ravel : equivalent function|| ndarray.flat : a flat iterator on the array.|| repeat(…)| a.repeat(repeats, axis=None)|| Repeat elements of an array.|| Refer to numpy.repeat for full documentation.|| 【参见】| ——–| numpy.repeat : equivalent function|| reshape(…)| a.reshape(shape, order=’C’)487|| Returns an array containing the same data with a new shape.|| Refer to numpy.reshape for full documentation.|| 【参见】| ——–| numpy.reshape : equivalent function|| resize(…)| a.resize(new_shape, refcheck=True)|| Change shape and size of array in-place.|| 【参数】| ———-| new_shape : tuple of ints, or n ints| Shape of resized array.| refcheck : bool, optional| If False, reference count will not be checked. Default is True.|| 【返回值】| ——-| None|| 【Raises 引发错误】| ——| ValueError| If a does not own its own data or references or views to it exist,| and the data memory must be changed.|| SystemError| If the order keyword argument is specified. This behaviour is a| bug in NumPy.|| 【参见】| ——–| resize : Return a new array with the specified shape.|| 【注意】| —–| This reallocates space for the data area if necessary.|| Only contiguous arrays (data elements consecutive in memory) can be| resized.|| The purpose of the reference count check is to make sure you| do not use this array as a buffer for another Python object and then| reallocate the memory. However, reference counts can increase in| other ways so if you are sure that you have not shared the memory| for this array with another Python object, then you may safely set| refcheck to False.|| 【示例】| ——–| Shrinking an array: array is flattened (in the order that the data are| stored in memory), resized, and reshaped:488|| &gt;&gt;&gt; a = np.array([[0, 1], [2, 3]], order=’C’)| &gt;&gt;&gt; a.resize((2, 1))| &gt;&gt;&gt; a| array([[0],| [1]])|| &gt;&gt;&gt; a = np.array([[0, 1], [2, 3]], order=’F’)| &gt;&gt;&gt; a.resize((2, 1))| &gt;&gt;&gt; a| array([[0],| [2]])|| Enlarging an array: as above, but missing entries are filled with zeros:|| &gt;&gt;&gt; b = np.array([[0, 1], [2, 3]])| &gt;&gt;&gt; b.resize(2, 3) # new_shape parameter doesn’t have to be a tuple| &gt;&gt;&gt; b| array([[0, 1, 2],| [3, 0, 0]])|| Referencing an array prevents resizing…|| &gt;&gt;&gt; c = a| &gt;&gt;&gt; a.resize((1, 1))| Traceback (most recent call last):| …| ValueError: cannot resize an array that has been referenced …|| Unless refcheck is False:|| &gt;&gt;&gt; a.resize((1, 1), refcheck=False)| &gt;&gt;&gt; a| array([[0]])| &gt;&gt;&gt; c| array([[0]])|| round(…)| a.round(decimals=0, out=None)|| Return a with each element rounded to the given number of decimals.|| Refer to numpy.around for full documentation.|| 【参见】| ——–| numpy.around : equivalent function|| searchsorted(…)| a.searchsorted(v, side=’left’, sorter=None)|| Find indices where elements of v should be inserted in a to maintain order.|| For full documentation, see numpy.searchsorted|| 【参见】| ——–489| numpy.searchsorted : equivalent function|| setfield(…)| a.setfield(val, dtype, offset=0)|| Put a value into a specified place in a field defined by a data-type.|| Place val into a‘s field defined by dtype and beginning offset| bytes into the field.|| 【参数】| ———-| val : object| Value to be placed in field.| dtype : dtype object| Data-type of the field in which to place val.| offset : int, optional| The number of bytes into the field at which to place val.|| 【返回值】| ——-| None|| 【参见】| ——–| getfield|| 【示例】| ——–| &gt;&gt;&gt; x = np.eye(3)| &gt;&gt;&gt; x.getfield(np.float64)| array([[ 1., 0., 0.],| [ 0., 1., 0.],| [ 0., 0., 1.]])| &gt;&gt;&gt; x.setfield(3, np.int32)| &gt;&gt;&gt; x.getfield(np.int32)| array([[3, 3, 3],| [3, 3, 3],| [3, 3, 3]])| &gt;&gt;&gt; x| array([[ 1.00000000e+000, 1.48219694e-323, 1.48219694e-323],| [ 1.48219694e-323, 1.00000000e+000, 1.48219694e-323],| [ 1.48219694e-323, 1.48219694e-323, 1.00000000e+000]])| &gt;&gt;&gt; x.setfield(np.eye(3), np.int32)| &gt;&gt;&gt; x| array([[ 1., 0., 0.],| [ 0., 1., 0.],| [ 0., 0., 1.]])|| setflags(…)| a.setflags(write=None, align=None, uic=None)|| Set array flags WRITEABLE, ALIGNED, and UPDATEIFCOPY, respectively.|| These Boolean-valued flags affect how numpy interprets the memory| area used by a (see Notes below). The ALIGNED flag can only| be set to True if the data is actually aligned according to the type.490| The UPDATEIFCOPY flag can never be set to True. The flag WRITEABLE| can only be set to True if the array owns its own memory, or the| ultimate owner of the memory exposes a writeable buffer interface,| or is a string. (The exception for string is made so that unpickling| can be done without copying memory.)|| 【参数】| ———-| write : bool, optional| Describes whether or not a can be written to.| align : bool, optional| Describes whether or not a is aligned properly for its type.| uic : bool, optional| Describes whether or not a is a copy of another “base” array.|| 【注意】| —–| Array flags provide information about how the memory area used| for the array is to be interpreted. There are 6 Boolean flags| in use, only three of which can be changed by the user:| UPDATEIFCOPY, WRITEABLE, and ALIGNED.|| WRITEABLE (W) the data area can be written to;|| ALIGNED (A) the data and strides are aligned appropriately for the hardware| (as determined by the compiler);|| UPDATEIFCOPY (U) this array is a copy of some other array (referenced| by .base). When this array is deallocated, the base array will be| updated with the contents of this array.|| All flags can be accessed using their first (upper case) letter as well| as the full name.|| 【示例】| ——–| &gt;&gt;&gt; y| array([[3, 1, 7],| [2, 0, 0],| [8, 5, 9]])| &gt;&gt;&gt; y.flags| C_CONTIGUOUS : True| F_CONTIGUOUS : False| OWNDATA : True| WRITEABLE : True| ALIGNED : True| UPDATEIFCOPY : False| &gt;&gt;&gt; y.setflags(write=0, align=0)| &gt;&gt;&gt; y.flags| C_CONTIGUOUS : True| F_CONTIGUOUS : False| OWNDATA : True| WRITEABLE : False| ALIGNED : False| UPDATEIFCOPY : False| &gt;&gt;&gt; y.setflags(uic=1)| Traceback (most recent call last):491| File ““, line 1, in | ValueError: cannot set UPDATEIFCOPY flag to True|| sort(…)| a.sort(axis=-1, kind=’quicksort’, order=None)|| Sort an array, in-place.|| 【参数】| ———-| axis : int, optional| Axis along which to sort. Default is -1, which means sort along the| last axis.| kind : {‘quicksort’, ‘mergesort’, ‘heapsort’}, optional| Sorting algorithm. Default is ‘quicksort’.| order : list, optional| When a is an array with fields defined, this argument specifies| which fields to compare first, second, etc. Not all fields need be| specified.|| 【参见】| ——–| numpy.sort : Return a sorted copy of an array.| argsort : Indirect sort.| lexsort : Indirect stable sort on multiple keys.| searchsorted : Find elements in sorted array.| partition: Partial sort.|| 【注意】| —–| See sort for notes on the different sorting algorithms.|| 【示例】| ——–| &gt;&gt;&gt; a = np.array([[1,4], [3,1]])| &gt;&gt;&gt; a.sort(axis=1)| &gt;&gt;&gt; a| array([[1, 4],| [1, 3]])| &gt;&gt;&gt; a.sort(axis=0)| &gt;&gt;&gt; a| array([[1, 3],| [1, 4]])|| Use the order keyword to specify a field to use when sorting a| structured array:|| &gt;&gt;&gt; a = np.array([(‘a’, 2), (‘c’, 1)], dtype=[(‘x’, ‘S1’), (‘y’, int)])| &gt;&gt;&gt; a.sort(order=’y’)| &gt;&gt;&gt; a| array([(‘c’, 1), (‘a’, 2)],| dtype=[(‘x’, ‘|S1’), (‘y’, ‘&gt;&gt; x = np.array([[0, 1], [2, 3]])| &gt;&gt;&gt; x.tobytes()493| b’\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00’| &gt;&gt;&gt; x.tobytes(‘C’) == x.tobytes()| True| &gt;&gt;&gt; x.tobytes(‘F’)| b’\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x03\\x00\\x00\\x00’|| tofile(…)| a.tofile(fid, sep=””, format=”%s”)|| Write array to a file as text or binary (default).|| Data is always written in ‘C’ order, independent of the order of a.| The data produced by this method can be recovered using the function| fromfile().|| 【参数】| ———-| fid : file or str| An open file object, or a string containing a filename.| sep : str| Separator between array items for text output.| If “” (empty), a binary file is written, equivalent to| file.write(a.tobytes()).| format : str| Format string for text file output.| Each entry in the array is formatted to text by first converting| it to the closest Python type, and then using “format” % item.|| 【注意】| —–| This is a convenience function for quick storage of array data.| Information on endianness and precision is lost, so this method is not a| good choice for files intended to archive data or transport data between| machines with different endianness. Some of these problems can be overcome| by outputting the data as text files, at the expense of speed and file| size.|| tolist(…)| a.tolist()|| Return the array as a (possibly nested) list.|| Return a copy of the array data as a (nested) Python list.| Data items are converted to the nearest compatible Python type.|| 【参数】| ———-| none|| 【返回值】| ——-| y : list| The possibly nested list of array elements.|| 【注意】| —–| The array may be recreated, a = np.array(a.tolist()).494|| 【示例】| ——–| &gt;&gt;&gt; a = np.array([1, 2])| &gt;&gt;&gt; a.tolist()| [1, 2]| &gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])| &gt;&gt;&gt; list(a)| [array([1, 2]), array([3, 4])]| &gt;&gt;&gt; a.tolist()| [[1, 2], [3, 4]]|| tostring(…)| a.tostring(order=’C’)|| Construct Python bytes containing the raw data bytes in the array.|| Constructs Python bytes showing a copy of the raw contents of| data memory. The bytes object can be produced in either ‘C’ or ‘Fortran’,| or ‘Any’ order (the default is ‘C’-order). ‘Any’ order means C-order| unless the F_CONTIGUOUS flag in the array is set, in which case it| means ‘Fortran’ order.|| This function is a compatibility alias for tobytes. Despite its name it returns bytes not strings.|| 【参数】| ———-| order : {‘C’, ‘F’, None}, optional| Order of the data for multidimensional arrays:| C, Fortran, or the same as for the original array.|| 【返回值】| ——-| s : bytes| Python bytes exhibiting a copy of a‘s raw data.|| 【示例】| ——–| &gt;&gt;&gt; x = np.array([[0, 1], [2, 3]])| &gt;&gt;&gt; x.tobytes()| b’\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00’| &gt;&gt;&gt; x.tobytes(‘C’) == x.tobytes()| True| &gt;&gt;&gt; x.tobytes(‘F’)| b’\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x03\\x00\\x00\\x00’|| trace(…)| a.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)|| Return the sum along diagonals of the array.|| Refer to numpy.trace for full documentation.|| 【参见】| ——–| numpy.trace : equivalent function|495| transpose(…)| a.transpose( get_option = options = plot_params = {'xaxis.compat': False} reset_option = set_option = 【版本】 0.17.1 【文件】 ： \\pandas\\__init__.py parser 所属模块：pandas.parser in pandas: 【名称】 pandas.parser 【类型】 builtins.Exception(builtins.BaseException) CParserError builtins.ValueError(builtins.Exception) OverflowError 【内置对象】 TextReader class CParserError(builtins.Exception) | Common base class for all non-exit exceptions. | 1166 | 【方法排序】 | CParserError | builtins.Exception | builtins.BaseException | 【内置对象】 | | Data descriptors defined here: | | __weakref__ | list of weak references to the object (if defined) | | ----------------------------------------------------------------------| Methods inherited from builtins.Exception: | | __init__(self, /, *args, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Methods inherited from builtins.BaseException: | | __delattr__(self, name, /) | Implement delattr(self, name). | | __getattribute__(self, name, /) | Return getattr(self, name). | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | __setattr__(self, name, value, /) | Implement setattr(self, name, value). | | __setstate__(...) | | __str__(self, /) | Return str(self). | | with_traceback(...) | Exception.with_traceback(tb) --| set self.__traceback__ to tb and return self. | | ----------------------------------------------------------------------| Data descriptors inherited from builtins.BaseException: | | __cause__ | exception cause | | __context__ | exception context | | __dict__ 1167 | | __suppress_context__ | | __traceback__ | | args class OverflowError(builtins.ValueError) | Inappropriate argument value (of correct type). | | 【方法排序】 | OverflowError | builtins.ValueError | builtins.Exception | builtins.BaseException | 【内置对象】 | | Data descriptors defined here: | | __weakref__ | list of weak references to the object (if defined) | | ----------------------------------------------------------------------| Methods inherited from builtins.ValueError: | | __init__(self, /, *args, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | ----------------------------------------------------------------------| Methods inherited from builtins.BaseException: | | __delattr__(self, name, /) | Implement delattr(self, name). | | __getattribute__(self, name, /) | Return getattr(self, name). | | __reduce__(...) | helper for pickle | | __repr__(self, /) | Return repr(self). | | __setattr__(self, name, value, /) | Implement setattr(self, name, value). | | __setstate__(...) | | __str__(self, /) | Return str(self). | | with_traceback(...) | Exception.with_traceback(tb) --| set self.__traceback__ to tb and return self. 1168 | | ----------------------------------------------------------------------| Data descriptors inherited from builtins.BaseException: | | __cause__ | exception cause | | __context__ | exception context | | __dict__ | | __suppress_context__ | | __traceback__ | | args class TextReader(builtins.object) | # source: StringIO or file object | | 【方法定义】 | | __init__(self, /, *args, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | debug_print(...) | | read(...) | rows=None --> read all rows | | remove_noconvert(...) | | set_error_bad_lines(...) | | set_noconvert(...) | | ----------------------------------------------------------------------| Data descriptors defined here: | | allow_leading_cols | | as_recarray | | buffer_lines | | compact_ints | | compression | | converters | | delim_whitespace | 1169 | delimiter | | dtype | | encoding | | header | | header_end | | header_start | | index_col | | leading_cols | | low_memory | | mangle_dupe_cols | | memory_map | | na_values | | names | | noconvert | | orig_header | | skip_footer | | skiprows | | table_width | | tupleize_cols | | use_unsigned | | usecols | | ----------------------------------------------------------------------| 其他数据、属性定义： | | __pyx_vtable__ = 【函数】 downcast_int64(...) 【数据】 DEFAULT_CHUNKSIZE = 262144 __test__ = {} na_values = {: -9223372036854775808, >> df A B C D 0 foo one small 1 1 foo one large 2 2 foo one large 2 3 foo two small 3 4 foo two small 3 1172 5 bar one large 4 6 bar one small 5 7 bar two small 6 8 bar two large 7 table = pivot_table(df, values='D', index=['A', 'B'], ... columns=['C'], aggfunc=np.sum) table small large foo one 1 4 two 6 NaN bar one 5 4 two 6 7 【返回值】 -------table : DataFrame plot_params _Options 模块所属：pandas.tools.plotting object: 类定义：_Options(builtins.dict) | Stores pandas plotting options. | Allows for parameter aliasing so you can just use parameter names that are | the same as the plot function parameters, but is stored in a canonical | format that makes it easy to breakdown into groups later | | 【方法排序】 | _Options | builtins.dict | 【内置对象】 | | 【方法定义】 | | __contains__(self, key) | True if D has a key k, else False. | | __delitem__(self, key) | Delete self[key]. | | __getitem__(self, key) | x.__getitem__(y) x[y] | | __init__(self) | Initialize self. See help(type(self)) for accurate signature. | | __setitem__(self, key, value) | Set self[key] to value. | 1173 | reset(self) | Reset the option store to its initial state | | 【返回值】 | -------| None | | use(self, key, value) | Temporarily set a parameter value using the with statement. | Aliasing allowed. | | ----------------------------------------------------------------------| Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | ----------------------------------------------------------------------| Methods inherited from builtins.dict: | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self>=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __gt__(self, value, /) | Return self>value. | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self None. Remove all items from D. | | copy(...) | D.copy() -> a shallow copy of D | | fromkeys(iterable, value=None, /) from builtins.type | Returns a new dict with keys from iterable and values equal to value. | | get(...) | D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. | | items(...) | D.items() -> a set-like object providing a view on D's items | | keys(...) | D.keys() -> a set-like object providing a view on D’s keys|| pop(…)| D.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.| If key is not found, d is returned if given, otherwise KeyError is raised|| popitem(…)| D.popitem() -&gt; (k, v), remove and return some (key, value) pair as a| 2-tuple; but raise KeyError if D is empty.|| setdefault(…)| D.setdefault(k[,d]) -&gt; D.get(k,d), also set D[k]=d if k not in D|| update(…)| D.update([E, ]F) -&gt; None. Update D from dict/iterable E and F.| If E is present and has a .keys() method, then does: for k in E: D[k] = E[k]| If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v| In either case, this is followed by: for k in F: D[k] = F[k]|| values(…)| D.values() -&gt; an object providing a view on D’s values|| ———————————————————————-| Data and other attributes inherited from builtins.dict:|| hash = Nonepnow函数 pnow 模块所属：pandas.tseries.period:pnow(freq=None)1175qcut函数 qcut 模块所属：pandas.tools.tile:qcut(x, q, labels=None, retbins=False, precision=3)Quantile-based discretization function. Discretize variable intoequal-sized buckets based on rank or based on sample quantiles. For example1000 values for 10 quantiles would produce a Categorical object indicatingquantile membership for each data point.【参数】———-x : ndarray or Seriesq : integer or array of quantilesNumber of quantiles. 10 for deciles, 4 for quartiles, etc. Alternatelyarray of quantiles, e.g. [0, .25, .5, .75, 1.] for quartileslabels : array or boolean, default NoneUsed as labels for the resulting bins. Must be of the same length as the resultingbins. If False, return only integer indicators of the bins.retbins : bool, optionalWhether to return the bins or not. Can be useful if bins is givenas a scalar.precision : intThe precision at which to store and display the bins labels【返回值】——-out : Categorical or Series or array of integers if labels is FalseThe return type (Categorical or Series) depends on the input: a Series of type category ifinput is a Series else Categorical. Bins are represented as categories when categoricaldata is returned.bins : ndarray of floatsReturned only if retbins is True.【注意】—–Out of bounds values will be NA in the resulting Categorical object【示例】——–&gt;&gt;&gt; pd.qcut(range(5), 4)[[0, 1], [0, 1], (1, 2], (2, 3], (3, 4]]Categories (4, object): [[0, 1] &lt; (1, 2] &lt; (2, 3] &lt; (3, 4]]pd.qcut(range(5), 3, labels=[“good”,”medium”,”bad”])[good, good, medium, bad, bad]Categories (3, object): [good &lt; medium &lt; bad]pd.qcut(range(5), 4, labels=False)array([0, 0, 1, 2, 3], dtype=int64)1176read_clipboard函数 read_clipboard 模块所属：pandas.io.clipboard:read_clipboard(kwargs)Read text from clipboard and pass to read_table. See read_table for thefull argument listIf unspecified, sep defaults to ‘\\s+’【返回值】——-parsed : DataFrameread_csv函数 read_csv 模块所属：pandas.io.parsers:read_csv(filepath_or_buffer, sep=’,’, dialect=None, compression=’infer’, doublequote=True, escapechar=None, quotechar=’”‘,quoting=0, skipinitialspace=False, lineterminator=None, header=’infer’, index_col=None, names=None, prefix=None,skiprows=None, skipfooter=None, skip_footer=0, na_values=None, true_values=None, false_values=None, delimiter=None,converters=None, dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False, na_filter=True,compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, warn_bad_lines=True, error_bad_lines=True,keep_default_na=True, thousands=None, comment=None, decimal=b’.’, parse_dates=False, keep_date_col=False,dayfirst=False, date_parser=None, memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False, infer_datetime_format=False,skip_blanklines=True)Read CSV (comma-separated) file into DataFrameAlso supports optionally iterating or breaking of the fileinto chunks.Additional help can be found in the online docs for IO Tools &lt;http://pandas.pydata.org/pandas-docs/stable/io.html&gt;.【参数】———-filepath_orbuffer : string or file handle / StringIOThe string could be a URL. Valid URL schemes includehttp, ftp, s3, and file. For file URLs, ahost is expected. For instance, a local file could befile ://localhost/path/to/table.csvsep : string, default ‘,’Delimiter to use. If sep is None, will try to automatically determine1177this. Regular expressions are accepted.engine : {‘c’, ‘python’}Parser engine to use. The C engine is faster while the python engine iscurrently more feature-complete.lineterminator : string (length 1), default NoneCharacter to break file into lines. Only valid with C parserquotechar : string (length 1)The character used to denote the start and end of a quoted item. Quoteditems can include the delimiter and it will be ignored.quoting : int or csv.QUOTE instance, default NoneControl field quoting behavior per ``csv.QUOTE_constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3). Default (None) results in QUOTE_MINIMAL behavior. skipinitialspace : boolean, default False Skip spaces after delimiter escapechar : string (length 1), default None One-character string used to escape delimiter when quoting is QUOTE_NONE. dtype : Type name or dict of column -&gt; type, default None Data type for data or columns. E.g. {&#39;a&#39;: np.float64, &#39;b&#39;: np.int32} (Unsupported with engine=&#39;python&#39;) compression : {&#39;gzip&#39;, &#39;bz2&#39;, &#39;infer&#39;, None}, default &#39;infer&#39; For on-the-fly decompression of on-disk data. If &#39;infer&#39;, then use gzip or bz2 if filepath_or_buffer is a string ending in &#39;.gz&#39; or &#39;.bz2&#39;, respectively, and no decompression otherwise. Set to None for no decompression. dialect : string or csv.Dialect instance, default None If None defaults to Excel dialect. Ignored if sep longer than 1 char See csv.Dialect documentation for more details header : int, list of ints, default &#39;infer&#39; Row number(s) to use as the column names, and the start of the data. Defaults to 0 if nonamespassed, otherwiseNone. Explicitly passheader=0to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns E.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example are skipped). Note that this parameter ignores commented lines and empty lines ifskip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file. skiprows : list-like or integer, default None Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file index_col : int or sequence or False, default None Column to use as the row labels of the DataFrame. If a sequence is given, a MultiIndex is used. If you have a malformed file with delimiters at the end of each line, you might consider index_col=False to force pandas to _not_ use the first column as the index (row names) names : array-like, default None List of column names to use. If file contains no header row, then you should explicitly pass header=None prefix : string, default None Prefix to add to column numbers when no header, e.g &#39;X&#39; for X0, X1, ... na_values : str, list-like or dict, default None Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values true_values : list, default None Values to consider as True false_values : list, default None Values to consider as False 1178 keep_default_na : bool, default True If na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they&#39;re appended to parse_dates : boolean, list of ints or names, list of lists, or dict, default False If True -&gt; try parsing the index. If [1, 2, 3] -&gt; try parsing columns 1, 2, 3 each as a separate date column. If [[1, 3]] -&gt; combine columns 1 and 3 and parse as a single date column. {&#39;foo&#39; : [1, 3]} -&gt; parse columns 1, 3 as date and call result &#39;foo&#39; A fast-path exists for iso8601-formatted dates. keep_date_col : boolean, default False If True and parse_dates specifies combining multiple columns then keep the original columns. date_parser : function, default None Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments. dayfirst : boolean, default False DD/MM format dates, international and European format thousands : str, default None Thousands separator comment : str, default None Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long asskip_blank_lines=True), fully commented lines are ignored by the parameter `header` but not by `skiprows`. For example, if comment=&#39;#&#39;, parsing &#39;#empty\\na,b,c\\n1,2,3&#39; with `header=0` will result in &#39;a,b,c&#39; being treated as the header. decimal : str, default &#39;.&#39; Character to recognize as decimal point. E.g. use &#39;,&#39; for European data nrows : int, default None Number of rows of file to read. Useful for reading pieces of large files iterator : boolean, default False Return TextFileReader object for iteration or getting chunks withgetchunk()`. chunksize : int, default None Return TextFileReader object for iteration.See IO Tools docs for moreinformationhttp://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking` oniterator and chunksize.skipfooter : int, default 0Number of lines at bottom of file to skip (Unsupported with engine=’c’)converters : dict, default NoneDict of functions for converting values in certain columns. Keys can eitherbe integers or column labelsverbose : boolean, default FalseIndicate number of NA values placed in non-numeric columnsdelimiter : string, default NoneAlternative argument name for sep. Regular expressions are accepted.encoding : string, default NoneEncoding to use for UTF when reading/writing (ex. ‘utf-8’). List of Python standard encodings &lt;https://docs.python.org/3/library/codecs.html#standard-encodings&gt;_1179squeeze : boolean, default FalseIf the parsed data only contains one column then return a Seriesna_filter : boolean, default TrueDetect missing value markers (empty strings and the value of na_values). Indata without any NAs, passing na_filter=False can improve the performanceof reading a large fileusecols : array-like, default NoneReturn a subset of the columns.Results in much faster parsing time and lower memory usage.mangle_dupe_cols : boolean, default TrueDuplicate columns will be specified as ‘X.0’…’X.N’, rather than ‘X’…’X’tupleize_cols : boolean, default FalseLeave a list of tuples on columns as is (default is to convert toa Multi Index on the columns)error_bad_lines : boolean, default TrueLines with too many fields (e.g. a csv line with too many commas) will bydefault cause an exception to be raised, and no DataFrame will be returned.If False, then these “bad lines” will dropped from the DataFrame that isreturned. (Only valid with C parser)warn_bad_lines : boolean, default TrueIf error_bad_lines is False, and warn_bad_lines is True, a warning for each“bad line” will be output. (Only valid with C parser).infer_datetime_format : boolean, default FalseIf True and parse_dates is enabled for a column, attempt to inferthe datetime format to speed up the processingskip_blank_lines : boolean, default TrueIf True, skip over blank lines rather than interpreting as NaN values【返回值】——-result : DataFrame or TextParserread_excel函数 read_excel 模块所属：pandas.io.excel:read_excel(io, sheetname=0, header=0, skiprows=None, skip_footer=0, index_col=None, parse_cols=None, parse_dates=False,date_parser=None, na_values=None, thousands=None, convert_float=True, has_index_names=None, converters=None,engine=None, **kwds)Read an Excel table into a pandas DataFrame【参数】———-io : string, file-like object, pandas ExcelFile, or xlrd workbook.The string could be a URL. Valid URL schemes include http, ftp, s3,and file. For file URLs, a host is expected. For instance, a localfile could be file://localhost/path/to/workbook.xlsxsheetname : string, int, mixed list of strings/ints, or None, default 0Strings are used for sheet names, Integers are used in zero-indexed sheet1180positions.Lists of strings/integers are used to request multiple sheets.Specify None to get all sheets.str|int -&gt; DataFrame is returned.list|None -&gt; Dict of DataFrames is returned, with keys representing sheets.Available Cases Defaults to 0 -&gt; 1st sheet as a DataFrame 1 -&gt; 2nd sheet as a DataFrame “Sheet1” -&gt; 1st sheet as a DataFrame [0,1,”Sheet5”] -&gt; 1st, 2nd &amp; 5th sheet as a dictionary of DataFrames None -&gt; All sheets as a dictionary of DataFramesheader : int, list of ints, default 0Row (0-indexed) to use for the column labels of the parsedDataFrame. If a list of integers is passed those row positions willbe combined into a MultiIndexskiprows : list-likeRows to skip at the beginning (0-indexed)skip_footer : int, default 0Rows at the end to skip (0-indexed)index_col : int, list of ints, default NoneColumn (0-indexed) to use as the row labels of the DataFrame.Pass None if there is no such column. If a list is passed,those columns will be combined into a MultiIndexconverters : dict, default NoneDict of functions for converting values in certain columns. Keys caneither be integers or column labels, values are functions that take oneinput argument, the Excel cell content, and return the transformedcontent.parse_cols : int or list, default None If None then parse all columns, If int then indicates last column to be parsed If list of ints then indicates list of column numbers to be parsed If string then indicates comma separated list of column names andcolumn ranges (e.g. “A:E” or “A,C,E:F”)na_values : list-like, default NoneList of additional strings to recognize as NA/NaNthousands : str, default NoneThousands separator for parsing string columns to numeric. Note thatthis parameter is only necessary for columns stored as TEXT in Excel,any numeric columns will automatically be parsed, regardless of displayformat.keep_default_na : bool, default TrueIf na_values are specified and keep_default_na is False the default NaNvalues are overridden, otherwise they’re appended toverbose : boolean, default FalseIndicate number of NA values placed in non-numeric columnsengine: string, default NoneIf io is not a buffer or path, this must be set to identify io.Acceptable values are None or xlrdconvert_float : boolean, default Trueconvert integral floats to int (i.e., 1.0 –&gt; 1). If False, all numeric1181data will be read in as floats: Excel stores all numbers as floatsinternallyhas_index_names : boolean, default NoneDEPRECATED: for version 0.17+ index names will be automatically inferredbased on index_col. To read Excel output from 0.16.2 and prior thathad saved index names, use True.【返回值】——-parsed : DataFrame or Dict of DataFramesDataFrame from the passed in Excel file. See notes in sheetname argumentfor more information on when a Dict of Dataframes is returned.read_fwf函数 read_fwf 模块所属：pandas.io.parsers:read_fwf(filepath_orbuffer, colspecs=’infer’, widths=None, **kwds)Read a table of fixed-width formatted lines into DataFrameAlso supports optionally iterating or breaking of the fileinto chunks.Additional help can be found in the online docs for IO Tools &lt;http://pandas.pydata.org/pandas-docs/stable/io.html&gt;.【参数】———-filepath_orbuffer : string or file handle / StringIOThe string could be a URL. Valid URL schemes includehttp, ftp, s3, and file. For file URLs, ahost is expected. For instance, a local file could befile ://localhost/path/to/table.csvcolspecs : list of pairs (int, int) or ‘infer’. optionalA list of pairs (tuples) giving the extents of the fixed-widthfields of each line as half-open intervals (i.e., [from, to[ ).String value ‘infer’ can be used to instruct the parser to trydetecting the column specifications from the first 100 rows ofthe data (default=’infer’).widths : list of ints. optionalA list of field widths which can be used instead of ‘colspecs’ ifthe intervals are contiguous.lineterminator : string (length 1), default NoneCharacter to break file into lines. Only valid with C parserquotechar : string (length 1)The character used to denote the start and end of a quoted item. Quoteditems can include the delimiter and it will be ignored.quoting : int or csv.QUOTE instance, default NoneControl field quoting behavior per ``csv.QUOTE_constants. Use one of 1182 QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3). Default (None) results in QUOTE_MINIMAL behavior. skipinitialspace : boolean, default False Skip spaces after delimiter escapechar : string (length 1), default None One-character string used to escape delimiter when quoting is QUOTE_NONE. dtype : Type name or dict of column -&gt; type, default None Data type for data or columns. E.g. {&#39;a&#39;: np.float64, &#39;b&#39;: np.int32} (Unsupported with engine=&#39;python&#39;) compression : {&#39;gzip&#39;, &#39;bz2&#39;, &#39;infer&#39;, None}, default &#39;infer&#39; For on-the-fly decompression of on-disk data. If &#39;infer&#39;, then use gzip or bz2 if filepath_or_buffer is a string ending in &#39;.gz&#39; or &#39;.bz2&#39;, respectively, and no decompression otherwise. Set to None for no decompression. dialect : string or csv.Dialect instance, default None If None defaults to Excel dialect. Ignored if sep longer than 1 char See csv.Dialect documentation for more details header : int, list of ints, default &#39;infer&#39; Row number(s) to use as the column names, and the start of the data. Defaults to 0 if nonamespassed, otherwiseNone. Explicitly passheader=0to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns E.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example are skipped). Note that this parameter ignores commented lines and empty lines ifskip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file. skiprows : list-like or integer, default None Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file index_col : int or sequence or False, default None Column to use as the row labels of the DataFrame. If a sequence is given, a MultiIndex is used. If you have a malformed file with delimiters at the end of each line, you might consider index_col=False to force pandas to _not_ use the first column as the index (row names) names : array-like, default None List of column names to use. If file contains no header row, then you should explicitly pass header=None prefix : string, default None Prefix to add to column numbers when no header, e.g &#39;X&#39; for X0, X1, ... na_values : str, list-like or dict, default None Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values true_values : list, default None Values to consider as True false_values : list, default None Values to consider as False keep_default_na : bool, default True If na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they&#39;re appended to parse_dates : boolean, list of ints or names, list of lists, or dict, default False If True -&gt; try parsing the index. If [1, 2, 3] -&gt; try parsing columns 1, 2, 3 each as a separate date column. If [[1, 3]] -&gt; combine columns 1 and 3 and parse as a single date column. {&#39;foo&#39; : [1, 3]} -&gt; parse columns 1, 3 as date and call result &#39;foo&#39; A fast-path exists for iso8601-formatted dates. keep_date_col : boolean, default False If True and parse_dates specifies combining multiple columns then 1183 keep the original columns. date_parser : function, default None Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments. dayfirst : boolean, default False DD/MM format dates, international and European format thousands : str, default None Thousands separator comment : str, default None Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long asskip_blank_lines=True), fully commented lines are ignored by the parameter `header` but not by `skiprows`. For example, if comment=&#39;#&#39;, parsing &#39;#empty\\na,b,c\\n1,2,3&#39; with `header=0` will result in &#39;a,b,c&#39; being treated as the header. decimal : str, default &#39;.&#39; Character to recognize as decimal point. E.g. use &#39;,&#39; for European data nrows : int, default None Number of rows of file to read. Useful for reading pieces of large files iterator : boolean, default False Return TextFileReader object for iteration or getting chunks withgetchunk()`. chunksize : int, default None Return TextFileReader object for iteration.See IO Tools docs for moreinformationhttp://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking` oniterator and chunksize.skipfooter : int, default 0Number of lines at bottom of file to skip (Unsupported with engine=’c’)converters : dict, default NoneDict of functions for converting values in certain columns. Keys can eitherbe integers or column labelsverbose : boolean, default FalseIndicate number of NA values placed in non-numeric columnsdelimiter : string, default NoneAlternative argument name for sep. Regular expressions are accepted.encoding : string, default NoneEncoding to use for UTF when reading/writing (ex. ‘utf-8’). List of Python standard encodings &lt;https://docs.python.org/3/library/codecs.html#standard-encodings&gt;_squeeze : boolean, default FalseIf the parsed data only contains one column then return a Seriesna_filter : boolean, default TrueDetect missing value markers (empty strings and the value of na_values). Indata without any NAs, passing na_filter=False can improve the performanceof reading a large fileusecols : array-like, default NoneReturn a subset of the columns.Results in much faster parsing time and lower memory usage.mangle_dupe_cols : boolean, default TrueDuplicate columns will be specified as ‘X.0’…’X.N’, rather than ‘X’…’X’1184tupleize_cols : boolean, default FalseLeave a list of tuples on columns as is (default is to convert toa Multi Index on the columns)error_bad_lines : boolean, default TrueLines with too many fields (e.g. a csv line with too many commas) will bydefault cause an exception to be raised, and no DataFrame will be returned.If False, then these “bad lines” will dropped from the DataFrame that isreturned. (Only valid with C parser)warn_bad_lines : boolean, default TrueIf error_bad_lines is False, and warn_bad_lines is True, a warning for each“bad line” will be output. (Only valid with C parser).infer_datetime_format : boolean, default FalseIf True and parse_dates is enabled for a column, attempt to inferthe datetime format to speed up the processingskip_blank_lines : boolean, default TrueIf True, skip over blank lines rather than interpreting as NaN values【返回值】——-result : DataFrame or TextParserAlso, ‘delimiter’ is used to specify the filler character of thefields if it is not spaces (e.g., ‘~’).read_gbq函数 read_gbq 模块所属：pandas.io.gbq:read_gbq(query, project_id=None, index_col=None, col_order=None, reauth=False, verbose=True)Load data from Google BigQuery.THIS IS AN EXPERIMENTAL LIBRARYThe main method a user calls to execute a Query in Google BigQuery and read resultsinto a pandas DataFrame using the v2 Google API client for Python. Documentation forthe API is available at https://developers.google.com/api-client-library/python/.Authentication to the Google BigQuery service is via OAuth 2.0 using the product name‘pandas GBQ’.【参数】———-query : strSQL-Like Query to return data valuesproject_id : strGoogle BigQuery Account project ID.index_col : str (optional)Name of result column to use for index in results DataFramecol_order : list(str) (optional)List of BigQuery column names in the desired order for results1185DataFramereauth : boolean (default False)Force Google BigQuery to reauthenticate the user. This is usefulif multiple accounts are used.verbose : boolean (default True)Verbose output【返回值】——-df: DataFrameDataFrame representing results of queryread_hdf函数 read_hdf 模块所属：pandas.io.pytables:read_hdf(path_or_buf, key=None, kwargs)read from the store, close it if we opened itRetrieve pandas object stored in file, optionally based on wherecriteria【参数】———-path_or_buf : path (string), or buffer to read fromkey : group identifier in the store. Can be omitted a HDF file containsa single pandas object.where : list of Term (or convertable) objects, optionalstart : optional, integer (defaults to None), row number to startselectionstop : optional, integer (defaults to None), row number to stopselectioncolumns : optional, a list of columns that if not None, will limit thereturn columnsiterator : optional, boolean, return an iterator, default Falsechunksize : optional, nrows to include in iteration, return an iterator【返回值】——-The selected objectread_html1186函数 read_html 模块所属：pandas.io.html:read_html(io, match=’.+’, flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False,tupleize_cols=False, thousands=’,’, encoding=None)Read HTML tables into a list of DataFrame objects.【参数】———-io : str or file-likeA URL, a file-like object, or a raw string containing HTML. Note thatlxml only accepts the http, ftp and file url protocols. If you have aURL that starts with &#39;https&#39; you might try removing the &#39;s&#39;.match : str or compiled regular expression, optionalThe set of tables containing text matching this regex or string will bereturned. Unless the HTML is extremely simple you will probably need topass a non-empty string here. Defaults to ‘.+’ (match any non-emptystring). The default value will return all tables contained on a page.This value is converted to a regular expression so that there isconsistent behavior between Beautiful Soup and lxml.flavor : str or None, container of stringsThe parsing engine to use. ‘bs4’ and ‘html5lib’ are synonymous witheach other, they are both there for backwards compatibility. Thedefault of None tries to use lxml to parse and if that fails itfalls back on bs4 + html5lib.header : int or list-like or None, optionalThe row (or list of rows for a :class:~pandas.MultiIndex) to use tomake the columns headers.index_col : int or list-like or None, optionalThe column (or list of columns) to use to create the index.skiprows : int or list-like or slice or None, optional0-based. Number of rows to skip after parsing the column integer. If asequence of integers or a slice is given, will skip the rows indexed bythat sequence. Note that a single element sequence means ‘skip the nthrow’ whereas an integer means ‘skip n rows’.attrs : dict or None, optionalThis is a dictionary of attributes that you can pass to use to identifythe table in the HTML. These are not checked for validity before beingpassed to lxml or Beautiful Soup. However, these attributes must bevalid HTML table attributes to work correctly. For example, ::attrs = {‘id’: ‘table’}is a valid attribute dictionary because the ‘id’ HTML tag attribute isa valid HTML attribute for any HTML tag as per this document &lt;http://www.w3.org/TR/html-markup/global-attributes.html&gt;. ::attrs = {‘asdf’: ‘table’}is not a valid attribute dictionary because ‘asdf’ is not a validHTML attribute even if it is a valid XML attribute. Valid HTML 4.01table attributes can be found here 1187 &lt;http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2&gt;. Aworking draft of the HTML 5 spec can be found here &lt;http://www.w3.org/TR/html-markup/table.html&gt;__. It contains thelatest information on table attributes for the modern web.parse_dates : bool, optionalSee :func:~pandas.read_csv for more details.tupleize_cols : bool, optionalIf False try to parse multiple header rows into a:class:~pandas.MultiIndex, otherwise return raw tuples. Defaults toFalse.thousands : str, optionalSeparator to use to parse thousands. Defaults to &#39;,&#39;.encoding : str or None, optionalThe encoding used to decode the web page. Defaults to None.Nonepreserves the previous encoding behavior, which depends on theunderlying parser library (e.g., the parser library will try to usethe encoding provided by the document).【返回值】——-dfs : list of DataFrames【注意】—–Before using this function you should read the :ref:gotchas about the HTML parsing libraries &lt;html-gotchas&gt;.Expect to do some cleanup after you call this function. For example, youmight need to manually assign column names if the column names areconverted to NaN when you pass the header=0 argument. We try to assume aslittle as possible about the structure of the table and push theidiosyncrasies of the HTML contained in the table to the user.This function searches for &lt;table&gt; elements and only for &lt;tr&gt;and &lt;th&gt; rows and &lt;td&gt; elements within each &lt;tr&gt; or &lt;th&gt;element in the table. &lt;td&gt; stands for “table data”.Similar to :func:~pandas.read_csv the header argument is applied after* skiprows is applied.This function will always return a list of :class:DataFrame orit will fail, e.g., it will not* return an empty list.【示例】——–See the :ref:read_html documentation in the IO section of the docs &lt;io.read_html&gt; for some examples of reading in HTML tables.【参见】——–pandas.read_csv1188read_json函数 read_json 模块所属：pandas.io.json:read_json(path_or_buf=None, orient=None, typ=’frame’, dtype=True, convert_axes=True, convert_dates=True,keep_default_dates=True, numpy=False, precise_float=False, date_unit=None)Convert a JSON string to pandas object【参数】———-path_or_buf : a valid JSON string or file-like, default: NoneThe string could be a URL. Valid URL schemes include http, ftp, s3, andfile. For file URLs, a host is expected. For instance, a local filecould be file://localhost/path/to/table.jsonorient Series default is &#39;index&#39; allowed values are: {&#39;split&#39;,&#39;records&#39;,&#39;index&#39;} The Series index must be unique for orient &#39;index&#39;. DataFrame default is &#39;columns&#39; allowed values are: {‘split’,’records’,’index’,’columns’,’values’} The DataFrame index must be unique for orients ‘index’ and‘columns’. The DataFrame columns must be unique for orients ‘index’,‘columns’, and ‘records’. The format of the JSON string split : dict like{index -&gt; [index], columns -&gt; [columns], data -&gt; [values]} records : list like[{column -&gt; value}, ... , {column -&gt; value}] index : dict like {index -&gt; {column -&gt; value}} columns : dict like {column -&gt; {index -&gt; value}} values : just the values arraytyp : type of object to recover (series or frame), default ‘frame’dtype : boolean or dict, default TrueIf True, infer dtypes, if a dict of column to dtype, then use those,if False, then don’t infer dtypes at all, applies only to the data.convert_axes : boolean, default TrueTry to convert the axes to the proper dtypes.convert_dates : boolean, default TrueList of columns to parse for dates; If True, then try to parsedatelike columns default is True; a column label is datelike if1189 it ends with &#39;_at&#39;, it ends with &#39;_time&#39;, it begins with &#39;timestamp&#39;, it is &#39;modified&#39;, or it is &#39;date&#39;keep_default_dates : boolean, default TrueIf parsing dates, then parse the default datelike columnsnumpy : boolean, default FalseDirect decoding to numpy arrays. Supports numeric data only, butnon-numeric column and index labels are supported. Note also that theJSON ordering MUST be the same for each term if numpy=True.precise_float : boolean, default FalseSet to enable usage of higher precision (strtod) function whendecoding string to double values. Default (False) is to use fast butless precise builtin functionalitydate_unit : string, default NoneThe timestamp unit to detect if converting dates. The default behaviouris to try and detect the correct precision, but if this is not desiredthen pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force parsing only seconds,milliseconds, microseconds or nanoseconds respectively.【返回值】——-result : Series or DataFrameread_msgpack函数 read_msgpack 模块所属：pandas.io.packers:read_msgpack(path_or_buf, iterator=False, **kwargs)Load msgpack pandas object from the specifiedfile pathTHIS IS AN EXPERIMENTAL LIBRARY and the storage formatmay not be stable until a future release.【参数】———-path_or_buf : string File path, BytesIO like or stringiterator : boolean, if True, return an iterator to the unpacker(default is False)【返回值】 1190obj : type of object stored in fileread_pickle函数 read_pickle 模块所属：pandas.io.pickle:read_pickle(path)Load pickled pandas object (or any other pickled object) from the specifiedfile pathWarning: Loading pickled data received from untrusted sources can beunsafe. See: http://docs.python.org/2.7/library/pickle.html【参数】———-path : stringFile path【返回值】——-unpickled : type of object stored in fileread_sas函数 read_sas 模块所属：pandas.io.sas:read_sas(filepath_or_buffer, format=’xport’, index=None, encoding=’ISO-8859-1’, chunksize=None, iterator=False)Read a SAS file into a DataFrame.【参数】———-filepath_or_buffer : string or file-like objectPath to SAS file or object implementing binary read method.format : stringFile format, only xport is currently supported.index : identifier of index columnIdentifier of column that should be used as index of the DataFrame.encoding : stringEncoding for text data.chunksize : intRead file chunksize lines at a time, returns iterator.iterator : boolean, default FalseReturn XportReader object for reading file incrementally.1191【返回值】——-DataFrame or XportReader【示例】——–Read a SAS Xport file:df = pandas.read_sas(‘filename.XPT’)Read a Xport file in 10,000 line chunks:itr = pandas.read_sas(‘filename.XPT’, chunksize=10000)for chunk in itr:do_something(chunk).. versionadded:: 0.17.0read_sql函数 read_sql 模块所属：pandas.io.sql:read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None)Read SQL query or database table into a DataFrame.【参数】———-sql : string SQL query or SQLAlchemy Selectable (select or text object)to be executed, or database table name.con : SQLAlchemy connectable(engine/connection) or database string URIor DBAPI2 connection (fallback mode)Using SQLAlchemy makes it possible to use any DB supported by thatlibrary.If a DBAPI2 object, only sqlite3 is supported.index_col : string or list of strings, optional, default: NoneColumn(s) to set as index(MultiIndex)coerce_float : boolean, default TrueAttempt to convert values to non-string, non-numeric objects (likedecimal.Decimal) to floating point, useful for SQL result setsparams : list, tuple or dict, optional, default: NoneList of parameters to pass to execute method. The syntax usedto pass parameters is database driver dependent. Check yourdatabase driver documentation for which of the five syntax styles,described in PEP 249’s paramstyle, is supported.Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}parse_dates : list or dict, default: None List of column names to parse as dates Dict of {column_name: format string} where format string isstrftime compatible in case of parsing string times or is one of1192(D, s, ns, ms, us) in case of parsing integer timestamps Dict of {column_name: arg dict}, where the arg dict correspondsto the keyword arguments of :func:pandas.to_datetimeEspecially useful with databases without native Datetime support,such as SQLitecolumns : list, default: NoneList of column names to select from sql table (only used when readinga table).chunksize : int, default NoneIf specified, return an iterator where chunksize is thenumber of rows to include in each chunk.【返回值】——-DataFrame【注意】—–This function is a convenience wrapper around read_sql_table andread_sql_query (and for backward compatibility) and will delegateto the specific function depending on the provided input (databasetable name or sql query). The delegated function might have more specificnotes about their functionality not listed here.【参见】——–read_sql_table : Read SQL database table into a DataFrameread_sql_query : Read SQL query into a DataFrameread_sql_query函数 read_sql_query 模块所属：pandas.io.sql:read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None)Read SQL query into a DataFrame.Returns a DataFrame corresponding to the result set of the querystring. Optionally provide an index_col parameter to use one of thecolumns as the index, otherwise default integer index will be used.【参数】———-sql : string SQL query or SQLAlchemy Selectable (select or text object)to be executed.con : SQLAlchemy connectable(engine/connection) or database string URIor sqlite3 DBAPI2 connectionUsing SQLAlchemy makes it possible to use any DB supported by thatlibrary.If a DBAPI2 object, only sqlite3 is supported.index_col : string or list of strings, optional, default: None1193Column(s) to set as index(MultiIndex)coerce_float : boolean, default TrueAttempt to convert values to non-string, non-numeric objects (likedecimal.Decimal) to floating point, useful for SQL result setsparams : list, tuple or dict, optional, default: NoneList of parameters to pass to execute method. The syntax usedto pass parameters is database driver dependent. Check yourdatabase driver documentation for which of the five syntax styles,described in PEP 249’s paramstyle, is supported.Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}parse_dates : list or dict, default: None List of column names to parse as dates Dict of {column_name: format string} where format string isstrftime compatible in case of parsing string times or is one of(D, s, ns, ms, us) in case of parsing integer timestamps Dict of {column_name: arg dict}, where the arg dict correspondsto the keyword arguments of :func:pandas.to_datetimeEspecially useful with databases without native Datetime support,such as SQLitechunksize : int, default NoneIf specified, return an iterator where chunksize is the number ofrows to include in each chunk.【返回值】——-DataFrame【注意】—–Any datetime values with time zone information parsed via the parse_datesparameter will be converted to UTC【参见】——–read_sql_table : Read SQL database table into a DataFrameread_sqlread_sql_table函数 read_sql_table 模块所属：pandas.io.sql:read_sql_table(table_name, con, schema=None, index_col=None, coerce_float=True, parse_dates=None, columns=None,chunksize=None)Read SQL database table into a DataFrame.Given a table name and an SQLAlchemy connectable, returns a DataFrame.This function does not support DBAPI connections.【参数】 1194table_name : stringName of SQL table in databasecon : SQLAlchemy connectable (or database string URI)Sqlite DBAPI connection mode not supportedschema : string, default NoneName of SQL schema in database to query (if database flavorsupports this). If None, use default schema (default).index_col : string or list of strings, optional, default: NoneColumn(s) to set as index(MultiIndex)coerce_float : boolean, default TrueAttempt to convert values to non-string, non-numeric objects (likedecimal.Decimal) to floating point. Can result in loss of Precision.parse_dates : list or dict, default: None List of column names to parse as dates Dict of {column_name: format string} where format string isstrftime compatible in case of parsing string times or is one of(D, s, ns, ms, us) in case of parsing integer timestamps Dict of {column_name: arg dict}, where the arg dict correspondsto the keyword arguments of :func:pandas.to_datetimeEspecially useful with databases without native Datetime support,such as SQLitecolumns : list, default: NoneList of column names to select from sql tablechunksize : int, default NoneIf specified, return an iterator where chunksize is the number ofrows to include in each chunk.【返回值】——-DataFrame【注意】—–Any datetime values with time zone information will be converted to UTC【参见】——–read_sql_query : Read SQL query into a DataFrame.read_sqlread_stata函数 read_stata 模块所属：pandas.io.stata:read_stata(filepath_or_buffer, convert_dates=True, convert_categoricals=True, encoding=None, index=None,convert_missing=False, preserve_dtypes=True, columns=None, order_categoricals=True, chunksize=None, iterator=False)Read Stata file into DataFrame【参数】 1195filepath_or_buffer : string or file-like objectPath to .dta file or object implementing a binary read() functionsconvert_dates : boolean, defaults to TrueConvert date variables to DataFrame time valuesconvert_categoricals : boolean, defaults to TrueRead value labels and convert columns to Categorical/Factor variablesencoding : string, None or encodingEncoding used to parse the files. Note that Stata doesn’tsupport unicode. None defaults to iso-8859-1.index : identifier of index columnidentifier of column that should be used as index of the DataFrameconvert_missing : boolean, defaults to FalseFlag indicating whether to convert missing values to their Statarepresentations. If False, missing values are replaced with nans.If True, columns containing missing values are returned withobject data types and missing values are represented byStataMissingValue objects.preserve_dtypes : boolean, defaults to TruePreserve Stata datatypes. If False, numeric data are upcast to pandasdefault types for foreign data (float64 or int64)columns : list or NoneColumns to retain. Columns will be returned in the given order. Nonereturns all columnsorder_categoricals : boolean, defaults to TrueFlag indicating whether converted categorical data are ordered.chunksize : int, default NoneReturn StataReader object for iterations, returns chunks withgiven number of linesiterator : boolean, default FalseReturn StataReader object【返回值】——-DataFrame or StataReader【示例】——–Read a Stata dta file:df = pandas.read_stata(‘filename.dta’)Read a Stata dta file in 10,000 line chunks:itr = pandas.read_stata(‘filename.dta’, chunksize=10000)for chunk in itr:do_something(chunk)read_table函数 read_table 模块所属：pandas.io.parsers:read_table(filepath_or_buffer, sep=’\\t’, dialect=None, compression=’infer’, doublequote=True, escapechar=None, quotechar=’”‘,1196quoting=0, skipinitialspace=False, lineterminator=None, header=’infer’, index_col=None, names=None, prefix=None,skiprows=None, skipfooter=None, skip_footer=0, na_values=None, true_values=None, false_values=None, delimiter=None,converters=None, dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False, na_filter=True,compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, warn_bad_lines=True, error_bad_lines=True,keep_default_na=True, thousands=None, comment=None, decimal=b’.’, parse_dates=False, keep_date_col=False,dayfirst=False, date_parser=None, memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None,verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False, infer_datetime_format=False,skip_blanklines=True)Read general delimited file into DataFrameAlso supports optionally iterating or breaking of the fileinto chunks.Additional help can be found in the online docs for IO Tools &lt;http://pandas.pydata.org/pandas-docs/stable/io.html&gt;.【参数】———-filepath_orbuffer : string or file handle / StringIOThe string could be a URL. Valid URL schemes includehttp, ftp, s3, and file. For file URLs, ahost is expected. For instance, a local file could befile ://localhost/path/to/table.csvsep : string, default \\t (tab-stop)Delimiter to use. Regular expressions are accepted.engine : {‘c’, ‘python’}Parser engine to use. The C engine is faster while the python engine iscurrently more feature-complete.lineterminator : string (length 1), default NoneCharacter to break file into lines. Only valid with C parserquotechar : string (length 1)The character used to denote the start and end of a quoted item. Quoteditems can include the delimiter and it will be ignored.quoting : int or csv.QUOTE instance, default NoneControl field quoting behavior per ``csv.QUOTE_constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3). Default (None) results in QUOTE_MINIMAL behavior. skipinitialspace : boolean, default False Skip spaces after delimiter escapechar : string (length 1), default None One-character string used to escape delimiter when quoting is QUOTE_NONE. dtype : Type name or dict of column -&gt; type, default None Data type for data or columns. E.g. {&#39;a&#39;: np.float64, &#39;b&#39;: np.int32} (Unsupported with engine=&#39;python&#39;) compression : {&#39;gzip&#39;, &#39;bz2&#39;, &#39;infer&#39;, None}, default &#39;infer&#39; For on-the-fly decompression of on-disk data. If &#39;infer&#39;, then use gzip or bz2 if filepath_or_buffer is a string ending in &#39;.gz&#39; or &#39;.bz2&#39;, respectively, and no decompression otherwise. Set to None for no decompression. dialect : string or csv.Dialect instance, default None If None defaults to Excel dialect. Ignored if sep longer than 1 char See csv.Dialect documentation for more details header : int, list of ints, default &#39;infer&#39; Row number(s) to use as the column names, and the start of the data. Defaults to 0 if nonamespassed, otherwiseNone. Explicitly passheader=0to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the 1197 columns E.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example are skipped). Note that this parameter ignores commented lines and empty lines ifskip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file. skiprows : list-like or integer, default None Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file index_col : int or sequence or False, default None Column to use as the row labels of the DataFrame. If a sequence is given, a MultiIndex is used. If you have a malformed file with delimiters at the end of each line, you might consider index_col=False to force pandas to _not_ use the first column as the index (row names) names : array-like, default None List of column names to use. If file contains no header row, then you should explicitly pass header=None prefix : string, default None Prefix to add to column numbers when no header, e.g &#39;X&#39; for X0, X1, ... na_values : str, list-like or dict, default None Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values true_values : list, default None Values to consider as True false_values : list, default None Values to consider as False keep_default_na : bool, default True If na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they&#39;re appended to parse_dates : boolean, list of ints or names, list of lists, or dict, default False If True -&gt; try parsing the index. If [1, 2, 3] -&gt; try parsing columns 1, 2, 3 each as a separate date column. If [[1, 3]] -&gt; combine columns 1 and 3 and parse as a single date column. {&#39;foo&#39; : [1, 3]} -&gt; parse columns 1, 3 as date and call result &#39;foo&#39; A fast-path exists for iso8601-formatted dates. keep_date_col : boolean, default False If True and parse_dates specifies combining multiple columns then keep the original columns. date_parser : function, default None Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments. dayfirst : boolean, default False DD/MM format dates, international and European format thousands : str, default None Thousands separator comment : str, default None Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long asskip_blank_lines=True), fully commented lines are ignored by the parameter `header` but not by `skiprows`. For example, if comment=&#39;#&#39;, parsing &#39;#empty\\na,b,c\\n1,2,3&#39; with `header=0` will result in &#39;a,b,c&#39; being treated as the header. 1198 decimal : str, default &#39;.&#39; Character to recognize as decimal point. E.g. use &#39;,&#39; for European data nrows : int, default None Number of rows of file to read. Useful for reading pieces of large files iterator : boolean, default False Return TextFileReader object for iteration or getting chunks withgetchunk()`. chunksize : int, default None Return TextFileReader object for iteration.See IO Tools docs for moreinformationhttp://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking` oniterator and chunksize.skipfooter : int, default 0Number of lines at bottom of file to skip (Unsupported with engine=’c’)converters : dict, default NoneDict of functions for converting values in certain columns. Keys can eitherbe integers or column labelsverbose : boolean, default FalseIndicate number of NA values placed in non-numeric columnsdelimiter : string, default NoneAlternative argument name for sep. Regular expressions are accepted.encoding : string, default NoneEncoding to use for UTF when reading/writing (ex. ‘utf-8’). List of Python standard encodings &lt;https://docs.python.org/3/library/codecs.html#standard-encodings&gt;_squeeze : boolean, default FalseIf the parsed data only contains one column then return a Seriesna_filter : boolean, default TrueDetect missing value markers (empty strings and the value of na_values). Indata without any NAs, passing na_filter=False can improve the performanceof reading a large fileusecols : array-like, default NoneReturn a subset of the columns.Results in much faster parsing time and lower memory usage.mangle_dupe_cols : boolean, default TrueDuplicate columns will be specified as ‘X.0’…’X.N’, rather than ‘X’…’X’tupleize_cols : boolean, default FalseLeave a list of tuples on columns as is (default is to convert toa Multi Index on the columns)error_bad_lines : boolean, default TrueLines with too many fields (e.g. a csv line with too many commas) will bydefault cause an exception to be raised, and no DataFrame will be returned.If False, then these “bad lines” will dropped from the DataFrame that isreturned. (Only valid with C parser)warn_bad_lines : boolean, default TrueIf error_bad_lines is False, and warn_bad_lines is True, a warning for each“bad line” will be output. (Only valid with C parser).infer_datetime_format : boolean, default FalseIf True and parse_dates is enabled for a column, attempt to inferthe datetime format to speed up the processingskip_blank_lines : boolean, default TrueIf True, skip over blank lines rather than interpreting as NaN values【返回值】——-result : DataFrame or TextParser1199reset_optionCallableDynamicDoc 模块所属：pandas.core.config object:类定义：CallableDynamicDoc(builtins.object)| 【方法定义】|| call(self, args, *kwds)| Call self as a function.|| init(self, func, doc_tmpl)| Initialize self. See help(type(self)) for accurate signature.|| ———————————————————————-| Data descriptors defined here:|| dict| dictionary for instance variables (if defined)|| weakref| list of weak references to the object (if defined)rolling_apply函数 rolling_apply 模块所属：pandas.stats.moments:rolling_apply(arg, window, func, min_periods=None, freq=None, center=False, args=(), kwargs={})Generic moving function application.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.func : functionMust produce a single value from an ndarray inputmin_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default False1200Whether the label should correspond with center of windowargs : tuplePassed on to funckwargs : dictPassed on to func【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_corr函数 rolling_corr 模块所属：pandas.stats.moments:rolling_corr(arg1, arg2=None, window=None, min_periods=None, freq=None, center=False, pairwise=None, how=None)Moving sample correlation.【参数】———-arg1 : Series, DataFrame, or ndarrayarg2 : Series, DataFrame, or ndarray, optionalif not supplied then will default to arg1 and produce pairwise outputwindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-samplingpairwise : bool, default FalseIf False then only matching columns between arg1 and arg2 will be used andthe output will be a DataFrame.If True then all pairwise combinations will be calculated and the outputwill be a Panel in the case of DataFrame inputs. In the case of missingelements, only complete pairwise observations will be used.1201【返回值】——-y : type depends on inputsDataFrame / DataFrame -&gt; DataFrame (matches on columns) or Panel (pairwise)DataFrame / Series -&gt; Computes result for each columnSeries / Series -&gt; Series【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_corr_pairwise函数 rolling_corr_pairwise 模块所属：pandas.stats.moments:rolling_corr_pairwise(df1, df2=None, window=None, min_periods=None, freq=None, center=False)Deprecated. Use rolling_corr(…, pairwise=True) instead.Pairwise moving sample correlation【参数】———-df1 : DataFramedf2 : DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-sampling【返回值】——-y : Panel whose items are df1.index values 【注意】1202By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_count函数 rolling_count 模块所属：pandas.stats.moments:rolling_count(arg, window, freq=None, center=False, how=None)Rolling count of number of non-NaN observations inside provided window.【参数】———-arg : DataFrame or numpy ndarray-likewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseWhether the label should correspond with center of windowhow : string, default ‘mean’Method for down- or re-sampling【返回值】——-rolling_count : type of caller【注意】—–The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_cov函数 rolling_cov 模块所属：pandas.stats.moments:rolling_cov(arg1, arg2=None, window=None, min_periods=None, freq=None, center=False, pairwise=None, how=None,1203ddof=1)Unbiased moving covariance.【参数】———-arg1 : Series, DataFrame, or ndarrayarg2 : Series, DataFrame, or ndarray, optionalif not supplied then will default to arg1 and produce pairwise outputwindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-samplingpairwise : bool, default FalseIf False then only matching columns between arg1 and arg2 will be used andthe output will be a DataFrame.If True then all pairwise combinations will be calculated and the outputwill be a Panel in the case of DataFrame inputs. In the case of missingelements, only complete pairwise observations will be used.ddof : int, default 1Delta Degrees of Freedom. The divisor used in calculationsis N - ddof, where N represents the number of elements.【返回值】——-y : type depends on inputsDataFrame / DataFrame -&gt; DataFrame (matches on columns) or Panel (pairwise)DataFrame / Series -&gt; Computes result for each columnSeries / Series -&gt; Series【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_kurt函数 roll_kurt 模块所属：pandas.algos:1204roll_kurt(…)Unbiased moving kurtosis.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-sampling【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_max函数 roll_max 模块所属：pandas.algos:roll_max(…)Moving max of 1d array of dtype=float64 along axis=0 ignoring NaNs.Moving maximum.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value1205(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘’max’Method for down- or re-sampling【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_mean函数 roll_mean 模块所属：pandas.algos:roll_mean(…)Moving mean.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-sampling【返回值】——-y : type of input argument1206【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_median函数 roll_median_c 模块所属：pandas.algos:roll_median_c(…)Moving median.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘’median’Method for down- or re-sampling【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).1207rolling_min函数 roll_min 模块所属：pandas.algos:roll_min(…)Moving min of 1d array of dtype=float64 along axis=0 ignoring NaNs.Moving minimum.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘’min’Method for down- or re-sampling【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_quantile函数 rolling_quantile 模块所属：pandas.stats.moments:rolling_quantile(arg, window, quantile, min_periods=None, freq=None, center=False)Moving quantile.【参数】1208———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.quantile : float0 &lt;= quantile &lt;= 1min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseWhether the label should correspond with center of window【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_skew函数 roll_skew 模块所属：pandas.algos:roll_skew(…)Unbiased moving skewness.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.1209how : string, default ‘None’Method for down- or re-sampling【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_std函数 模块所属：pandas.stats.moments: lambda a, *kwMoving standard deviation.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-samplingddof : int, default 1Delta Degrees of Freedom. The divisor used in calculationsis N - ddof, where N represents the number of elements.【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can be1210changed to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_sum函数 roll_sum 模块所属：pandas.algos:roll_sum(…)Moving sum.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-sampling【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).1211rolling_var函数 roll_var 模块所属：pandas.algos:roll_var(…)Numerically stable implementation using Welford’s method.Moving variance.【参数】———-arg : Series, DataFramewindow : intSize of the moving window. This is the number of observations used forcalculating the statistic.min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseSet the labels at the center of the window.how : string, default ‘None’Method for down- or re-samplingddof : int, default 1Delta Degrees of Freedom. The divisor used in calculationsis N - ddof, where N represents the number of elements.【返回值】——-y : type of input argument【注意】—–By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).rolling_window函数 rolling_window 模块所属：pandas.stats.moments:1212rolling_window(arg, window=None, win_type=None, min_periods=None, freq=None, center=False, mean=True, axis=0,how=None, kwargs)Applies a moving window of type window_type and size windowon the data.【参数】———-arg : Series, DataFramewindow : int or ndarrayWeighting window specification. If the window is an integer, then it istreated as the window length and win_type is requiredwin_type : str, default NoneWindow type (see Notes)min_periods : int, default NoneMinimum number of observations in window required to have a value(otherwise result is NA).freq : string or DateOffset object, optional (default None)Frequency to conform the data to before computing the statistic. Specifiedas a frequency string or DateOffset object.center : boolean, default FalseWhether the label should correspond with center of windowmean : boolean, default TrueIf True computes weighted mean, else weighted sumaxis : {0, 1}, default 0how : string, default ‘mean’Method for down- or re-sampling【返回值】——-y : type of input argument【注意】—–The recognized window types are: boxcar triang blackman hamming bartlett parzen bohman blackmanharris nuttall barthann kaiser (needs beta) gaussian (needs std) general_gaussian (needs power, width) slepian (needs width).By default, the result is set to the right edge of the window. This can bechanged to the center of the window by setting center=True.The freq keyword is used to conform time series data to a specifiedfrequency by resampling the data. This is done with the default parametersof :meth:~pandas.Series.resample (i.e. using the mean).1213scatter_matrix函数 scatter_matrix 模块所属：pandas.tools.plotting:scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False, diagonal=’hist’, marker=’.’, density_kwds=None,hist_kwds=None, range_padding=0.05, kwds)Draw a matrix of scatter plots.【参数】———-frame : DataFramealpha : float, optionalamount of transparency appliedfigsize : (float,float), optionala tuple (width, height) in inchesax : Matplotlib axis object, optionalgrid : bool, optionalsetting this to True will show the griddiagonal : {‘hist’, ‘kde’}pick between ‘kde’ and ‘hist’ foreither Kernel Density Estimation or Histogramplot in the diagonalmarker : str, optionalMatplotlib marker type, default ‘.’hist_kwds : other plotting keyword argumentsTo be passed to hist functiondensity_kwds : other plotting keyword argumentsTo be passed to kernel density estimate plotrange_padding : float, optionalrelative extension of axis range in x and ywith respect to (x_max - x_min) or (y_max - y_min),default 0.05kwds : other plotting keyword argumentsTo be passed to scatter function【示例】——–&gt;&gt;&gt; df = DataFrame(np.random.randn(1000, 4), columns=[‘A’,’B’,’C’,’D’])scatter_matrix(df, alpha=0.2)set_eng_float_format函数 set_eng_float_format 模块所属：pandas.core.format:1214set_eng_float_format(accuracy=3, use_eng_prefix=False)Alter default behavior on how float is formatted in DataFrame.Format float in engineering format. By accuracy, we mean the number ofdecimal digits after the floating point.参见：EngFormatter.set_optionCallableDynamicDoc 模块所属：pandas.core.config object:类定义：CallableDynamicDoc(builtins.object)| 【方法定义】|| call(self, *args, kwds)| Call self as a function.|| init(self, func, doc_tmpl)| Initialize self. See help(type(self)) for accurate signature.|| ———————————————————————-| Data descriptors defined here:|| dict| dictionary for instance variables (if defined)|| weakref| list of weak references to the object (if defined)show_versions函数 show_versions 模块所属：pandas.util.print_versions:show_versions(as_json=False)1215sparse模块包所属：pandas.sparse in pandas:【名称】pandas.sparse【模块包·内容】apiarrayframelistpanelscipy_sparseseriestests (package)【文件】 ： \\pandas\\sparse__init.pystats模块包所属：pandas.stats in pandas:【名称】pandas.stats【模块包·内容】apicommonfama_macbethinterfacemathmiscmomentsolsplmtests (package)var【文件】 ： \\pandas\\stats\\init__.py1216timedelta_range函数 timedelta_range 模块所属：pandas.tseries.tdi:timedelta_range(start=None, end=None, periods=None, freq=’D’, name=None, closed=None)Return a fixed frequency timedelta index, with day as the defaultfrequency【参数】———-start : string or timedelta-like, default NoneLeft bound for generating datesend : string or datetime-like, default NoneRight bound for generating datesperiods : integer or None, default NoneIf None, must specify start and endfreq : string or DateOffset, default ‘D’ (calendar daily)Frequency strings can have multiples, e.g. ‘5H’name : str, default NoneName of the resulting indexclosed : string or None, default NoneMake the interval closed with respect to the given frequency tothe ‘left’, ‘right’, or both sides (None)【注意】—–2 of start, end, or periods must be specified【返回值】——-rng : TimedeltaIndexto_datetime函数 to_datetime 模块所属：pandas.tseries.tools:to_datetime(arg, errors=’raise’, dayfirst=False, yearfirst=False, utc=None, box=True, format=None, exact=True, coerce=None,unit=’ns’, infer_datetime_format=False)Convert argument to datetime.【参数】———-arg : string, datetime, array of strings (with possible NAs)errors : {‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’ If ‘raise’, then invalid parsing will raise an exception If ‘coerce’, then invalid parsing will be set as NaT1217 If ‘ignore’, then invalid parsing will return the inputdayfirst : boolean, default FalseSpecify a date parse order if arg is str or its list-likes.If True, parses dates with the day first, eg 10/11/12 is parsed as 2012-11-10.Warning: dayfirst=True is not strict, but will prefer to parsewith day first (this is a known bug, based on dateutil behavior).yearfirst : boolean, default FalseSpecify a date parse order if arg is str or its list-likes. If True parses dates with the year first, eg 10/11/12 is parsed as 2010-11-12. If both dayfirst and yearfirst are True, yearfirst is preceded (same as dateutil).Warning: yearfirst=True is not strict, but will prefer to parsewith year first (this is a known bug, based on dateutil beahavior)... versionadded: 0.16.1utc : boolean, default NoneReturn UTC DatetimeIndex if True (converting any tz-awaredatetime.datetime objects as well).box : boolean, default True If True returns a DatetimeIndex If False returns ndarray of values.format : string, default Nonestrftime to parse time, eg “%d/%m/%Y”, note that “%f” will parseall the way up to nanoseconds.exact : boolean, True by default If True, require an exact format match. If False, allow the format to match anywhere in the target string.unit : unit of the arg (D,s,ms,us,ns) denote the unit in epoch(e.g. a unix timestamp), which is an integer/float number.infer_datetime_format : boolean, default FalseIf no format is given, try to infer the format based on the firstdatetime string. Provides a large speed-up in many cases.【返回值】——-ret : datetime if parsing succeeded.Return type depends on input: list-like: DatetimeIndex Series: Series of datetime64 dtype scalar: TimestampIn case when it is not possible to return designated types (e.g. whenany element of input is before Timestamp.min or after Timestamp.max)return will have datetime.datetime type (or correspoding array/Series).【示例】——–Take separate series and convert to datetimeimport pandas as pdi = pd.date_range(‘20000101’,periods=100)df = pd.DataFrame(dict(year = i.year, month = i.month, day = i.day))pd.to_datetime(df.year10000 + df.month100 + df.day, format=’%Y%m%d’)0 2000-01-011 2000-01-02…121898 2000-04-0899 2000-04-09Length: 100, dtype: datetime64[ns]Or from stringsdf = df.astype(str)pd.to_datetime(df.day + df.month + df.year, format=”%d%m%Y”)0 2000-01-011 2000-01-02…98 2000-04-0899 2000-04-09Length: 100, dtype: datetime64[ns]Date that does not meet timestamp limitations:pd.to_datetime(‘13000101’, format=’%Y%m%d’)datetime.datetime(1300, 1, 1, 0, 0)pd.to_datetime(‘13000101’, format=’%Y%m%d’, errors=’coerce’)NaTto_msgpack函数 to_msgpack 模块所属：pandas.io.packers:to_msgpack(path_or_buf, args, *kwargs)msgpack (serialize) object to input file pathTHIS IS AN EXPERIMENTAL LIBRARY and the storage formatmay not be stable until a future release.【参数】———-path_or_buf : string File path, buffer-like, or Noneif None, return generated stringargs : an object or objects to serializeappend : boolean whether to append to an existing msgpack(default is False)compress : type of compressor (zlib or blosc), default to None (nocompression)to_numeric1219函数 to_numeric 模块所属：pandas.tools.util:to_numeric(arg, errors=’raise’)Convert argument to a numeric type.【参数】———-arg : list, tuple or array of objects, or Serieserrors : {‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’ If ‘raise’, then invalid parsing will raise an exception If ‘coerce’, then invalid parsing will be set as NaN If ‘ignore’, then invalid parsing will return the input【返回值】——-ret : numeric if parsing succeeded.Return type depends on input. Series if Series, otherwise ndarray【示例】——–Take separate series and convert to numeric, coercing when told toimport pandas as pds = pd.Series([‘1.0’, ‘2’, -3])pd.to_numeric(s)s = pd.Series([‘apple’, ‘1.0’, ‘2’, -3])pd.to_numeric(s, errors=’ignore’)pd.to_numeric(s, errors=’coerce’)to_pickle函数 to_pickle 模块所属：pandas.io.pickle:to_pickle(obj, path)Pickle (serialize) object to input file path【参数】———-obj : any objectpath : stringFile pathto_timedelta1220函数 to_timedelta 模块所属：pandas.tseries.timedeltas:to_timedelta(arg, unit=’ns’, box=True, errors=’raise’, coerce=None)Convert argument to timedelta【参数】———-arg : string, timedelta, array of strings (with possible NAs)unit : unit of the arg (D,h,m,s,ms,us,ns) denote the unit, which is an integer/float numberbox : boolean, default True If True returns a Timedelta/TimedeltaIndex of the results if False returns a np.timedelta64 or ndarray of values of dtype timedelta64[ns]errors : {‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’ If ‘raise’, then invalid parsing will raise an exception If ‘coerce’, then invalid parsing will be set as NaT If ‘ignore’, then invalid parsing will return the input【返回值】——-ret : timedelta64/arrays of timedelta64 if parsing succeededtools模块包所属：pandas.tools in pandas:【名称】pandas.tools【模块包·内容】mergepivotplottingrplottests (package)tileutil【文件】 ： \\pandas\\tools__init.pytseries1221模块包所属：pandas.tseries in pandas:【名称】pandas.tseries【模块包·内容】apibasecommonconverterfrequenciesholidayindexintervaloffsetsperiodplottingresampletditests (package)timedeltastoolsutil【文件】 ： \\pandas\\tseries\\init.pytslib所属模块：pandas.tslib in pandas:【名称】pandas.tslib【数据】 all = [] pyx_capi = {‘_check_all_nulls’: &lt;capsule object “int (PyObject *)”… test = {‘_get_rule_month (line 1826)’: “\\n Return starting mont…【文件】 ： \\pandas\\tslib.cp35-win_amd64.pydunique1222函数 unique 模块所属：pandas.core.algorithms:unique(values)Compute unique values (not necessarily sorted) efficiently from input arrayof values【参数】———-values : array-like【返回值】——-uniquesutil模块包所属：pandas.util in pandas:【名称】pandas.util【模块包·内容】clipboarddecoratorsdoctoolsmiscprint_versionsterminaltesting【文件】 ： \\pandas\\util\\init__.pyvalue_counts函数 value_counts 模块所属：pandas.core.algorithms:value_counts(values, sort=True, ascending=False, normalize=False, bins=None, dropna=True)Compute a histogram of the counts of non-null values.【参数】 1223values : ndarray (1-d)sort : boolean, default TrueSort by valuesascending : boolean, default FalseSort in ascending ordernormalize: boolean, default FalseIf True then compute a relative histogrambins : integer, optionalRather than count values, group them into half-open bins,convenience for pd.cut, only works with numeric datadropna : boolean, default TrueDon’t include counts of NaN【返回值】——-value_counts : Serieswide_to_long函数 wide_to_long 模块所属：pandas.core.reshape:wide_to_long(df, stubnames, i, j)Wide panel to long format. Less flexible but more user-friendly than melt.【参数】———-df : DataFrameThe wide-format DataFramestubnames : listA list of stub names. The wide format variables are assumed tostart with the stub names.i : strThe name of the id variable.j : strThe name of the subobservation variable.stubend : strRegex to match for the end of the stubs.【返回值】——-DataFrameA DataFrame that contains each stub name as a variable as well asvariables for i and j.【示例】——–&gt;&gt;&gt; import pandas as pdimport numpy as npnp.random.seed(123)df = pd.DataFrame({“A1970” : {0 : “a”, 1 : “b”, 2 : “c”},1224… “A1980” : {0 : “d”, 1 : “e”, 2 : “f”},… “B1970” : {0 : 2.5, 1 : 1.2, 2 : .7},… “B1980” : {0 : 3.2, 1 : 1.3, 2 : .1},… “X” : dict(zip(range(3), np.random.randn(3)))… })df[“id”] = df.indexdfA1970 A1980 B1970 B1980 X id0 a d 2.5 3.2 -1.085631 01 b e 1.2 1.3 0.997345 12 c f 0.7 0.1 0.282978 2wide_to_long(df, [“A”, “B”], i=”id”, j=”year”)X A Bid year0 1970 -1.085631 a 2.51 1970 0.997345 b 1.22 1970 0.282978 c 0.70 1980 -1.085631 d 3.21 1980 0.997345 e 1.32 1980 0.282978 f 0.1【注意】—–All extra variables are treated as extra id variables. This simply usespandas.melt under the hood, but is hard-coded to “do the right thing”in a typicaly case.","categories":[],"tags":[]},{"title":"","slug":"十分钟搞定Pandas","date":"2017-04-05T02:12:40.000Z","updated":"2017-04-06T01:27:35.773Z","comments":true,"path":"2017/04/05/十分钟搞定Pandas/","link":"","permalink":"http://yoursite.com/2017/04/05/十分钟搞定Pandas/","excerpt":"十分钟搞定Pandas 本文是学习Pandas的简要学习笔记。主要参考资料：http://python.jobbole.com/84416/","text":"十分钟搞定Pandas 本文是学习Pandas的简要学习笔记。主要参考资料：http://python.jobbole.com/84416/ 本文是对pandas官方网站上《10 Minutes to pandas》的一个简单的翻译。这篇文章是对pandas的一个简单的介绍，详细的介绍请参考：CookBook。也可以查看原文。习惯上，我们会按下面格式引入所需要的包：123456In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 一、 创建对象可以通过 Data Structure Intro Setion 来查看有关该节内容的详细信息。1、可以通过传递一个list对象来创建一个Series，pandas会默认创建整型索引：1234567891011In [4]: s = pd.Series([1,3,5,np.nan,6,8])In [5]: sOut[5]: 0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float64 2、通过传递一个numpy array，时间索引以及列标签来创建一个DataFrame：12345678910111213141516171819In [6]: dates = pd.date_range('20130101', periods=6)In [7]: datesOut[7]: DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')In [8]: df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))In [9]: dfOut[9]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 3、通过传递一个能够被转换成类似序列结构的字典对象来创建一个DataFrame： 1234567891011121314In [10]: df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)In [11]: df2Out[11]: A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo 4、查看不同列的数据类型：123456789In [12]: df2.dtypesOut[12]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 5、如果你使用的是IPython，使用Tab自动补全功能会自动识别所有的属性以及自定义的列，下图中是所有能够被自动识别的属性的一个子集：123456789101112131415161718192021222324In [13]: df2.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D 二、 查看数据详情请参阅：Basics Section1、 查看frame中头部和尾部的行：123456789101112131415In [14]: df.head()Out[14]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401In [15]: df.tail(3)Out[15]: A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 2、 显示索引、列和底层的numpy数据：1234567891011121314151617In [16]: df.indexOut[16]: DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')In [17]: df.columnsOut[17]: Index([u'A', u'B', u'C', u'D'], dtype='object')In [18]: df.valuesOut[18]: array([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]]) 3、 describe()函数对于数据的快速统计汇总：1234567891011In [19]: df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 4、 对数据的转置：1234567In [20]: df.TOut[20]: 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06A 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690B -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648C -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427D -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.524988 5、 按轴进行排序 123456789In [21]: df.sort_index(axis=1, ascending=False)Out[21]: D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690 6、 按值进行排序123456789In [22]: df.sort_values(by='B')Out[22]: A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 三、 选择虽然标准的Python/Numpy的选择和设置表达式都能够直接派上用场，但是作为工程使用的代码，我们推荐使用经过优化的pandas数据访问方式：.at, .iat, .loc, .iloc 和 .ix详情请参阅Indexing and Selecing Data 和 MultiIndex / Advanced Indexing。 获取1、 选择一个单独的列，这将会返回一个Series，等同于df.A：123456789In [23]: df['A']Out[23]: 2013-01-01 0.4691122013-01-02 1.2121122013-01-03 -0.8618492013-01-04 0.7215552013-01-05 -0.4249722013-01-06 -0.673690Freq: D, Name: A, dtype: float64 2、 通过[]进行选择，这将会对行进行切片12345678910111213In [24]: df[0:3]Out[24]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804In [25]: df['20130102':'20130104']Out[25]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.271860 通过标签选择1、 使用标签来获取一个交叉的区域1234567In [26]: df.loc[dates[0]]Out[26]: A 0.469112B -0.282863C -1.509059D -1.135632Name: 2013-01-01 00:00:00, dtype: float64 2、 通过标签来在多个轴上进行选择123456789In [27]: df.loc[:,['A','B']]Out[27]: A B2013-01-01 0.469112 -0.2828632013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.5670202013-01-06 -0.673690 0.113648 3、 标签切片 123456In [28]: df.loc['20130102':'20130104',['A','B']]Out[28]: A B2013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.706771 4、 对于返回的对象进行维度缩减12345In [29]: df.loc['20130102',['A','B']]Out[29]: A 1.212112B -0.173215Name: 2013-01-02 00:00:00, dtype: float64 5、 获取一个标量12In [30]: df.loc[dates[0],'A']Out[30]: 0.46911229990718628 6、 快速访问一个标量（与上一个方法等价）12In [31]: df.at[dates[0],'A']Out[31]: 0.46911229990718628 通过位置选择1、 通过传递数值进行位置选择（选择的是行）1234567In [32]: df.iloc[3]Out[32]: A 0.721555B -0.706771C -1.039575D 0.271860Name: 2013-01-04 00:00:00, dtype: float64 2、 通过数值进行切片，与numpy/python中的情况类似12345In [33]: df.iloc[3:5,0:2]Out[33]: A B2013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.567020 3、 通过指定一个位置的列表，与numpy/python中的情况类似123456In [34]: df.iloc[[1,2,4],[0,2]]Out[34]: A C2013-01-02 1.212112 0.1192092013-01-03 -0.861849 -0.4949292013-01-05 -0.424972 0.276232 4、 对行进行切片12345In [35]: df.iloc[1:3,:]Out[35]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804 5、 对列进行切片123456789In [36]: df.iloc[:,1:3]Out[36]: B C2013-01-01 -0.282863 -1.5090592013-01-02 -0.173215 0.1192092013-01-03 -2.104569 -0.4949292013-01-04 -0.706771 -1.0395752013-01-05 0.567020 0.2762322013-01-06 0.113648 -1.478427 6、 获取特定的值12In [37]: df.iloc[1,1]Out[37]: -0.17321464905330858 快速访问一个值，访问结果与上面代码相同。12In [38]: df.iat[1,1]Out[38]: -0.17321464905330858 布尔索引1、 使用一个单独列的值来选择数据：123456In [39]: df[df.A &gt; 0]Out[39]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-04 0.721555 -0.706771 -1.039575 0.271860 2、 使用where操作来选择数据：123456789In [40]: df[df &gt; 0]Out[40]: A B C D2013-01-01 0.469112 NaN NaN NaN2013-01-02 1.212112 NaN 0.119209 NaN2013-01-03 NaN NaN NaN 1.0718042013-01-04 0.721555 NaN NaN 0.2718602013-01-05 NaN 0.567020 0.276232 NaN2013-01-06 NaN 0.113648 NaN 0.524988 3、 使用isin()方法来过滤：12345678910111213141516171819In [41]: df2 = df.copy()In [42]: df2['E'] = ['one', 'one','two','three','four','three']In [43]: df2Out[43]: A B C D E2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 one2013-01-02 1.212112 -0.173215 0.119209 -1.044236 one2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-04 0.721555 -0.706771 -1.039575 0.271860 three2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four2013-01-06 -0.673690 0.113648 -1.478427 0.524988 threeIn [44]: df2[df2['E'].isin(['two','four'])]Out[44]: A B C D E2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four 设置1、 设置一个新的列：12345678910111213In [45]: s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range('20130102', periods=6))In [46]: s1Out[46]: 2013-01-02 12013-01-03 22013-01-04 32013-01-05 42013-01-06 52013-01-07 6Freq: D, dtype: int64In [47]: df['F'] = s1 2、 通过标签设置新的值：1In [48]: df.at[dates[0],'A'] = 0 3、 通过位置设置新的值：1In [49]: df.iat[0,1] = 0 4、 通过一个numpy数组设置一组新值：1In [50]: df.loc[:,'D'] = np.array([5] * len(df)) 上述操作结果如下：123456789In [51]: dfOut[51]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 0.119209 5 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.02013-01-04 0.721555 -0.706771 -1.039575 5 3.02013-01-05 -0.424972 0.567020 0.276232 5 4.02013-01-06 -0.673690 0.113648 -1.478427 5 5.0 5、 通过where操作来设置新的值：12345678910111213In [52]: df2 = df.copy()In [53]: df2[df2 &gt; 0] = -df2In [54]: df2Out[54]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 -5 NaN2013-01-02 -1.212112 -0.173215 -0.119209 -5 -1.02013-01-03 -0.861849 -2.104569 -0.494929 -5 -2.02013-01-04 -0.721555 -0.706771 -1.039575 -5 -3.02013-01-05 -0.424972 -0.567020 -0.276232 -5 -4.02013-01-06 -0.673690 -0.113648 -1.478427 -5 -5.0 四、 缺失值处理在pandas中，使用np.nan来代替缺失值，这些值将默认不会包含在计算中，详情请参阅：Missing Data Section。1、 reindex()方法可以对指定轴上的索引进行改变/增加/删除操作，这将返回原始数据的一个拷贝：、1234567891011In [55]: df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])In [56]: df1.loc[dates[0]:dates[1],'E'] = 1In [57]: df1Out[57]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 NaN 1.02013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.0 NaN2013-01-04 0.721555 -0.706771 -1.039575 5 3.0 NaN 2、 去掉包含缺失值的行： 1234In [58]: df1.dropna(how='any')Out[58]: A B C D F E2013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.0 3、 对缺失值进行填充：1234567In [59]: df1.fillna(value=5)Out[59]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 5.0 1.02013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.0 5.02013-01-04 0.721555 -0.706771 -1.039575 5 3.0 5.0 4、 对数据进行布尔填充： 1234567In [60]: pd.isnull(df1)Out[60]: A B C D F E2013-01-01 False False False False True False2013-01-02 False False False False False False2013-01-03 False False False False False True2013-01-04 False False False False False True 五、 相关操作详情请参与 Basic Section On Binary Ops 统计（相关操作通常情况下不包括缺失值）1、 执行描述性统计：12345678In [61]: df.mean()Out[61]: A -0.004474B -0.383981C -0.687758D 5.000000F 3.000000dtype: float64 2、 在其他轴上进行相同的操作：123456789In [62]: df.mean(1)Out[62]: 2013-01-01 0.8727352013-01-02 1.4316212013-01-03 0.7077312013-01-04 1.3950422013-01-05 1.8836562013-01-06 1.592306Freq: D, dtype: float64 3、 对于拥有不同维度，需要对齐的对象进行操作。Pandas会自动的沿着指定的维度进行广播： 123456789101112131415161718192021In [63]: s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)In [64]: sOut[64]: 2013-01-01 NaN2013-01-02 NaN2013-01-03 1.02013-01-04 3.02013-01-05 5.02013-01-06 NaNFreq: D, dtype: float64In [65]: df.sub(s, axis='index')Out[65]: A B C D F2013-01-01 NaN NaN NaN NaN NaN2013-01-02 NaN NaN NaN NaN NaN2013-01-03 -1.861849 -3.104569 -1.494929 4.0 1.02013-01-04 -2.278445 -3.706771 -4.039575 2.0 0.02013-01-05 -5.424972 -4.432980 -4.723768 0.0 -1.02013-01-06 NaN NaN NaN NaN NaN Apply1、 对数据应用函数：123456789101112131415161718In [66]: df.apply(np.cumsum)Out[66]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 -1.389850 10 1.02013-01-03 0.350263 -2.277784 -1.884779 15 3.02013-01-04 1.071818 -2.984555 -2.924354 20 6.02013-01-05 0.646846 -2.417535 -2.648122 25 10.02013-01-06 -0.026844 -2.303886 -4.126549 30 15.0In [67]: df.apply(lambda x: x.max() - x.min())Out[67]: A 2.073961B 2.671590C 1.785291D 0.000000F 4.000000dtype: float64 直方图具体请参照：Histogramming and Discretization1234567891011121314151617181920212223In [68]: s = pd.Series(np.random.randint(0, 7, size=10))In [69]: sOut[69]: 0 41 22 13 24 65 46 47 68 49 4dtype: int64In [70]: s.value_counts()Out[70]: 4 56 22 21 1dtype: int64 字符串方法Series对象在其str属性中配备了一组字符串处理方法，可以很容易的应用到数组中的每个元素，如下段代码所示。更多详情请参考：Vectorized String Methods. 1234567891011121314In [71]: s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])In [72]: s.str.lower()Out[72]: 0 a1 b2 c3 aaba4 baca5 NaN6 caba7 dog8 catdtype: object 六、 合并Pandas提供了大量的方法能够轻松的对Series，DataFrame和Panel对象进行各种符合各种逻辑关系的合并操作。具体请参阅：Merging section Concat1234567891011121314151617181920212223242526272829303132In [73]: df = pd.DataFrame(np.random.randn(10, 4))In [74]: dfOut[74]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495# break it into piecesIn [75]: pieces = [df[:3], df[3:7], df[7:]]In [76]: pd.concat(pieces)Out[76]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495 Join类似于SQL类型的合并，具体请参阅：Database style joining1234567891011121314151617181920212223In [77]: left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;)In [78]: right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;)In [79]: leftOut[79]: key lval0 foo 11 foo 2In [80]: rightOut[80]: key rval0 foo 41 foo 5In [81]: pd.merge(left, right, on='key')Out[81]: key lval rval0 foo 1 41 foo 1 52 foo 2 43 foo 2 5 Append将一行连接到一个DataFrame上，具体请参阅 Appending：12345678910111213141516171819202122232425262728In [87]: df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])In [88]: dfOut[88]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.708758In [89]: s = df.iloc[3]In [90]: df.append(s, ignore_index=True)Out[90]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.7087588 1.453749 1.208843 -0.080952 -0.264610 七、 分组对于”group by”操作，我们通常是指以下一个或多个操作步骤： （Splitting）按照一些规则将数据分为不同的组； （Applying）对于每组数据分别执行一个函数； （Combining）将结果组合到一个数据结构中；详情请参阅：Grouping section12345678910111213141516171819In [91]: df = pd.DataFrame(&#123;'A' : ['foo', 'bar', 'foo', 'bar', ....: 'foo', 'bar', 'foo', 'foo'], ....: 'B' : ['one', 'one', 'two', 'three', ....: 'two', 'two', 'one', 'three'], ....: 'C' : np.random.randn(8), ....: 'D' : np.random.randn(8)&#125;) ....: In [92]: dfOut[92]: A B C D0 foo one -1.202872 -0.0552241 bar one -1.814470 2.3959852 foo two 1.018601 1.5528253 bar three -0.595447 0.1665994 foo two 1.395433 0.0476095 bar two -0.392670 -0.1364736 foo one 0.007207 -0.5617577 foo three 1.928123 -1.623033 1、 分组并对每个分组执行sum函数：123456In [93]: df.groupby('A').sum()Out[93]: C DA bar -2.802588 2.42611foo 3.146492 -0.63958 2、 通过多个列进行分组形成一个层次索引，然后执行函数： 12345678910In [94]: df.groupby(['A','B']).sum()Out[94]: C DA B bar one -1.814470 2.395985 three -0.595447 0.166599 two -0.392670 -0.136473foo one -1.195665 -0.616981 three 1.928123 -1.623033 two 2.414034 1.600434 八、 Reshaping详情请参阅 Hierarchical Indexing 和 Reshaping。 Stack1234567891011121314151617181920In [95]: tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])) In [96]: index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])In [97]: df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])In [98]: df2 = df[:4]In [99]: df2Out[99]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.100230 The stack() method “compresses” a level in the DataFrame’s columns. 1234567891011121314In [100]: stacked = df2.stack()In [101]: stackedOut[101]: first second bar one A 0.029399 B -0.542108 two A 0.282696 B -0.087302baz one A -1.575170 B 1.771208 two A 0.816482 B 1.100230dtype: float64 With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:1234567891011121314151617181920212223242526In [102]: stacked.unstack()Out[102]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.100230In [103]: stacked.unstack(1)Out[103]: second one twofirst bar A 0.029399 0.282696 B -0.542108 -0.087302baz A -1.575170 0.816482 B 1.771208 1.100230In [104]: stacked.unstack(0)Out[104]: first bar bazsecond one A 0.029399 -1.575170 B -0.542108 1.771208two A 0.282696 0.816482 B -0.087302 1.100230 数据透视表数据透视表详情请参阅：Pivot Tables. 123456789101112131415161718192021In [105]: df = pd.DataFrame(&#123;'A' : ['one', 'one', 'two', 'three'] * 3, 'B' : ['A', 'B', 'C'] * 4, 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, 'D' : np.random.randn(12), 'E' : np.random.randn(12)&#125;) In [106]: dfOut[106]: A B C D E0 one A foo 1.418757 -0.1796661 one B foo -1.879024 1.2918362 two C foo 0.536826 -0.0096143 three A bar 1.006160 0.3921494 one B bar -0.029716 0.2645995 one C bar -1.146178 -0.0574096 two A foo 0.100900 -1.4256387 three B foo -1.035018 1.0240988 one C foo 0.314665 -0.1060629 one A bar -0.773723 1.82437510 two B bar -1.170653 0.59597411 three C bar 0.648740 1.167115 可以从这个数据中轻松的生成数据透视表： 12345678910111213In [107]: pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])Out[107]: C bar fooA B one A -0.773723 1.418757 B -0.029716 -1.879024 C -1.146178 0.314665three A 1.006160 NaN B NaN -1.035018 C 0.648740 NaNtwo A NaN 0.100900 B -1.170653 NaN C NaN 0.536826 九、 时间序列Pandas在对频率转换进行重新采样时拥有简单、强大且高效的功能（如将按秒采样的数据转换为按5分钟为单位进行采样的数据）。这种操作在金融领域非常常见。具体参考：Time Series section。12345678In [108]: rng = pd.date_range('1/1/2012', periods=100, freq='S')In [109]: ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)In [110]: ts.resample('5Min').sum()Out[110]: 2012-01-01 25083Freq: 5T, dtype: int64 1、 时区表示： 1234567891011121314151617181920212223In [111]: rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')In [112]: ts = pd.Series(np.random.randn(len(rng)), rng)In [113]: tsOut[113]: 2012-03-06 0.4640002012-03-07 0.2273712012-03-08 -0.4969222012-03-09 0.3063892012-03-10 -2.290613Freq: D, dtype: float64In [114]: ts_utc = ts.tz_localize('UTC')In [115]: ts_utcOut[115]: 2012-03-06 00:00:00+00:00 0.4640002012-03-07 00:00:00+00:00 0.2273712012-03-08 00:00:00+00:00 -0.4969222012-03-09 00:00:00+00:00 0.3063892012-03-10 00:00:00+00:00 -2.290613Freq: D, dtype: float64 2、 时区转换：12345678In [116]: ts_utc.tz_convert('US/Eastern')Out[116]: 2012-03-05 19:00:00-05:00 0.4640002012-03-06 19:00:00-05:00 0.2273712012-03-07 19:00:00-05:00 -0.4969222012-03-08 19:00:00-05:00 0.3063892012-03-09 19:00:00-05:00 -2.290613Freq: D, dtype: float64 3、 时间跨度转换： 1234567891011121314151617181920212223242526272829303132In [117]: rng = pd.date_range('1/1/2012', periods=5, freq='M')In [118]: ts = pd.Series(np.random.randn(len(rng)), index=rng)In [119]: tsOut[119]: 2012-01-31 -1.1346232012-02-29 -1.5618192012-03-31 -0.2608382012-04-30 0.2819572012-05-31 1.523962Freq: M, dtype: float64In [120]: ps = ts.to_period()In [121]: psOut[121]: 2012-01 -1.1346232012-02 -1.5618192012-03 -0.2608382012-04 0.2819572012-05 1.523962Freq: M, dtype: float64In [122]: ps.to_timestamp()Out[122]: 2012-01-01 -1.1346232012-02-01 -1.5618192012-03-01 -0.2608382012-04-01 0.2819572012-05-01 1.523962Freq: MS, dtype: float64 十、 Categorical从0.15版本开始，pandas可以在DataFrame中支持Categorical类型的数据，详细 介绍参看：categorical introduction和API documentation1In [127]: df = pd.DataFrame(&#123;\"id\":[1,2,3,4,5,6], \"raw_grade\":['a', 'b', 'b', 'a', 'a', 'e']&#125;) 1、 将原始的grade转换为Categorical数据类型： 123456789101112In [128]: df[\"grade\"] = df[\"raw_grade\"].astype(\"category\")In [129]: df[\"grade\"]Out[129]: 0 a1 b2 b3 a4 a5 eName: grade, dtype: categoryCategories (3, object): [a, b, e] 2、 将Categorical类型数据重命名为更有意义的名称：1In [130]: df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"] 3、 对类别进行重新排序，增加缺失的类别：123456789101112In [131]: df[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"])In [132]: df[\"grade\"]Out[132]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (5, object): [very bad, bad, medium, good, very good] 4、 排序是按照Categorical的顺序进行的而不是按照字典顺序进行： 123456789In [133]: df.sort_values(by=\"grade\")Out[133]: id raw_grade grade5 6 e very bad1 2 b good2 3 b good0 1 a very good3 4 a very good4 5 a very good 5、 对Categorical列进行排序时存在空的类别： 123456789In [134]: df.groupby(\"grade\").size()Out[134]: gradevery bad 1bad 0medium 0good 2very good 3dtype: int64 十一、 画图具体文档参看：Plotting docs123456In [135]: ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))In [136]: ts = ts.cumsum()In [137]: ts.plot()Out[137]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff2ab2af550&gt; 对于DataFrame来说，plot是一种将所有列及其标签进行绘制的简便方法： 12345678In [138]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, .....: columns=['A', 'B', 'C', 'D']) .....: In [139]: df = df.cumsum()In [140]: plt.figure(); df.plot(); plt.legend(loc='best')Out[140]: &lt;matplotlib.legend.Legend at 0x7ff29c8163d0&gt; 十二、 导入和保存数据CSV操作CSV参考：Writing to a csv file1、 写入csv文件：1In [141]: df.to_csv('foo.csv') 2、 从csv文件中读取： 1234567891011121314151617181920In [142]: pd.read_csv('foo.csv')Out[142]: Unnamed: 0 A B C D0 2000-01-01 0.266457 -0.399641 -0.219582 1.1868601 2000-01-02 -1.170732 -0.345873 1.653061 -0.2829532 2000-01-03 -1.734933 0.530468 2.060811 -0.5155363 2000-01-04 -1.555121 1.452620 0.239859 -1.1568964 2000-01-05 0.578117 0.511371 0.103552 -2.4282025 2000-01-06 0.478344 0.449933 -0.741620 -1.9624096 2000-01-07 1.235339 -0.091757 -1.543861 -1.084753.. ... ... ... ... ...993 2002-09-20 -10.628548 -9.153563 -7.883146 28.313940994 2002-09-21 -10.390377 -8.727491 -6.399645 30.914107995 2002-09-22 -8.985362 -8.485624 -4.669462 31.367740996 2002-09-23 -9.558560 -8.781216 -4.499815 30.518439997 2002-09-24 -9.902058 -9.340490 -4.386639 30.105593998 2002-09-25 -10.216020 -9.480682 -3.933802 29.758560999 2002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 5 columns] HDF5HDF5，参考：HDFStores1、 写入HDF5存储： 1In [143]: df.to_hdf('foo.h5','df') 2、 从HDF5存储中读取： 1234567891011121314151617181920In [144]: pd.read_hdf('foo.h5','df')Out[144]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 4 columns] ExcelExcel，参考：MS Excel 1、 写入excel文件：1In [145]: df.to_excel('foo.xlsx', sheet_name='Sheet1') 2、 从excel文件中读取：1234567891011121314151617181920In [146]: pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])Out[146]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 4 columns] 12 12 12 12 12","categories":[],"tags":[{"name":"十分钟搞定Pandas","slug":"十分钟搞定Pandas","permalink":"http://yoursite.com/tags/十分钟搞定Pandas/"}]},{"title":"Rocky English","slug":"Rocky-English","date":"2017-04-04T14:04:48.000Z","updated":"2017-04-05T09:25:05.370Z","comments":true,"path":"2017/04/04/Rocky-English/","link":"","permalink":"http://yoursite.com/2017/04/04/Rocky-English/","excerpt":"we are a team .","text":"we are a team . we are a teamYour browser does not support the audio element. 1 We’re a team. We struggle together.We triumph together.If one of us fails, we all fail.If one of us breaks a ruleWe all should be punished.If one of us lags behind,We all should help him.If one of us succeeds,We all should celebrateWe’re a great team. 我们是一支团队 我们是一支团队。我们一起奋斗。我们一起取得胜利。如果我们中有一个人失败了，那么我们大家都会失败如果我们中有一个人违反了规则。那我们大家都要受到惩罚。如果我们中有一个人落后了，我们大家都要帮助他如果我们中有一个人成功了，我们大家就会一起庆祝我们是一支伟大的团队。 2 .We care about each other We encourage each otherWe push each otherWe love each otherWe share good times and bad times.We’re a great team 2 我们互相关心。我们相互鼓励。我们相互推动。我们彼此关爱我们甘苦与共。我们是一支伟大的团队。 3 .We fight for honorWe strive for excellenceWe aim for success 3．我们为荣誉而战我们为卓越而奋斗我们力争成功 4 We think like winners.We act like winnersWe will be winnersWe believe in our bright futureWe’re a great team 4．我们像赢家一样思考 我们像赢家一样行动我们将会成为赢家我们相信我们的前途光明我们是一支伟大的团队","categories":[],"tags":[{"name":"英语","slug":"英语","permalink":"http://yoursite.com/tags/英语/"}]},{"title":"正念的奇迹","slug":"正念的奇迹","date":"2017-04-03T12:10:18.000Z","updated":"2017-04-03T18:23:27.118Z","comments":true,"path":"2017/04/03/正念的奇迹/","link":"","permalink":"http://yoursite.com/2017/04/03/正念的奇迹/","excerpt":"生活即是禅修,感恩度过每一天。","text":"生活即是禅修,感恩度过每一天。 推荐序 生活即是禅修 陈琴富 导 读 持续灵光一闪的顿悟 自鼐法师 英文版译序 这本书本身就是奇迹 摩比．侯 缘 起 以慈悲之眼观照的一行禅师 詹姆斯．佛斯特 一、日常生活中的正念 二、在大地上行走就是奇迹 三、正念日 四、鹅卵石 五、一即一切，一切即一：五蕴 六、你前院的杏树 七、三个绝妙答案 八、正念的练习 附 录 佛经选读 推荐序生活即是禅修 陈琴富 学僧有源问大珠慧海禅师：「和尚最近怎么用功？」大珠答道：「饥来吃饭困来眠。」「平常人不也吃饭睡觉？这也叫修行？」大珠禅师说：「平常人吃饭时千般计较，不肯吃饭；睡觉时百般思索，不肯睡觉。」悟道的禅师和凡人一样要吃饭睡觉，只是他「吃饭的时候吃饭，睡觉的时候睡觉」，这就是禅──真正的活在当下。 如何由白日梦中醒来 一般人平日生活忙碌，心除了随着境团团转之外，根本不知道自己的「觉性」何在。回想昨天白天所过的日子和夜晚所做的梦有何差别？都是如梦如幻，无二无别，所以一般人所过的生活无异于白日梦；一个没有「觉性」的人，就像是拖着死尸的梦中人。 我们应该怎样在白日梦中醒过来，成为一个「觉者」呢？这就要靠佛陀教导的「安那般那念」以及「四念处」，也就是一行禅师所说的「正念」。正念不仅能让我们从醉生梦死的轮回中警醒，也能让我们在行住坐卧之间充满喜乐。这就是「正念的奇迹」。 一行禅师在1995年春天到台湾来弘法，我有幸亲炙他的法教，他教导我们先要如何掌握自己的呼吸，把它运用在行住坐卧之间，让日常生活与修行融合为一，透过观照呼吸提起我们的觉性，在一动一 静之间都能够念念分明。更深入的可以透过觉性的增强，观照到觉受、心念乃至于诸法，进入四念处──观身不净、观受是苦、观心无常、观法无我，通达人生与宇宙的实相。 佛陀在涅盘之前，弟子奔走呼号，最后阿难尊者收起眼泪代替大众问法：「佛陀入灭后，我们将以何为师？」佛陀说：「以戒为师。」阿难又问：「佛陀入灭后，我们将依何而住？」佛陀说：「依四念处而住。」可以知道四念处在佛陀心目中的重要性。南传佛教几乎是以四念处作为修行的法要。 佛陀在《大念处经》的开场白即说：「比丘们，只有一条道路可以使众生清净、克服愁叹、灭除苦忧、获得正道、体证涅盘，这条道路就是四念处。」依「安念般那念」修习「四念处」可以具足四禅八定住、可以具足慈悲喜舍住、可以得六种神通力、证阿罗汉果。这虽然是简单的方法，也是通达究竟的方法。 简单却究竟的方法 一行禅师以他最单纯的心境，把观呼吸和四念处的法义，透过简洁易懂的文字，让我们能在日常生活中运用自如。如果我们懂得在每一个当下提起正念、念念分明，我们就不会有烦恼，而且可以安住当下，内心中流泄出平静的自在和喜乐。 很多人把修行和生活分开，修行的时候坐在静室里，不让旁人打扰，生活的时候又搅在烦恼痛苦之中，如此硬生生地将修行和生活切 成两半，这是完全不懂得修行的法要。一行禅师明白地告诉我们，生命只在念念分明的此时此刻，心念离开当下就是拖死尸的人、离开觉性也等于是梦中人。 在本书中，一行禅师不厌其烦的教导我们在行住坐卧间禅修的方法，如何从观照一草一木中回到当下、如何在喝茶洗碗间提起觉性，如果能深化这样的修行，在每一个当下当具足正念，这就是大珠和尚「吃饭时吃饭，睡觉时睡觉」的境界了。愿有缘读到此书者能活在当下、身心自在。 （本文作者现职为《中时晚报》执行副总编辑） 导 读持续灵光一闪的顿悟 释自鼐 一个午后，微风轻柔地从临时搭架的棚帐吹进如来禅院的厨房，我决定问这里的越南禅修者──琳静，一个几年来一直萦绕在心里、感触颇深的问题：「是什么支持你能持续地来这个禅院煮饭，提供餐饮给这些参加每两周一次禅修的人？」 「你是第二位问我这个问题的人。」她从容地回答：「这个越南禅院在加州刚成立时，大家轮流煮。后来我发现有些人因负责厨房而感到干扰禅修；或者借着帮忙厨房的工作，而不去禅堂。发现这些情形后，我便决定由我固定负责厨房，让每个人都可以专心禅修。」 沉吟了一会儿，她平静地继续说：「虽然我因为负责厨房，禅修的时间比别人少。但每次我煮完后，常常内心充满『喜』，所以当我进禅堂打坐时，便很容易地进入，而能保持正念及专注。」 我内心不禁为之一震；我相信这是她多年来既深入地了解佛法，又活用到实际生活中的结果。同时内心有很深的感动：在她那不带一点自我色彩的描述中，我知道这是因为她精确地掌握了培养禅修的要素。更重要的是，我也观察到她不是只有在禅堂禅修，当她在厨房煮饭的时候，总是安详保持正念地在做事。 结束和琳静的对谈后，我在心中告诉自己，要把她的故事带回台 湾。她的体验厘清一个「入世佛教」的课题：入世人间服务和出世解脱烦恼如何不冲突？从琳静的经验中我学到的是：她将服务奉献的历程视为是增益个人禅修时，入世与解脱就是相辅相成，一体的两面。 这样的课题，在中国禅门的训诲及高僧传记上，已有许多的解释及例证。然而，在当代佛教，能充分地揭示如何以禅修，运用在日常生活及社会服务的理论架构及实践步骤并不多见。 越南籍的一行禅师以「正念禅」提供越战中、战后受难者及参战国家人民一个疗愈及依归的途径。他所带领的「入世佛教」（EngagedBuddhism）基本上是来自他一向的悲愿：让现代人能在今世便得到佛法的利益──现世法乐，让一个社会能因更多人保持正念禅的修持而共同建设一个和平的社群。 基于这个愿景，他对佛法的诠释便不局限在一个传统、一个门户，而是以他个人的体证为基础，揉合了南北传禅法，并以诗意的描述方式，引导他人如何欢喜地直接进入修持为目标。 这本在1975年出版一行禅师所著作的《正念的奇迹》原是一封给越战期间，社会服务青年学校广兄的信函，这封信旨在提供收信的人，如何将一行禅师的教法应用在自己的处境中。它并不完全是教导，而是一种经验分享，分享一行禅师如何将这些方法，应用在他所领导的小区中。 因此，在他的字里行间，学派传统不被提及，有的只是经典名称，也没有太多的佛学专有术语。让人读来贴切感动的，是他温厚直接的 观察，没有价值判断和道德训示，而是对现象的可改进处提供一套细腻贴切的方法。 这种禅风呼应出严峻禅门中的温厚慈悲，但却没有落入任何形式。相反地，修行是每个人的日常生活：洗碗、喝茶，走路，陪太太孩子及和别人谈话。禅修可以无所不在，出禅堂、下坐后，正是用功的好时机。藉由正念禅，日常生活的点滴不再是琐碎无意义的细节枝末，而是通往「奇迹」的入手处。 这本书是一行法师的早期著作，文末附加上两篇初期佛教有关禅修方面的经典《正念经》和《大念处经》，及三篇大乘佛教的经典《学处集要》、《维摩诘所说经》和《心经》。这些经文的附加并不是一个随性的安排，笔者认为是一行禅师意图整合初期及大乘佛教的禅法，可视为是禅师「正念禅」的雏型。本书共分七章，以下是针对各章内容，做分析性的介绍。 本书的开场，是藉由和一位禅师的好友亚伦的家常对话中展开。借着这位年轻父亲的体悟，一行禅师便埋下了两个伏笔： 第一，解决生命的困顿是自「个人的日常生活」中学到的，不是从书本上。 第二，是无限世界的切入，是来自专注地融入他者的世界；因而能消融他人的世界和自己的世界的界线。既能清楚地付出，同时又能超越「给予这件事」、「能给的这个人」及「受惠的人」。 上述两点，是亚伦在现实婚姻的困顿中，所顿悟的一个解答。 第二章则是一行禅师对亚伦的顿悟，做进一步的检证：如果亚伦不能一直醒觉地记得理智上所认识到的「自他不二」的原则，则当亚伦的「自我」意识生起时，因「自我感」所产生的冲突及压迫则仍会存在。 理智上的了解是概念的产物，它不等同已发生过的或正在发生的身心活动。如何不落入推理思辨的泥沼、又能在每一个当下持续灵光一闪的顿悟，是一行禅师要立的论点；也是各个宗教精神修养传统共同的技术问题。禅师所提供的方法是：从最初透过知觉、观察呼吸，来培养时时刻刻醒觉的心。这时时刻刻醒觉的心，在最初阶段是在培养「正念」，进阶则是「观智」。 「念」，是我们时时刻刻在运用的一个中性的心理活动。它基本上就像一栋大楼的守卫，他知道有人进来，但并不需要最更进一步的判断、分析，或是交涉任何事情。如向智尊者所分析的，和情绪、意志、想象等心里活动相比，「念」在我们一串极快速的心理活动中，是十分不起眼，极微不足道。 但是，如果我们能让这纯粹觉知的「念」持续地工作：保持听只是听，看只是看，不对所知觉、所思考的对象做任何附加上的想象。由此，心便能渐渐地脱离一向的惯性反应模式。如向智尊者所比喻的，「『念』最后显现为『阿基米得点』，在这支点上撬开了世间大量循环不已的『苦』的两个停泊地──无明与渴爱。」但这是一个历程，需有基础，也就是专注平稳的心。时刻保持知觉，观察呼吸的出入是最 容易入手的方法。 呼吸和生理及心理的活动息息相关，从呼吸的粗细，急缓也可反呈心理的状况。藉由不同的方法，让心能够单纯地观察呼吸，进而开发心的洞察力。这是释迦牟尼佛所发展出来的一套系统方法。 《安般守意经》（Anapanasati Sutta）是一部完整地介绍从最初观察呼吸的出入，到彻底开悟的佛经。正念呼吸是Anapanasati的意译，是以呼吸做为心可以时时刻刻回归的依靠。就像一头牛被大绳系绑在一根大柱子上：时刻警觉的心就像大绳，将心安住在呼吸的大柱子。 从培养全然地专注呼吸到开悟共有十六种方法，在依照身体、感受、心及法分成四组。第二、三章内容主要，是根据第一组的禅修法，但一行禅师也加入个人体证所发展的方法。循序渐进地介绍以观呼吸来培养「正念」。 然而伴随着要透过「文字概念」要传达「非概念」的意旨时，禅师自己也落入文字概念的范畴，任何他所要教导的内容都有可能只是概念，并不一定能引导读者进入直接禅修的操练。 为避免因使用文字而导致读者只局限在文字层次，一行禅师大量使用如诗般的情境描述法，以避开传统注释书的分析法，或是依经文逐字解释可能带来的障碍：只停留在阅读文字的阶段（所谓「闻」的阶段），不懂实际操练（「修」的阶段）。 一行禅师发展出如诗般的禅修的「操作」语法，使阅读他文章的读者，可以跟着随他的文字去练习。禅师不落入「文字障」的坚持，在生活中持续地以呼吸来保持正念。将每一个生活细节琐碎的工作都视为是通往修练正念的场所。因为透过全然专注，当下生命的尊严；不可取代的神奇便在其中。 进入第四章，正念的观照包括更细微的对象：觉受及心念。一行禅师并未分别地探讨两者修法的不同，便将两者归属心念处的修法。他所着重的是如何「歇心」：借着身体的放松，单纯地观看各种觉受、情绪、想法；让心平静地观照心，像站在桥上看桥下的流水般地看。持续不断直接且持续地观察心，渐渐地从见自心到见自性。到达见性的境界，则是主、客消融，真、妄一如。 为说明两者境界的不同，一行禅师引用两个明喻：猴子及影子；来显示初阶修练以心观心时，此时心仍需要刻意地去观心。「一旦心能直接且持续地觉知它自己」，心念处的功夫便进入另一阶段的境界：「并没有两颗心」，不会落入自我中心所建构的对立分别的世界。不会「被孤立个体这样的虚妄分别所分割。」 这简要的说明，有助于我们了解如何契入「一味，不二」的境界：是透过持续地「观心」所体会的悟境；而非经由解释或是想象的概念产物。 延续第四章观察自己的心念及心性的基础，第五章则是教导读者进一步地观察心和心念的对象如何地相依相生：没有一个认识对象和能认识的心是绝缘无关的。换言之，只要有认识的活动就有被认知的 对象。基于此「因缘观」的体认，一行禅师明确地请读者远离哲学推理的思维，而直接观察自己的身心「五蕴（色受想行识）」。然而一行禅师并未根据《大念处经》中法念处的五蕴观的方法。此经主要以观各个蕴的生灭，并非以无常、苦、无我等诸蕴的共同特性做为观照的对象。 从第五章开始，一行禅师所提出的观法很明显地是更接近《般若波罗密多心经》的空观世界：一个不落入以语言、概念所界定的二元世界；一个经持续不断地观察心念纯熟后，所乍现的世界。 因此切入直观世界的途径是有其步骤，首先从观察个人的心念，接着是要个人身心和外在宇宙的相依关系开始观照，直到能观到：「你的生命和宇宙的生命本为一个整体……这个世界每一秒都在滋养着五蕴。自我就是五蕴的和合。」根植于此种体悟，方能透澈地了悟：生死是一体两面，进而能活出同体大悲的精神。 第六章，为更清楚地证明因缘观如何能引领入禅修者真实世界，一行禅师特别探讨大乘唯识学中宇宙万法的三种特性：遍计所执、依他起、及圆成实。再次地，他强调由禅修的观照切入依他起之本性，进而摧破一向被坚固地认为是具有实质的虚妄自我。 同时，为不执着这个立场的强调，他又以一个生活中的不二经验，呈显因果同时不二的洞察。当行文到一个如此深刻及高妙的境界，一行禅师赶紧在第七章引入一个宗教故事：一位禅修者无论志向、境界多高远，都不忘照顾周围的人。 此书中所描述的美妙超然的空性、不二的境界并不是一种想象，而是一行禅师个人为解决个人的痛苦，为高深的佛法能在现世中便能让他人得到利益，所提出的禅法。一般初学者要在短期内，仅仅靠读书面的字，是无法马上达到上述的意境。 我们如果沉静地检视一下自己活过的经验，短期内要达到上述的意境，事实上也是需要不断地练习。果真如此，此书的内容岂不成画饼充饥？或者只是一种禅诗意境的欣赏？更清楚地说是：要如何在日常生活中化平凡庸碌为神奇？难易与否就在读者的切入角度。 笔者建议读者把此书当成「心灵操练手册」把例子当成动画示范，时时刻刻都拿来用，只因禅观的修练就像练习任何一种技术；例如学开车，唯有透过实际地操作练习，神奇自然在心头！ （本文作者为美国加州整合学院文学博士，现职为香光尼众佛学院讲师） 英译者序这本书本身就是奇迹 摩比.侯（Mobi Ho） 《正念的奇迹》原是以越南文写成的长信，写给广兄，一位1974年在南越的社会服务青年学校的主要成员。一行禅师在1960年代推动「入世佛教」，设立了这间学校，引导年轻人以慈悲精神投身社会运动。毕业后，学生运用所受的训练，帮助在战争的骚动中被捕的农民。他们帮忙重建遭轰炸的村庄，教导孩童，设立医药站，还组织农业合作社。 在战争引起的恐惧与不信任的气氛中，这些工作者的调停方式常被误解。他们不愿支持任何一个武装政党，他们认为两边都仅呈现单面真相；他们相信真正的敌人不是人，而是意识型态、憎恨与无知。但是这样的立场，威胁到那些卷入这场混战的人。因此，在青年学校设立初年，学生遭到一连串攻击，还有数人被绑架、谋杀。战争不断拖延，即使在1973年巴黎和平协和条约签订之后也还是如此；不因困顿和苦厄而屈服，有时似乎是不可能的。 以爱与宽容的精神工作，确实需要很大的勇气。 再困厄也要保持正念 一行禅师在被越南放逐、停留于法国时，写信给广兄，好在这黑暗时期鼓励他们。一行禅师希望能提醒他们那最基本的守则︰即使在 最困苦的时候，也要随顺自己的呼吸来培育并维持平静的正念。由于广兄和学生们，既是他的同僚也是他的朋友，这封最后成了《正念的奇迹》一书的长信，显得私密而直接。当一行禅师说到村落小径时，他说的是他和广兄共同走过的那条小径；当他提到孩子明亮的眼眸时，他说的是广兄的孩子。 老师写这封信时，我正以美国志工的身份与越南佛教和平代表团一起待在巴黎。代表团由老师领导，可说是集结越南佛教徒对和平与重建之努力（包括社会服务青年学校）的海外联络工作处。我记得那些深夜，在喝茶后、老师对代表团成员和一些密友解释信中的篇章。那时，很自然地，我们会想到，在其它国家的其它人也可能从这本书所描述的修习中获益。 老师那时与泰国的年轻僧侣渐渐熟悉起来，他们也深受越南入世佛教的见证鼓舞。他们很希望以觉知与调停的精神行动，以避免泰国发生武装冲突，并且想知道怎么样才能不被愤怒与气馁击倒。他们之中有几人会说英文，我们因而讨论起翻译广兄这封书信的事。翻译这想法说来其实触痛我们的伤处，由于在越南的佛教出版室已被没收，因此无法在越南将这封信印成一本小书。 我很高兴地接下翻译这本书的英文版的任务。最近三年来，我都和越南佛教和平代表团在一起，日夜沉浸于越南语的韵律中。一行禅师已经成了我「正式的」越南语老师；我们逐句逐句地、缓慢地读完了他较早的著作，我还因此知道了许多不常见的越南佛教词汇。当然，老师在那三年教我的，远远超过语言这件事。他的存在本身，即能恒久而温柔地提醒我们重返真我，并保持正念分明而达致觉悟。 连结世上众生的媒介 当我坐下来翻译《正念的奇迹》时，我记起过去那养成我正念之修持的岁月片段。有一次，我正手忙脚乱地炒菜，偏偏又找不着一只原本放在胡乱堆栈的锅子与材料间的汤匙，在我忙着四处搜寻时，老师走进厨房，笑了。他问︰「摩比在找什么？」理所当然地，我回答︰「汤匙！我在找汤匙！」老师再次带着微笑，回答道︰「不，摩比在找摩比！」 老师建议我缓慢且稳定地翻译，好维持正念。我一天只翻译两页。在夜间，老师和我仔细检视文页，修改并校正字汇和文句。其它朋友则提供编辑上的协助。很难确切描述翻译老师所言的经验，但我对笔和纸的感觉的觉察、对己身姿势和呼吸的觉察，使我能尽可能清楚透澈地洞悉正念，那正是老师写下每一字时所怀抱着的。当我注意我的呼吸时，我能看到广兄和社会服务青年学校的工作者。更甚地，我开始看到每一字、每一句，对任何读者都展现出同样的私密与直接坦率，因为它们都是在正念中写就，且充满钟爱地提到那些确实存在的人物。当我继续翻译，我看到愈来愈多的社群──学校的工作者、年轻的泰国僧侣，还有世上许多其它的朋友。 翻译完成、我们也打好字后，老师用塞在代表团盥洗室的小型胶印机印了一百份。在信封上正念分明地写下多国朋友的姓名地址，对代表团的成员来说，真是件快乐的工作。 从那时起，《正念的奇迹》就像池上涟漪，愈扩愈远。它已经被 译成数种语言，在世界各地印刷销售。身为译者的喜悦之一，乃是听到许多人都发现了这本书。有一次，我在书店遇到一个人，他提到有个学生带了这本书给在苏联的朋友们。最近，我遇到一位正处于被祖国驱逐之险境的伊拉克学生，在伊拉克，他因为拒绝加入他认为是残酷无意义的战争，而面临死亡威胁。他和他母亲都读了《正念的奇迹》，正在修习观呼吸。我还知道，葡语版正被用来帮助巴西的贫童。囚犯、难民、医护人员、教育人员与艺术家，都曾被这本小书感动过。我常觉得《正念的奇迹》本身就是个奇迹，是继续连结世上众生的媒介。 清楚而简明的修持方法 本书所描述的佛教传统是越南佛教，也就是融合上座部与大乘佛教所形成的独特传统，美国佛教徒对此印象深刻。以一本讲述佛法的书来看，《正念的奇迹》的特别之处在于︰清楚而简明地强调基本修持方法，让所有读者都能立即开始练习。然而，本书所关注的对象不只限于佛教徒，而是为了庇护所有不同宗教传统的人。毕竟，呼吸这件事很难只归属于某个教派传统。 喜欢这本书的人，大概也会对一行禅师的其它作品感兴趣。他的越南文作品，包括了短篇故事、小说、散文、佛教历史性论述及诗，可说著作等身。部分早期作品的英文版已绝版，但近作如《行禅指南》（A Guide to Walking Meditation）、《活得安详》（Being Peace）及《太阳，我的心》（The Sun My Heart）都还找得到。 由于返回越南一事遭拒，今年一行禅师大都待在梅村这个他在法 国设立的小区。那里，在《正念的奇迹》多年前的原稿收信人广兄指导下，小区成员植了数百棵梅树。卖梅子的收入，都拿来帮助越南饥饿的孩童。此外，梅村每年夏天都开放给来自世界各地的访客，进行一个月的正念与禅修修习。近年来，一行禅师也都每年定期访问美国和加拿大，指导由佛教和平联会组织的一周禅修会。 我要特别感谢Beacon Press有慧见地出版新版的《正念的奇迹》。我希望本书所接触到的每一位新读者都能感到这本书是特别为他或她所写的，就像广兄和社会服务青年学校的工作者所感到的一样。 摩比.侯，1987年8月 缘 起以慈悲之眼观照的一行禅师 詹姆斯.佛斯特（James Forest） 1968年，我和一行禅师跟着调停委员会（Fellowship ofReconciliation）旅行，我们沿途与教会、学生团体、参议员、新闻记者、教授、企业人士和（谢天谢地，终于可以休息一下）一些诗人会谈。这位穿着棕色僧袍的越南僧侣（看起来比多年前他还四十多岁时还要年轻）几乎在每个地方都很快就让所有见到他的人信服。 一行禅师的温文儒雅和明智聪慧，几乎让每个遇到他的人都抛弃了「越南人该像什么样子」的刻板印象。他所说的故事、所做的开示，满溢着越南人和佛教徒浩瀚的宝藏。他对基督教的兴趣，甚至怀有宗教热忱，感召了基督教徒也开始包容一行禅师所代表的佛教传统。他促使数以千计的美国人以不同角度看待越战，透过在竹林树丛环绕的传统农庄中耕作维生并抚育子孙的农民的眼睛来看这场战争。当他描述村中风筝工匠的手艺，以及这些看似脆弱的飞船一旦直上云霄就能迎风呼啸时，成人心中隐藏的孩童莫不被唤醒。 只要和他在一起一个小时，人们莫不震慑于越南的美，并对美国介入越南人民在政治和文化的苦难一事，充满愤怒。人们将不再对战争中的两造任何一方的意识型态死忠拥护，对战争的种种感到恐惧，因为战争带来的是：轰炸机扫过天际、房屋和人们被烧成灰烬、孩子们得独自面对父母祖辈的疼爱永远缺席的生命。 然而有个晚上，有一个美国人不但无法理解一行禅师的震耳明钟，还因而燃起巨大怒火。当时一行禅师正在美国圣路易郊区一座华贵的基督教教堂演讲，像往常一样，他强调美国人必须停止在越南的轰炸和杀戮。一个高大的男士站起来，苛刻地嘲讽道：「这位一行先生所认为的慈悲……」，他的话引发了一连串的质疑和答复。 「如果你这么关心你的同胞，一行先生，你为什么在这儿？如果你这么关心那些受伤的人，你为什么不把时间花在他们身上？」在提笔的此刻，回忆起当时，那位男士让我不知所措的盛怒，都还压过我对他话语的记忆。 当那位男士说完之后，我困惑地看向一行禅师，心想他或其它任何人会说些什么？剎那间，战争的幽灵充满室内，令人透不过气来。 满室寂静。然后，一行禅师开始说话了，他那种极具穿透力的安详宁谧，明显地怀着对那刚刚才咒骂他的男人的关照。他的话就像大火中的甘霖。 「如果你希望树生长，」他说，「把水浇在叶子上是徒劳无功的。你必须灌溉的是树根。而这场战争的大多数根源都在这儿，在你的国家。要帮助那些被轰炸的人，要试着保护他们不再受苦，我必须来这儿。」 室内的气氛改变了。在这男人的怒焰中，我们经验了我们自己的愤怒；我们是透过一个被轰炸的半岛来看这世界的。 然而，在一行禅师的答复中，我们经验了另一种可能性︰用慈爱来克服瞋恨的可能性（由一个佛教徒带给我们这些基督徒；由一个美国人的「敌人」，带给我们这些美国人），中断人类历史上似乎永无止尽的暴力连锁反应的可能性。 但在做完回应后，一行禅师对主席低声说了些什么，就快步离开讲厅。我觉得有些不对劲，就跟着他走出去。那一夜冷冽清朗。一行禅师站在教堂停车场旁边的人行道，就快喘不过气来──就像一个潜入深水而无法浮到水面上换气的人。过了几分钟后，我才敢问他怎么了，究竟发生了什么事。 一行禅师解释说，那位男士的说法让他极度心烦意乱。他也想以愤怒反击，所以他必须尽可能缓缓地深深吸一口气，好让自己冷静下来，好包容地回应。但那口呼吸太缓也太深了。 「为什么你不对他生气？」我问。「就算是反战人士也有权利生气。」 「假如这只是我个人的事，我是可以生气。但我在这里是为了越南农民讲话。我必须让这里的人看到我们怎样做才好。」 那是我生命中一个重要的时刻，一个从那时起让我一次又一次反覆思索的时刻。我第一次了解到人的呼吸方式和他对周遭世界的响应方式之间，有着某种关联。 直到最近，一行禅师才试着教导西方人禅修的方法，也就是他通 常称之为「正念」的方法。这还只是去年的事，先是在巴黎与一些西方朋友帮助越南佛教和平代表团时，以及稍后与某个城市的社团在当地的基督教贵格会国际中心，才开始教授禅修方法。现在他终于写下这个禅修主题的小书《正念的奇迹》，一本用来禅修的手册。 一行禅师是位诗人、禅师，也是调停委员会的联合主席。在越南，他积极参与推动「入世佛教」，这是一个根植于慈悲与服务且意义深远的宗教改革，规画过无数帮助战争受难者并以非暴力反战的计划。为了这个工作，数千佛教徒包括尼师、比丘与在家居士因而被射杀或囚禁。 他在越南的工作，催生了社会服务青年学校、万行大学（VanHanh University）、一座非暴力运动初期基地的小寺院、一份反战的地下刊物（由他的同伴高玉芳［Cao Ngoc Phuong］主办），以及致力文化宗教改革的主要媒介──锦囊出版社（the La Boi Press）。 他的诗成为许多当代越南最受欢迎的歌曲的歌词，都是在悲伤中仍然吟咏希望的歌。 即使在被放逐时，他也在海外代表越南统一佛教团（UnifiedBuddhist Church of Vietnam），继续做为非暴力与支持越南停战的一股力量，并统筹来自其它国家的支持援助。（他与马丁.路德.金恩［Martin Luther King］的友谊，是金恩博士决定不理同僚与支持者反对「混淆黑人民权和反越战诉求议题」的建议，并加入反对越战的因素之一。就在金恩博士被刺杀前，他提名一行禅师为诺贝尔和平奖候选人。） 他的书在越南境外出版的只有少数几本（编按：这是1976年的状况，现在一行禅师的英文出版品数量颇丰，中文出版品也将陆续出版）︰《火海之莲》（Lotus in a Sea of Fire）、《越南的吶喊》（The Cryof Vietnam）、《步步安乐行》（The Path of Return Continues theJourney）、《禅之心钥》（Zen Keys）以及《渡筏非彼岸》（The Raft IsNot the Shore）。 在越南佛教和平代表团在巴黎的处所，我与一行禅师和他的同伴谈到，这么多美国和平运动中，都缺少禅修这个面向。禅修面向的缺席解释了为什么这么多「和平」运动（或最好称为美国撤军运动）都对佛教徒非暴力的反战活动兴趣缺缺。手无寸铁的佛教徒不被看做真正的「政治」行为，而仅仅被视为宗教运动︰是很值得敬佩，跟其它宗教运动比起来也可说相当勇敢，但究竟只是不重要的边缘运动。 美国和平运动者可以从越南弟兄身上学到的是，和平运动必须纳入更多的禅修面向，否则我们对真实的感知（以及进一步得以帮助人们理解事件并改变事情的能力）将会严重偏差。不管我们的宗教或非宗教的背景为何，不管我们说什么语言，我们都会忽略一些事，而这些事情却像呼吸一样，对我们的生活和工作都极为关键重要。 就是呼吸本身。呼吸。简单如专注呼吸之事，乃是禅修和祈祷的关键，但对许多人来说就像个惊人的讯息。它就像是悬疑小说家将钻石藏在金鱼缸里的点子一样，太明显以致没有人注意到。但是自从这个讯息成功地超越了我个人的怀疑主张，我就从此确信不疑了，而之所以如此确信主要是根据我的经验。 禅修的问题与生活太过密切。 就像一行禅师指出的，禅修的机会处处都是︰在浴缸里、在厨房水槽、在砧板上、在人行道或小径上、在上下楼阶梯上、在示威抗议队伍中、在打字机前……可说是无所不在。有寂静无声的时刻地点，当然最好最有益，但那并非不可或缺。 禅修生活并不需要像待在温室中一样隐居。（它确实需要一些特定的时刻，甚至得是一星期中的某一天，好在特别的关注培育下变得更正念分明。基督教徒和犹太教徒对这样的安息日应不陌生。） 对怀疑论者来说，一行禅师的建议可能相当荒谬不可行，只不过是历史终结时的一个烂笑话，是鬼扯「神秘学」这副旧牌所洗出的最后一次牌。但和平主义者选择在这残酷的世界继续养成生命，并且「手无寸铁地（非暴力）」过活的这种信念，不也让许多人震惊，那种荒谬不可行感并不亚于一行禅师的主张。 禅修的方法，只是将「解除个人武装」这个我们已经跨出的一大步踩得更深一点──不仅在面对政府、团体及解放军时坚持非暴力，更要以非暴力面对真实本身。 一行禅师曾在别处提及了解一个简单真理的方法︰「缺乏慈悲的人，看不见那些须以慈悲之眼观看的事物。」 那更具涵摄性的视野，区别了「绝望」与「希望」之间微小但关键的差异。 詹姆斯．佛斯特写于1976年 一、日常生活中的正念 有了无限的时间 昨天，亚伦带着他的儿子乔伊来拜访我。乔伊长得真快！他已经七岁了，讲得一口流利的英语跟法语，甚至夹杂着一些他从街上学来的俚语。 在法国养育小孩的方式和我们家乡越南是非常不一样的。这里，父母相信「自由对于孩子的发展是必要的」。在我和亚伦谈话的两个小时里，亚伦必须时时注意着乔伊。乔伊一下子嬉闹、一下子喋喋不休，不时打断我们，叫我们没办法好好的谈话。我给了他几本图画书，但他几乎看都不看就把书丢到一边，然后继续打岔。他要大人不时地关注他。 之后，乔伊穿上外套，跑出去和一个邻居的小孩玩。我问亚伦：「你觉得家庭生活轻松吗？」亚伦没直接回答。他说，从两个礼拜前鄂娜出生后，他就没法好好睡上一觉。夜间，苏因为自己太疲倦了，总会叫醒他，要他去确定鄂娜是不是还在呼吸。「我起床去看小婴儿，然后再回去睡觉。这种情况，有时候一个晚上会有两、三次。」 「家庭生活会比当个单身汉来得轻松吗？」我问。亚伦也没直接回答；但是我明白了。我又问了另一个问题：「很多人说有个家庭比较不寂寞，而且会比较有安全感，真的是这样吗？」亚伦点点头，轻 声地咕哝了些什么；我了解他的意思。 然后亚伦说：「我发现可以让自己有更多时间的方法。以前，我都把时间分割成好几个部分，一部分陪乔伊，一部分陪苏，一部分给鄂娜，另一部分拿来做家务。剩下的时间是我自己的──我可以读书、写点东西、做些研究，或者去散散步。 「但是现在，我试着别再去分割时间了。 「我把跟乔伊和苏在一起的时间也当作我自己的时间。帮乔伊看他的家庭作业时，我想办法把他的时间看作是我自己的时间；我和他一起做作业，感受他的存在，并且想办法让自己对我们在那段时间中所做的事情感兴趣。我和苏在一起的时候也是这样。 「结果，不可思议的是，现在我有了无限的时间给自己！」 亚伦微笑着说完这些话。我很惊讶。我知道这些不是亚伦从书上学来的；他是在日常生活中自己发现的。 洗碗就是洗碗 三十年前，我还是慈孝寺（Tu Hieu Pagada）的沙弥，那时，洗碗可不是件惬意的工作。在结夏安居时，所有比丘都回到了寺院，有时甚至有逾百位的比丘，而所有煮饭、洗碗的工作，全靠我们两个沙弥。那儿没有肥皂；只有草灰、稻壳、椰子壳，就这样。清洗堆积如 山的碗盘可真是苦差；特别是冬天时，水冻得像冰一样，你必须在洗碗前先热好一大壶水。 如今，厨房都有洗洁精、专用的菜瓜布、甚至一开即来的热水，洗碗变得轻松多了。现在人们比较可以享受洗碗这件事；任何人都可以很轻易地洗好碗盘，然后坐下来喝杯茶。我可以接受用洗衣机洗衣服──虽然我自己还是用手洗，但是用机器洗碗就有点过头了！ 洗碗时，就应该只是洗碗，也就是说，洗碗时，应该对「正在洗碗」这个事实保持全然的觉照。 乍看之下，可能有点傻──干嘛要这么强调这样简单的事呢？但这正是关键所在。 「我正站在这里洗这些碗盘」这件事实，是个不可思议的真实。当下的我，正是完完整整的我自己，随着我的呼吸、觉知到我的存在、觉察到我的思想和动作。我不会像个被浪花左拍右击的瓶子一样，毫无觉知地被抛来抛去。 你手中的杯子 在美国，我有个叫做吉姆.佛斯特（Jim Forest）的密友，八年前我第一次碰见他时，他在天主教和平联会（Catholic PeaceFellowship）工作。 去年冬天，吉姆来拜访我。我通常会用完晚餐先洗碗，再坐下来和大家喝杯茶。有一天晚上，吉姆说让他来洗，我说：「好呀，但是如果你要洗碗，你就得知道洗碗的方法。」吉姆回答道：「少来了，你以为我不知道怎么洗碗吗？」 我回答：「洗碗的方式有两种。第一种是为了把碗洗干净而洗碗，第二种是为了洗碗而洗碗。」吉姆很高兴地说：「我选第二种──为了洗碗而洗碗。」从那时起，吉姆知道怎么洗碗了。我把这个「责任」交给他整整一个星期。 如果在洗碗盘时，我们只想着接下来要喝的那杯茶，因此急急忙忙地把碗盘洗完，就好像它们很令人憎厌似的，那么我们就不是「为了洗碗而洗碗」。进一步来说就是，洗碗的时候我们根本没有活在当下；我们站在水槽前，完全不能体会生命的奇迹。 如果我们不懂得洗碗，我们很可能也不懂得喝茶：在喝茶的时候，我们又只会想着别的事，而几乎没有觉知到自己手中的这杯茶。这样，我们就是被未来给吸走了──不能够实实在在地活着，甚至连一分钟都不能。 吃橘子 我记得数年前，吉姆和我第一次一起到美国旅行时，我们坐在树下分一颗橘子吃。他开始谈论我们将来要做些什么。只要我们谈到一个吸引人或令人振奋的计划，吉姆就深深陷在其中，以致于完全忘了 他当下正在做的事。他往嘴里扔一瓣橘子，在还没开始咀嚼前，就又准备往嘴里扔进另外一瓣。他几乎没有意识到他正在吃橘子。我还得告诉他：「你应该先把已经含在嘴里的那瓣橘子吃了。」吉姆这才惊觉到自己正在做什么。 这就好像他根本没在吃橘子。如果说他吃下了什么，那么他是在「吃」他未来的计划。 一颗橘子有很多瓣。如果你懂得好好吃一瓣，你大概就懂得吃颗完整的橘子。但是如果你连一瓣橘子都不会吃，那么你根本就不会吃橘子。吉姆明了了。他慢慢垂下头，专注地吃那片已经在他嘴里的橘子。他仔仔细细地咀嚼它之后，才伸手拿另一瓣。 之后，吉姆因为反战活动而入狱，我很担心他能不能忍受监狱那四面墙的囚禁，我写了封短信给他︰「记得我们一起分享的橘子吗？你在那里的生活就像那颗橘子。吃了它，与它合而为一。不用担心明天会怎么样。」 日常的叮咛 三十多年前，我第一次踏入寺院时，法师给我一本宝华山的读体律师写的小书《毗尼日用切要》，叫我背起来。这本书很薄，不到四十页，但涵括了所有读体律师做任何事务时用来唤醒自心觉知的正念思惟。 早上醒来时，他第一个念头就是： 睡眠始寤， 当愿众生， 一切智觉， 周顾十方。 洗手时，他也这样提醒自己维持正念︰ 以水盥掌， 当愿众生， 得清净手， 受持佛法。 这本书全部是由这类句子所组成，用来帮助初学者保持正念觉知。 读体律师用一种相对简单的方式，帮助我们这些小沙弥修习《正念经》（The Sutra of Mindfulness）中所开示的事理。每当你穿衣、洗碗、上厕所、收起垫子、提水或刷牙时，你都能依循书中的偈颂来保持觉知。 《正念经》说： 走路时，修行者一定要觉知自己在走路。坐着时，修行者一定要觉知自己正坐着。躺着时，修行者一定要觉知自己躺着‥‥不管身体是什么样的姿态，修行者一定要觉知那姿态。经由这样的修习，修行者才能活在对身体的直接而不中断的正念观照中‥‥ 然而，仅仅对身体姿态的抱持正念是不够的。我们必须觉知到每一息呼吸、每一瞬移动、每一丝意念与感觉，和每一件与我们有关的事物。 但是经文为什么要教导这些呢？何时是修习正念的时机？如果你整天都在修习正念，怎么会有时间改变现状并且建立一个更理想的社会呢？亚伦要如何在工作、帮乔伊看作业、把鄂娜的尿布拿到洗衣机时，同时修习正念呢？ 二、在大地上行走就是奇迹 每一步都是无上惊奇 亚伦说，自从他把乔伊和苏的时间当成是自己的，他就有了「无限的时间」。但是他大概只是原则上拥有这「无限的时间」；因为只要他在帮乔伊看作业时忘了把乔伊的时间当成自己的，他就可能失去这些时间。在那些时候，亚伦可能会期望时间过得快一些，或者由于时间不是他的，而觉得自己被耽误，因此变得急躁没耐心。 所以，如果亚伦真的想要有「无限的时间」，他就必须在与乔伊一起做作业时对「这是我的时间」这份认知保持警醒。但在这种时刻，一个人不可避免地会因为其它事物而分心，所以如果想时时保持觉知状态（以下我将用「正念」来指称「对当下的真实保持觉知」），就要立刻在日常生活中开始修习，而不只是在禅修时才练习。 当你在一条通往村落的小径上行走时，你就可以修习正念。走在这条四周都是绿地的泥路上，如果你练习正念，你就能真正体验这条小径，这条引你往村落去的小径。你得一直敏于觉察着：「我正走在这条通往村落的小径上。」 不管天气是晴是雨，不管路径是干是湿，你都要一直保持这个思惟，但是别只是机械式地重复它。「机械性的思考」跟「正念」是对立的。如果我们真的抱持正念走这条通往村落的小径，我们会觉得每 一步都是无上的惊奇，喜悦之情将令心灵如花朵般绽放，让我们进入实相（reality）的世界。 我喜欢独自漫步在乡村小径上，道路两旁尽是稻作和野草。我在正念中踏出每一步，感知自己正走在这不可思议的大地上。在这样的时刻，存在本身就是个惊人的奇迹。一般说来，人们认为在水上或空中行走才叫「奇迹」，但是我觉得真正的奇迹并非在水上或空中行走，而是在大地上行走。 每一天，我们都身处于自己甚至都没认知到的奇迹中：蔚蓝的天空、雪白的云朵、碧绿的树叶、孩子充满好奇的黑色眼眸──那也是我们自己的双眼。 所有一切，尽是奇迹。 静坐 读体律师说过，在禅坐时，一个人应该坐得笔直，生起这样的念头︰ 正身端坐， 当愿众生， 坐菩提座， 心无所著。 「菩提座」就是佛陀开悟时坐的地方。如果任何人都可以成佛，而「佛陀」指的就是所有那些已经开悟的无数的人，那么，肯定有许多佛曾经坐过我现在正坐着的这个地方。如果坐在佛陀离苦得乐而证悟的地方，并且保持着正念，那就意味着成佛了。 越南诗人阮公着在某处静坐时，就曾有此体验；他突然了解，无数年前许多人就曾坐在现在他静坐的位置，而未来也会有其它人来这儿静坐︰ 今日我坐处， 过往他人亦静坐。 千年后，来者仍纷纷。 究竟谁为歌者，谁为听者？ 他静坐的位置和静坐的那段时间，就成为通往永恒真实的重要桥梁。 但是忙碌而多虑的人们没时间可以悠闲地生活，没时间在绿地间的小径行走，没时间在树下静坐。他们必须准备一套又一套的计划，不断地和身边的人协商，试着解决无数危机；他们总有要事得做。他们必须处理种种困境，时时刻刻都专注于工作，分分秒秒保持警醒且一切就绪，好掌握状况，随机应变。 你可能会问︰「那我们要怎么修习正念？」 我的答案是︰时时刻刻都专注于工作，分分秒秒保持警醒且一切 就绪，掌握各种可能会发生的状况，随机应变──这就是正念。 没有理由把正念与「专注于手边的工作，保持灵敏且做出最佳判断」画分开来。在协商、解决和处理各种状况时，若要获得好结果，冷静的心和自我控制绝对是必要的。 任何人都知道，如果我们没能好好控制自己，而让急躁或愤怒干扰我们，我们的工作就不再有任何价值了。 正念是个奇迹，藉由它，我们得以主宰自己、重建自我。 举个例子︰一位魔术师把身体切成许多块，并把它们放在不同的地方──手掌放南方，手臂放东方，腿放在北方，然而借着某种魔力，他大声一喝，身体各部分就重组归位了。正念就像这个魔术，是一个奇迹，能在一瞬间召回涣散的心思，并恢复重组成一整体，如此，我们就能过好生命中的每一分钟。 控制呼吸 因此，正念既是方法、也是目的，既是因、也是果。当我们为了达致专注而练习正念，正念是因；但正念本身就是觉知的生命︰正念存在意味着生命存在，因此正念也是果。正念让我们不再漫不经心、神思游移，使人得以充份地过好每一分钟。正念让我们能真正地活着。 呼吸能自然且极为有效地防止心思涣散，因此你该知道怎么用呼吸来维持正念。呼吸是连结生命与意识的桥梁，能统合你的身体和思绪。不论何时，只要你心思游离不定，都可以拿呼吸当工具，重新掌握你的心。 轻轻地深吸一口气，且觉知你正在深吸一口气的事实。 现在，吐出肺中所有的气，整个呼气的过程要保持觉照。 《正念经》教导我们用下面的方式控制呼吸︰ 吸气时，觉知你在呼气；呼气时，觉知你在呼气。 深深地吸进一口气时，你知道，「我正深深地吸进一口气」。 深深地呼出一口气时，你知道，「我正深深地呼出一口气」。 浅浅地吸进一口气时，你知道，「我正浅浅地吸进一口气」。 浅浅地呼出一口气时，你知道，「我正浅浅地呼出一口气」。 「吸气，了了分明地觉知整个身体，」你就这样训练自己。 「呼气，了了分明地觉知整个身体，」你就这样训练自己。 「吸气，让整个身体平静下来，」你就这样训练自己。 「呼气，让整个身体平静下来，」你就这样训练自己。 在寺院中，人人都学着以呼吸为工具，来克服心思涣散并且藉此 增强专注力（定力）。定力能助人开悟，而这种力量正是修习正念而来。所以当一个人能控制自己的呼吸时，他就已经开悟了。为了维持长时间的正念，我们必须不间断地观照自己的呼吸。 这儿正值秋天，金色的叶子逐一飘落，真是美极了。在林间散步个十分钟，观照呼吸并维持正念，就觉得神清气爽、焕然一新。 如此，我可以真正地跟每一片叶子交流。 当然，独自走在乡间小径上比较容易保持正念。假如你身边有个朋友，他不忙着叽叽喳喳而是也注意自己的呼吸，那么你就能毫无困难地继续维持正念。但是要是你身边的朋友唠唠叨叨，你要维持正念就没那么简单了。 如果你心中想着：「希望这家伙闭上嘴，这样我才可以专心。」这时你就已经失去了正念。 但如果你想的是：「要是他想谈天，我会回答，但是我会继续保持正念，觉知我们正一起在小径上行走的事实、觉知着我们所说的话，这样我还是能继续注意我的呼吸。」 如果你能有这样的念头，你就能继续保持正念。这种情况比你独处时要难修习；但是如果你能继续修习，你就能发展出维持更深的专注力的能力。有句越南民谣这么唱︰「最难莫过于在家修道，其次是在人群中，再来是在寺塔里。」只有在繁忙嘈杂且费神费力的情况下，修习正念才真的会是一种考验！ 数息和随息 在最近为非越南人开的禅修课上，我会建议各种我自己用过的方法，这些方法都相当简单。我建议初学者一种「随息（随顺呼吸）」的方法。 学生背贴着地板躺着，然后我请所有上课的同修围过来，好给他们解说一些简单的要点︰ 一、虽然吸气和呼气都靠肺运作，而且范围都在胸腔内，但胃在此也扮演一角。肺充气时，胃会鼓起。刚开始呼吸时，胃会开始向上鼓出来；但是呼气进行到三分之二时，胃又会开始瘪下去。 二、为什么呢？胸腔和胃部之间有一层肌肉膜，也就是横隔膜。当你正确地呼吸时，空气会先充满肺的下半部；空气充满肺的上半部前，横隔膜就会往下推到胃，使得胃向上鼓起。当肺的上半部也充满空气的时候，胸腔会往外扩张，使得胃又瘪下去。 三、这就是为什么古人说，呼吸是始于肚脐而终于鼻尖。 对初学者来说，躺下来练习呼吸非常有用。重要的是要防止努力太过：太过努力对肺很危险，特别是肺已经因为多年不正确的呼吸而变得很虚弱。 开始练习时，修行者应该背枕着薄垫子或毯子躺下，双臂轻松地 放在身侧，不要垫枕头。 专注于你的呼气上，看看它有多长，心中默数：一、二、三……，缓慢地测量它。这样，数次之后，你就能知道自己的呼吸「长度」；或许是五。 现在，试着延长呼气的长度，多数一或两个数，让呼吸长度变为六或七。接下来开始一边呼气、一边从一数到五。数到五时，不要像以前一样立刻吸气，试着让呼气延长到六或七。 这个方法能够清空你肺部的气体。呼气结束时，停顿一会，让你的肺自发地吸入新鲜的空气。让肺自己不需要费力下能吸入多少空气就吸入多少空气。 吸气通常要比呼气来得「短些」。默默保持稳定的计数，测量吸气和呼气的长度。像这样练习数个星期，躺下时永远对你的吸气和呼气保持觉知。（如果你的钟滴答声很大的话，就可以用它来帮助自己测量吸气和呼气的长度。） 继续在你行走、坐下、站立时测量你的呼吸，尤其是在户外时。行走时，你可以用脚步来测量呼吸。大约一个月后，你吸气和呼气的长度就会差不多了，然后渐渐平衡，最后变得完全相同。如果你呼气的长度是六，你吸气的长度也会是六。 如果练习时你觉得累了，就立刻停止。但即使你一点也不觉得累，也不要太长时间地练习这种长且平均的呼吸，一次练习个十或二十回 呼吸就够了。 当你开始觉得有点疲累，就回复一般的呼吸状态。「疲累」是身体的一种出色机制，是最好的警示，会告诉我们是该休息还是该继续。 为了测量呼吸的长度，你可以用默数的，或者用你喜欢的具有韵律的词组。（假如你呼吸的长度是六，你可以用六个字来代替数数：「此、刻、我、心、平、和。」如果长度是七，你可以用「我、走、在、新、绿、地、球。」佛教徒可以说：「我、皈、依、于、佛、陀。」基督教徒可以说「我、们、天、上、的、父。」当你行走时，每一步都要跟每个字相对应。） 安静地呼吸 你的呼吸该是轻柔的、平稳的、顺畅的，像潺潺流过沙地的小溪一般。 你的呼吸应该非常安静，静得连坐在你身边的人也听不见。 你的呼吸该优雅地流动，像条河流，像水蛇游走水中，而不是像畸岖不平的山脉或马儿的飞奔疾驰。 统御自己的呼吸，就是控制自己的身心。每次我们发现自己再度心思涣散、或用尽方法也实在难以控制自己时，都该运用「观呼吸」 这方法。 当你坐禅时，开始观照你的呼吸。首先，像你平常那样呼吸，然后渐渐缓和下来，直到呼吸变得安静平稳，每次呼吸都很长。从你坐下到呼吸变得深细无声的期间，要一直对发生在你身上的一切保持觉知。就像《正念经》上说的： 吸气时，觉知你在呼气；呼气时，觉知你在呼气。 深深地吸进一口气时，你知道，「我正深深地吸进一口气」。 深深地呼出一口气时，你知道，「我正深深地呼出一口气」。 浅浅地吸进一口气时，你知道，「我正浅浅地吸进一口气」。 浅浅地呼出一口气时，你知道，「我正浅浅地呼出一口气」。 「吸气，了了分明地觉知整个身体，」你就这样训练自己。 「呼气，了了分明地觉知整个身体，」你就这样训练自己。 「吸气，让整个身体平静下来，」你就这样训练自己。 「呼气，让整个身体平静下来，」你就这样训练自己。 大约十到二十分钟后，你的思绪将会沉淀下来，像一池平静无波的湖水。 数息 让呼吸宁静且平稳下来的方法，可称为「随息（随顺呼吸）」。如果这方式一开始看起来很难，你可以改用数息（数呼吸）的方式。 当你吸气时，心里默数一，呼气时，也数一。再吸一口气，数二，呼出第二口气时，也数二。这样一直数到十，然后再从一开始数起。这种数法就像一条绳子，能把正念拴稳在你的呼吸上。要想持续觉知呼吸，这个练习就是起点。 要注意的是，没有正念的话，你很快就会忘了继续默数。如果忘了数到哪儿，就要回到「一」重新开始数，直到你能保持正确地数算。一旦你能真正专注地数数，你就有资格丢弃数呼吸的方式，只全神贯注在呼吸这件事上。 在你烦乱不安或心思涣散的时刻，若你觉得很难修习正念，就回到你的呼吸上──掌握呼吸本身就是正念。 呼吸是掌握自己意识的绝妙方法。就像有个教团在它的教规中说的：「人不该在心思散乱或周遭环境中迷失了自己。学着练习呼吸，就是为了再度主控身心，为了修习正念，也为了发展专注力（定力）和智慧。」 一举一动都是一个仪式 让我们想象一下：有座高耸的墙，从这座墙的顶端看去是一望无边。但是却没有什么工具可以让人爬上墙顶，只有一条从顶端往墙壁两边垂下的细线。聪明的人会在细线的一端绑条较粗的绳子，然后走到墙的另一边，把细线拉下来，绳子就会被牵引到墙的这一边来。接着再把绳子的末端绑上牢固的粗绳索，然后将绳索拽到墙对面。当这根粗绳垂到对面墙根并且被固定住时，就可以很轻松地爬上墙了。 我们的呼吸就像那条细线。然而，一旦我们知道怎么运用它，它就会成为帮助我们克服那些看来无望的情况的绝佳工具。呼吸是连接身体和心灵的桥梁，能协调身心，使身心得以合一。呼吸跟身心状况是相呼应的，它能统合身心，既能启发身心两者，又能带来安宁与平和。 许多人和书都曾讨论过正确呼吸的无尽好处。他们说，知道怎么呼吸，就知道怎么增进无穷的活力：呼吸使肺强健，强化血液，更让身体每个器官都像重新活过来一样。他们还说，正确的呼吸比食物还重要。这些说法都很有道理。 多年前，我病得很重。在吃了几年药并忍受疗程后，我病情并没改善。最后，我回过头来用「呼吸法」，藉由这个方法，我把自己给治好了。 呼吸是一种工具。呼吸本身就是正念。尽管把呼吸当作工具来运 用能让我们受益无穷，但我们不该把这些益处当成是学习呼吸的目的。这些益处不过是修习正念所带来的副产品罢了。 在我为非越南人开的禅修班上，有许多年轻人。我告诉他们，如果每天能禅修一个小时当然很好，但那其实根本不够。你得在走路、站立、躺下、坐着和工作时，乃至于洗手、洗碗、拖地、喝茶和朋友聊天时，都练习禅修。 不管你在做什么，你都得练习禅修：「洗碗时，你可能想着等会儿要喝茶，因此想尽快把碗洗完，好坐下来喝杯茶。但是那意味着你在洗碗时根本没有活在当下。当你洗碗时，洗碗就是你生命中最重要的事。当你喝茶时，喝茶就是你最重要的事。当你如厕时，如厕就是你最重要的事。」 就像这样。劈柴是禅。担水是禅。 一天二十四个小时都要保持正念，而不是只有在你禅修、读经或祈祷的一个小时内如此。做什么事都要秉持正念。 一举一动都是一个仪式，一个典礼。 将茶杯举到唇边是一个仪式。「仪式」这个字眼似乎太沉重了？我用这个词是为了震醒你，让你理解「觉照」这件生死大事。 三、正念日 今天是你的 一个人每天每时每刻都应该修习正念。 这说起来很容易，做起来却不简单。这就是为什么我在禅修班上建议大家，每星期应有一天全部用来修习正念。原则上，每一天、每个小时都应该是你自己的，可是事实上很少人能够如此。我们总感觉自己的时间被家庭、工作和社会事务占去。 所以，我建议每个人从一个星期中抽出一天来，或许是星期六。 如果你选了星期六，那么星期六就完全是你自己的，一个你可以完全主控的日子。星期六将会成为一道梯子，它会提升你，使你养成修习正念的习惯。 和平工作队或服务团体的所有工作人员，不管工作有多么急迫，都有权利拥有这样的一天。因为如果没有自己的一天，我们会很快在充满烦忧和忙碌的人生中迷失。不管我们选了哪一天，都可以把那天视为「正念日」。 为了设定一个正念日，你要想办法在那一天一醒来的时候就提醒自己，这一天是你的正念日。你可以在天花板或墙上挂些什么，像是 一张写着「正念」的纸或一根松枝－－任何在你张眼时就会看到、会提醒你「今天是我的正念日」的东西。 今天是你的。记着这一点，或许你会感觉到一抹微笑正在脸上绽放，这笑容证明你完全处在觉照中，并且能进一步滋养出更纯净的正念。 正念中苏醒 还躺在床上时，慢慢地跟随着你的呼吸一缓慢悠长且充满自觉的呼吸，然后缓缓起身（而不是像平时一样一跃而起般，借着每一个动作来滋养正念。起床后，刷牙洗脸，平静而放松地做所有晨间事务，但每个动作都要以正念去做。 随顺你的呼吸，掌握它，不要让念头东奔西跑。每个动作都该平静地去做。用安静而悠长的呼吸来测量你的脚步。保持面带微笑。 至少花半个小时洗澡。缓慢而正念地洗，那么在洗好时，你会觉得轻松且精神焕发。之后，你可能会做点家事，像是洗碗、打扫、擦桌子、刷厨房地板，或是整理架上的书。不管你做什么，都要在正念中从容轻松地做。不要抱着只想把它完成的心态做事。 下定决心以一种放松的方式全神贯注地做每一件工作。享受你的工作，并与它合一。如果你做不到这样，正念日就毫无意义了。 如果你用正念去做每件工作，觉得每件工作都很令人头痛的这种感觉就会很快消逝。以那些禅师为例吧，不管做什么工作，举手投足，都不慌不忙，非常沉稳，丝毫没有勉强。 对刚开始修习的人来说，正念日那天最好整天都保持一种沉默的精神。 这不是说你一句话都不能讲。你可以聊天，甚至可以唱歌，但不管你聊天或是唱歌，都要对你在说什么、或者在唱什么保持全然的觉照，前且尽量少说或少唱。当然，只要觉知到自己正在唱、觉知到自己在唱什么，在说和唱的同时修习正念也是可能的。 要注意的是，如果你的禅修力还很弱，在谈天唱歌的时候，就很容易失去正念。 在正念中入睡 午餐时间到了，好好地为自己准备一餐。在正念中煮饭、洗碗。 早上清扫房子后，以及下午忙完园艺工作或看云摘花后，在正念中给自己泡壶茶，坐下好好品尝。给自己充裕的时间来做这些事；不要像有些人在休息时间内牛饮咖啡那样喝茶。要不疾不徐且虔敬地喝它，就好像它是地球绕着旋转的中轴；和缓地、平稳地朝未来行进，而不是匆匆忙忙地冲去。 过好这真实的这一刻。这一刻，就是生活本身，就是生命。不要成为未来的俘虏。不要烦恼你未来要做的那些事。不要急着想开始或摆脱什么事。不要想着「启程」。 化为树篱中静静坐着的芽 化为微笑，成为这不可思议的存在的一部分 伫立在这儿。不须启程。 这块土地就像我儿时的故土那样美丽 请不要伤害它，并且继续歌唱‥‥ －－《金芥菜花田上的蝴蝶》 晚上，你可以诵经并且抄写几段经文，写信给朋友，或者做其它你喜欢做的事，只要不是这周的日常事务。但不管你做什么，都要保持正念。晚餐吃一点就好；这样，在大约十点或十一点坐禅时，你会因为空腹而较能坐得轻松点。之后你可以在晚间清爽的空气中悠闲地散步，在正念中跟随着呼吸，并借着脚步测量自己呼吸的长度。最后，回到房间，在正念中入睡。 我们必须想办法让每个工作人员都能拥有正念日。这样的一天绝对是必要的。它的效果对这一周内的其它日子影响是不可估量的。 十年前，多亏了正念日，周文（Chu Van）和我们互即互入团的其它姊妹弟兄，了得以引导他们自己度过种种困境。 我相信，只要施行每周一次的正念日三个月，你们就会看到生命中巨大的改变。正念日会开始渗透到一周内的其它日子，最后你一周 七天都会活在正念中。 我确定，你会像我一样同意正念日的重要！ 四、鹅卵石 做一颗鹅卵石 你为什么要禅修呢？ 首先，因为我们每个人都需要彻底的休息。一个晚上的睡眠并不能提供完全的休息。翻来覆去、脸部肌肉紧张、还有做梦－－哪里好好休息过了？当你仍然觉得没有得到休息而辗转反侧的时候，躺着并不意味着你在休息。 背贴着床躺着，手脚放直但不僵硬，头下不要垫枕头，这是个练习呼吸且放松肌肉的好姿势，不过这种方式也很容易让你睡着。躺着禅修时，你没办法像坐着禅修时那么深入。 其实，坐着也可以让你完全休息，并且进一步帮助你进入更深的禅修境界，解决那些扰乱并阻塞你意识的忧愁与烦恼。 我们在越南的工作人员中，有许多人可以结踟跌坐，就是左脚放在右大腿上，同时右脚也能放在左大腿上。其它的人可以用半跏跌姿坐着，左脚放在右大腿上，或是右脚放在左大腿上。我们在巴黎的禅修班上，有的人不管用上述哪个坐姿，都觉得不舒服，因此我教他们日本人的坐法：跪着，臀部坐在两条腿上。如果在腿下垫个蒲团，这样的坐姿可以维持一个半小时以上。 虽然如此，但是任何人都能学会半跏跌坐，虽然一开始多少会觉得痛。但练习几个星期后，就会觉得这种姿势很舒服。在初学阶段，如果真的痛得坐立不安，可以换另一脚，或是换成其它坐姿。如果一个人结跏跌坐或半踟跌坐，身下必须垫个蒲团，好让双膝碰触地面。这么一来，身体与地板会有三个接触点，能让坐姿非常稳定。 背脊保持挺直。这非常重要。头和颈子必须与脊椎成一直线，但是不要僵直或像块木头似地。看着前方约一、两公尺的地方。如果可以的话，稍微面带微笑。 现在开始随着你的呼吸，放松所有的肌肉。专注地保持脊椎挺直，并且随着你的呼吸。至于其它的事，随它去。什么事也不要管。如果你想放松因烦恼而紧绷的脸部肌肉，那么先轻微地微笑。当你稍微微笑时，所有的脸部肌肉就会开始放松。轻微的微笑维持得愈久愈好。那微笑就是你在佛陀脸上看到的那种微笑。 掌心向上，左手放在右手上。放松双手、十指、双臂和双腿。什么事都不要管。要像随波漂流的水生植物，而水面下的河床却保持不动。除了呼吸和轻微的微笑之外，心无所系。 对初学者来说，静坐不宜超过二十或三十分钟。在这期间，你可以很轻易地得到彻底的休息。诀窍有两个：「观」和「舍」；「观」呼吸，「舍」下其它一切事情。放松身体每一条肌肉。在大约十五分钟后，就可能达到深刻的安静状态，内心充满平和与喜悦。保持这种安静与平和。 有些人视禅修为苦事，希望时间快点过去，之后好休息一下。这样的人还不懂怎么静坐。如果你坐得正确，就有可能在坐姿中再得彻底的放松与平和。通常，就「一颗被扔进河里的鹅卵石」这意象做观想，会对我们有帮助。 要怎样藉助这颗鹅卵石的意象？ 以最适合你的姿势坐下，踟趺坐或半跏跌都可以，脊背挺直，面带微笑。缓慢地深呼吸，观照每一次呼吸，与它合一。放下一切，把你自己想成一颗被丢进河里的鹅卵石。鹅卵石毫不费力地直接下沉。它以最短的距离坠落，最后沉到河底，那最佳的休憩处，途中什么也不黏附。你就像那颗让自己沉到河里的鹅卵石一样，放下一切。 你存在的核心就是你的呼吸。你不需要知道要花多少时间才能到达细沙河床上完美的休憩处。当你觉得自己像那颗到达河床的鹅卵石那样地歇息，就是你开始得到休息的时刻。你不再被任何事物牵动了。 如果连在静坐的当下，你都无法找到平和喜悦，那么「未来」本身就只会像流水般流过，你阻挡不了它的流逝，而在它成为「现在」时，你也无法好好地过它。喜悦与平和正是静坐的当下所生起的喜悦与平和。 如果你在这里找不到它，你在别的地方也找不到。别如影随形地紧迫着你的思绪。不要跟着思绪胞。在当下发现喜悦与平和。 这是你自己的时间。你坐的这个地方是你自己的地方。在此地、此时，你就可以开悟。你不用去坐在遥远的异乡某棵特定的树下。像这样修习几个月，你将会开始体认一种深刻且重生的喜悦。 静坐时是否轻安，要看你每天修习正念的时间多寡，还要看你是否规律地静坐。要是可能的话，就和亲朋好友每晚一起静坐一个小时，比方说，从十点到十一点。想参加的人，都可以来坐个半小时甚至一小时。 观心 有人可能会问：放松就是禅修的目标吗？ 事实上，禅修的目标远远不只如此。然而放松却是禅修必经的起点，一个人如果懂得放松，就能有颗宁静的心与澄澈的头脑。达到这样的境界，就已经是在禅修之路上迈出了一大步。 当然，为了掌握自己心并且让念头平静下来，我们也必须练习观照我们的觉「受（觉受）」和「想（念头）」。若要掌握自心，你就必须练习观心。你必须知道如何观察并且辨识浮现在自己心中的每一种觉受和每一个念头。常照禅师写道： 如果修行者透澈地了解自心，他就只需要吹灰之力而有所成。但是倘若他对自心一无所知，那所有努力都将成空。 如果你想了解自心，只有一个办法：去观察并辨识出跟心有关的一切。你必须时时刻刻这么做，必须在日常生活中随时进行，而不是只在禅修的时刻这么做。 禅修时，各种觉受和念头都可能浮现。如果你没有练习观照呼吸，这些念头很快就会诱引你偏离正念。但呼吸不只是个藉以驱赶这些念头和觉受的工具；呼吸是联系身心并开启智能之门的工具。 当某种觉受或念头浮现时，你不该刻意地去驱赶它，只要持续专注于呼吸，这种觉受和念头自然会从你心中消失。你不能只想着躲避它、憎恨它、气恼它或是惧怕它。 那么，对于这些觉受和念头，你到底该做什么？ 其实你只须认知到它们的存在。举例来说，当悲伤悄悄浮现，你要立即辨识到它的存在：「一种悲伤的感觉刚刚在我心中生起。」如果这悲伤的感觉持续，就继续觉知：「这悲伤的感受仍在我心里。」假如有个这样的念头浮现：「已经很晚了，可是邻居还在制造噪音。」那就要觉知到自己冒出了这个念头。 假如这念头继续存在，就继续觉知它。如果浮现了另外一种觉受或念头，就用同样的方式辨识它。重要的是，不要让任何觉受或念头浮现却不加以观照，要像皇宫守卫一样，对所有经过前廊的人的脸孔都保持清清楚楚的觉察。 假如现在没有任何觉受或念头，那么就觉知自己此刻没有什么觉 受或念头。像这样练习，你就能敏于觉知自己的觉受和念头。很快地你就能掌握自己的心。你可以将观呼吸和观照觉受和念头的方法结合运用。 心如何观心？ 练习正念时，不要去区分好坏善恶，因为那会引发修行者内心的战争。 不论什么时候，当好念头生起，就认知：「我心中生起了一个好念头。」而当坏念头生起，也要认知：「我心中生起了一个坏念头。」不管你有多么不喜欢，都别老想着或试着消抹它。认知到它就够了。如果你已经离开了，你必须知道你已经离开了；而如果你还停留原处，也要知道你还在原处。一旦你有了这样的醒悟，就没有什么好怕的了。 当我提到皇宫大门的守卫时，你大概已经想象到这样的画面：一个有着两扇门的前廊，一个入口，一个出口，而你的心就是那个守卫。不管什么觉受或念头进入，你都了知它进入了，而当它离去时，你也注意到它离去了。 但是这个意象有个缺点：它让人觉得进出走廊的人跟那守卫是不一样的。而事实上，我们的念头和觉受就是我们自己。它们是我们的一部分。有一种诱惑使得我们把它们，或者至少是它们当中的一部分，当作是一股与自己敌对的力量，一直试图干扰你的专注（定）或洞见 （慧）。 但是，事实上，当我们愤怒时，我们自己就是愤怒本身。当我们快乐时，我们自己就是快乐本身。当我们产生某些念头，我们本身就是那些念头。我们既是守卫，也是访客。我们既是自己的心，也是心的观察者。 因此，驱赶或执着于任何念头都没必要。重要的是去察觉到这个念头。这种观察并不是将心当成对立的客体；它并不是要建立主体和客体之间的区别。心不强占心，心也不会把心赶走。心只能观察它自己。这种观察不是在观察某个独立于观察者之外的事物。 我记得白隐禅师的公案，他曾经问：「单手相击会拍出什么声音？」或者，以舌头味觉的经验为例：「是什么把味道和味蕾分开了？」心直接在心里体验它自己。这非常重要，这他是为什么在《正念经》中佛陀总会提到： 即受观受，即心观心。 有人说过佛陀之所以用这样的句子是为了要强调「受」和「心」这两个字，但是我不认写这种说法领悟了佛陀的真意。 「即受观受」，就是体验某种觉受时，就直接观照那觉受，而不是去思考觉受的复制品，也就是有关觉受的概念或形象。那些概念形象只是人们为觉受构设出来的东西，某种外在于觉受且和真正觉受不相干而看似客观的概念。 「即受观受」就是心在体验「即心观心」。描述性的语句使得它听来像个谜语、谬论或是绕口令。以外在的观察者客观地观察某件事物，这是科学的方法，而不是禅观的方法。因此，守卫和访客的概念无法适当说明心是怎么观察心的。 《正念经》说，心像是一只在森林中不停从这儿荡到那儿的猴子。为了不要因为牠行踪飘忽而失去牠的踪迹，我们必须持续观察这只猴子，甚至与牠合而为一。心观察心就像一个物体和它自己的影子－－这物体根本没办法摆脱它自己的影子。两者其实是一体的。不管心去了哪里，它还是受心掌控。 佛经里有时会用「绑住猴子」这样的形容来比喻「掌控自心」。但是猴子的意象也只是种表达方式而已。一旦心能直接且持续地觉知它自己，它就不再像只猴子。并没有两颗心，一颗从一根树枝荡到另一根树枝上，另一颗则紧跟在后，想用绳索绑住它。 习禅的人通常都希望能见到自性，以达到证悟。但是如果你才刚开始学习禅，别期待「见到自性」，甚至最好什么也别期盼。特别是，别在静坐的时候希望见到佛陀或任何样态的「实相」。 在刚开始习禅的前六个月，只要试着培养自己的专注力（定力），创造内在的平静和安详的喜悦。你会摆脱焦虑不安，享受到彻底的休息，并且使心安静下来。你会焕然一新，对事物将会有更广泛、更清明的认识，内心的爱也会更深、更强。然后，你就能对于周遭的一切做出更有建设性的回应。 坐禅乃是精神和身体的滋养剂。藉由静坐，我们的身体达到和谐，感觉更轻盈，也更深入平和中。从观察自心至见到自性，这条路其实并不难走。一旦你能够让心平静，一旦觉受和念头都不再能扰乱你，你的心就开始安住在自身了。 你的心会以一种直接且不可思议的方式掌握它自己，而这种方式从不会去区隔主体和客体。用这种方式，喝茶时，喝茶的人和被喝的茶之间的区别将会消失；喝茶变成一种直接而奇妙的体验，在这过程中，主体和客体之间的分别不再存在了。 涣散的心也是心，就像浪花也还是水一样。当心掌握了自身，被蒙蔽的心（妄心）就变成真实的心（真心）。 真心就是真实的自我，也就是佛陀－－就是未被分割、完完整整的「纯一」，它不会被孤立个体这样的虚妄分别所分割，而这些虚妄分别则是由概念和语言制造出来的。 但是现在我不想就此谈太多。 五、一即一切，一切即一：五蕴 有「自我」吗？ 让我在这里用几行字来谈谈几个修行法门，藉由这些方法，能让你跳脱狭隘的知见，变得无惧且慈悲。这些方法就是：缘起（相互依存）观、无常观与慈悲观。 当你坐禅时，在掌握自心之后，你就能专注地观照特定物体间的相互依存关系。这种禅修法并非对相互依存做哲学的推论思考，而是心深入心自身，是修行者藉由专注力（定力）揭示被静观的物体展露真实自性。 想想这个简单而古老的真理：认知的主体不能独立于认知的客体而独立存在。「看」，就是去看某样东西。「听」，就是去听某样东西。「愤怒」，是因为某事而愤怒。「希望」，是希望某事。「思考」，乃是思考某事。一旦认知的对象（那个事物）不存在，也就没有认知的主体。修行者静观内心，而藉由这么做，得以洞察「认知的主体」与「认知的客体」之间相互依存的关系。当我们修习观呼吸时，觉知呼吸的就是心。当我们修习观身时，觉知身体的就是心。当我们修习观照身外之物时，觉知这些事物的还是心。因此，静观所有物体间的相互依存关系（缘起观），就是「观心」。 心的所有对象就是心本身。佛法上，我们称心的对象为「法」。「法」 通常被归类于五个范畴： 一、色 二、受 三、想 四、行 五、识 这五个范畴通常称为「五蕴」。不过，第五蕴「识」含括了所有其它的四蕴，而且是其它四蕴存在的基础。 对相互依存关系的静观（缘起观），乃是对诸法的深入观照，藉以契入它们的本性，洞见它们其实都以是实相这一大「整体」的一部分，也领悟到实相这个整体乃是不可分割的，无法被切成小块而各自独立存在。 静观的第一个对象是我们自己，亦即我们自身的五蕴之和合。你可以就在此时此地，静观那组成了你自己的五蕴。 你将觉察到色、受、想、行和识的存在。持续观察这些「对象」，直到你看到它们每一个都与你身外的世界紧密相联：如果世界不存在，那么这五蕴的和合也就不存在。 想想桌子这个例子。只有在我们或可称之为「非桌子的世界」，像是森林（树木在那里生长，并且被砍伐掉）、木匠、铁矿（变成钉子和螺丝），以及其它无数与这张桌子有关的东西，包括木匠的父母 及祖先、乃至于让树得以生长的阳光和雨水等等存在时，桌子才有可能存在。 如果你领会了这张桌子的实相，你就会看到，这张桌子中存在着所有那些我们通常认为它们是属于「非桌子的世界」的事物。如果你将这些「非桌子」因素中的任何一个抽走，并把它还原至它的本源，像是让钉子恢复成铁矿，让木材回到森林，将木匠还给他父母，这张桌子就不存在了。 一个能经由观照这张桌子而看到整个宇宙的人，就是能看见道的人。你要用同样的方式静观自己五蕴之和合。静观它们，直到你在自我中看到「一」的实相的存在，看到你的生命和宇宙的生命本为一个整体。 如果五蕴还归至它们的源头，自我就不存在了。这个世界每一秒都在滋养着五蕴。自我就是五蕴的和合。在宇宙万物的形成、创造与灭亡中，五蕴的和合也扮演了关键性的角色。 从受苦中解脱 人们通常把实相切割成不同部分，因而看不到所有现象间的相互依存关系。看到了一切中的「一」和「一」中的一切，就是突破了一个大障碍，这个障碍限制了人们对于实相的感知。佛教称这个阻碍为「我见」。 「我见」是指相信有独立存在而且永远不变的实体我。突破这个妄见，就能从各种害怕、痛苦以及焦虑中解脱出来。对越南和平工作人员有众多启发的观世音菩萨，在洞察了「五蕴皆空（无我）」的实相时，她就从各种折磨、苦痛、怀疑与愤怒中解脱了。这种禅修法对每个人都适用。假如我们坚持不懈，精进地静观五蕴，我们也能从受苦、害怕、恐惧中解放。 为了成为宇宙生命的一部分活着，我们必须除去所有障碍。人并非像被包覆在一个厚贝壳里头那样的孤立个体，与世界隔绝，能够不受外界影响而自由穿越时空。如果那样与世隔绝地活个一百世或十万世，那就称不上是活着，而这种情况也根本不可能发生。 我们的生命乃是万象的展现，就像我们自己也身处各种不同现象中。我们就是生命，而且生命是无限的。也许我们只有在过着世间生活的时候，也就是说，在经历着别人的悲苦喜乐时，才算是活着的。别人的痛苦就是我们自己的痛苦，别人的快乐就是我们自己的快乐。 如果我们的生命是无限的，那么组成我们的五蕴之和合也会是无限的。宇宙之无常、生命之成败，都不能再摆布我们。如果能领悟缘起的实相并且深深契入，就再没有什么能压制我们了。你解脱了。结踟跌坐，观照你的呼吸，并为那些用他人献身的人禅修。 我们应该时时刻刻练习静观事物的相互依存关系，让它成为日常生活中不可缺的一部分，而不只在静坐时练习。我们必须学着将眼前的人看成我们自己，而我们自己就是那个人。我们必须能洞察所有正在发生和将要发生的事件的缘起和相互依存关系。 骑在生死的浪头上 我不能略过生死问题不谈。 许多年轻人和其它许多人出于对受苦的人的爱，来这儿为他人服务，为和平而工作。他们都很清楚最重要的问题就是生死的问题，但是常常看不清生死不过是同一实相的两面。一旦我们领悟这个道理，我们就有勇气面对生死了。 在我仅十九岁的时候，一位年长的比丘要我禅观「墓园里的一具尸体」这个意象。我觉得这很困难，于是抗拒这样的禅修练习。但是现在我不这么觉得了，不过那个时候我觉得这种禅修方式应该是给较年老的比丘。 但是在那之后，我看过许多一个挨着一个躺着、动也不动的年轻士兵，他们有些才不过十三、十四或十九岁。他们对死亡一点准备都没有。 现在我知道，如果一个人不知道怎么死，就几乎不知道怎么活，因为死亡就是生命的一部分。就在两天前，摩比（Mobi）告诉我说，她认为一个人二十岁大就可以禅观尸体了。而她也才不过刚满二十一岁。 我们必须直视死亡，辨识出它，并接受它，就像我们直视生命，并接受它一样。 佛教的《正念经》上谈到了如何观尸体：静观身体的腐烂，静观身体如何膨胀、变紫，如何被蛆吃得只剩几丝血肉附在骨上。一直静观到它只剩下白骨，然后慢慢朽坏，直到化为尘土。像这样静观，知道你的身体也将经历同样的过程。静观尸体，直到你变得宁静且安详，直到你的心变得轻安，直到你的脸庞泛起微笑。 借着克服嫌恶与惧怕，生命将会被视为无限珍贵，每一秒都该珍惜。进一步来说，不是只有自己的生命该被珍视，而是每一个人的生命，每一个人、每一个众生、每一个存在都应该受到珍视。 我们不会再被「为了自己的生存，毁灭别人是必要的」这样的想法所迷惑。生和死是生命的两面；如果两者不同时存在，生命也就不可能存在，就像一个铜板必须要有两面才得以存在一样。只有在当下，我们才有可能超越生死，才有可能知道怎样去活，怎样去死。 佛经上说，菩萨洞彻了相互依存的实相，突破了一切狭隘的见解，因而得以像一个人驾着小舟却不被生死的浪头淹没或溺亡一样，自由地进出生死。 有些人说，如果你以佛教徒的角度去看现实，就会变得悲观。但是用「悲观」或「乐观」这样的词语，都太过简化真理了。重点是要看现实真正呈现出来的样子。悲观的态度永不可能绽放安详的微笑，就像菩萨和其它得证道者的唇上浮现的宁谧微笑一样。 六、你前院的杏树 你真的看到前院的杏树了吗？ 我已经谈过了静观事物的相互依存关系。当然，所有追求真理的方法都该被视为手段，而不是目的或绝对真理。静观事物的相互依存关系，目的是要破除虚妄的分别障碍，让人们得以融入生命的整体和谐，而不是为了创造一个缘起（相互依存、依他起）的哲学体系。 赫曼．赫塞在他的小说《悉达求道记》中还未能洞彻这点，因此他笔下的悉达是在宣讲相互依存的哲学，那些话语对我们来说就不免显得天真。作者提供我们一幅万物相互依存的图象，在这图像中，一切都是互相关联的；它是一个毫无瑕疵的系统：每一样物事都一定能嵌入这个互相依存、绝无谬误的体系，而在这个体系中，人是不可能去思考如何从这世界解脱的问题。 依据我们佛教传统的观点，宇宙万法有三种特性：遍计所执性、依他起性和圆成实性。 我们首先来思考「依他起」（缘起、相互依存）。由于疏忽（失念）与偏见，我们常在实相上蒙上一层妄见的面纱。这是通过「遍计所执性」来看真实。遍计所执性可以说是实相的幻相，它把实相看成许多不相干的独立实体和自我的集合体。 为了要打破遍计所执性所造成的幻相，修行者必须在万事万物的生灭过程中静观万物间的相互依存和万象中的相互关连。这种思维的方式乃是一种禅观方法，而不是哲学教条的基础。如果一个人只是抱着某种概念体系不放，那他将会陷入僵局。 「缘起观」可以让人契入实相，与它合而为一，而不被某种哲学观念或禅修方法所束缚。筏是用来渡河的，而不是要拿来扛在肩膀上的；指着月亮的手指并不是月亮本身。 最后，让我们谈谈「圆成实性」，也就是解脱了「遍计所执性」所造成的谬见后所显现的实相。实相就是真实，超越任何概念，也没有任何概念能够恰当地描述它，甚至「缘起」的概念也无能为力。为了确保不执着于任何哲学概念，佛学还谈及三无性，以免我们被「三性」的教义所束缚。大乘佛教教法的最重要的特性正是在此。 当了知实相的「圆成实性」，修行者就达致了「无分别心」的智慧层次。这是一种奇妙的圆融境界，在这境界中，主体、客体的区别消失了。这并不是一种遥不可及的状态。只要稍微坚持地精进修习，任何人都至少能够经验它。 我的桌上有一迭孤儿申请扶助金的申请书。我每天都翻译一些。在我开始翻译一份申请书之前，我会凝视照片中的孩子的眼睛，并仔细观察他的表情和特征。我觉得我和每个小孩间都有一种深刻的联系，使我能和他们之间达至一种特别的交流。当我将这些写下来给你时，我看到在那些时刻，在我在翻译申请书那些简单的文字时所体验的交流，就已经是一种无分别心。 我看不到有一个「我」在翻译这些申请书好帮助那些孩子，我看不到有哪个孩子在接受爱与帮助。小孩和我是一体的：没有人在怜悯，没有人在求助，没有人在援助。没有任务，没有待做的社会工作，没有悲悯，没有特殊的智能。这种时刻就是无分别心的时刻。 当你体会了实相的「圆成实性」，在你前院的一棵杏树就有可能完全地显露它的本性。杏树本身就是真理，就是实相，就是你的自我。所有经过你前院的人当中，有多少人曾经真的好好看过杏树？ 艺术家的心可能会敏感些，很可能会用一种比别人来得深入的方式看这棵树。由于拥有更开放的心，艺术家和这棵树之间无疑地有某种交流存在。 重要的是你自己的心。 假如你的心没有被谬见所蒙蔽，你就自然能融入与这棵树的交流中。物我合一时，这棵杏树将把自己完完整整地显现在你面前。洞彻杏树就是「见道」。 曾有人请求一位禅师解释实相的奥秘，那时禅师就曾指着一棵柏树说道：「看看那边那棵柏树。」 2.涨潮的声音 当你的心获得解脱，你的心中会满溢慈悲：对你自己慈悲，因为 你曾受过这么多的苦，只因为你那时还未能将自己从妄见、憎恨、无知与愤怒中释放出来；对他人慈悲，因为他现在还未能看清自己被妄见、憎恨与无知所囚禁，并且会因此继续被囚禁下去，给自己和他人带来更多痛苦。 现在请你用慈悲之眼看着自己和他人，像个听见宇宙所有众生哭喊的圣徒，而这位圣者的声音，就是每一个彻见实相的人的声音。就像有一部佛经中曾经这么描述大悲观世音菩萨的声音： 这绝妙之声，倾听了苍生哭喊的声音， 这尊贵之声，超越了世间之声的涨潮中 让我们的心与那声音共鸣。 将疑虑抛到一边， 谛观世界之声的倾听者之纯净神圣， 因为那就是我们在痛苦挫折、灾难死亡时的倚赖。 具足一切功德，慈眼凝视着所有众生， 使祝福之海无限宽广， 在衪前面，我们应当顶礼。 练习以慈悲之眼观众生：这种禅修法可以称为「慈悲观」。 慈悲观必须在静坐以及在为他人服务的每一时刻修习。不管你去哪里或是坐在哪儿，都要记得这个神圣的呼吁－－「以慈悲之眼观众生」。 禅修的主题和方法有很多，多到我从来没想过要把它们写下来给朋友们看。在这里我只提过一些简单但是很基本的方法。一位和平工作者就像其它任何人一样，她或他都必须过自己的生活。 工作只是生命的一部分；但是如果在正念中工作，工作就是生命。否则，人们就会像是行尸走肉。 我们必须点亮自己的火炬支持下去。然而，我们每个人的生命都与身边的人相系。如果我们知道如何活在正念中，如果我们知道如何留意保护自己的心灵，那么我们的兄弟姊妹也会因此懂得如何活在正念中。 禅修：默示和疗愈 在正念中静坐，我们的身心都会变得平和而且完全放松。但这种平和放松的状态，和人们在休息和打盹时心的慵懒、半无意识状态，有根本上的不同。在这种远离正念、慵懒和半无意识的状态中坐着，就像坐在黑暗的洞穴中。在正念中，一个人不仅平静快乐，而且会更灵敏警觉。 禅修并非逃避；它是与实相宁静地照面。 修习正念的人应该像汽车驾驶一样警觉；如果修行者不警觉，他的心就会被散乱与漫不经心（失念）所盘据，就像昏昏欲睡的驾驶，很可能造成大车祸。你得像踩着高跷那般警觉－－只要踏错一步就会 跌倒。 你得像赤手空拳、走在剑林中的中世纪骑士一样。你得像头狮子，以缓慢、轻柔而坚定的脚步向前迈进。只有怀着这样的警戒心，你才能彻底地觉悟。 对初学者，我建议修习清净观（纯粹辨识）：辨识而不加以评断。不管是怎样的感受，是慈悲或是苦恼，都应该展臂欢迎，辨识它，并且平等看待；因为这些感觉都是我们自己。我正在吃的橘子就是我。我正在种的芥菜就是我。我全心全意地种植，我用浴佛或浴耶稣的那种全心全意来洗茶壶。在正念中，慈悲、苦恼、芥菜和茶壶，都是神圣的。 如果被悲伤、不安、愤怒、激情或任何其它感受占据，似乎就很难修习清净观。这时候，不妨转而禅观一个静物，以自己的心态做为禅修的主题。这样的禅修法能够揭露实相并有疗愈的功能。 在禅观的凝视下，悲伤或不安、愤怒、激情会显现它的本性－－这种显现能自然地带来疗愈与解脱。悲伤（或者任何导致痛苦的事物）能做为从痛苦与折磨中解放的方法，就像用一根剌拔除另一根刺。我们该温柔而充满敬意地对待自己的不安、痛苦、愤怒与激情，不要排拒它，而是与它共处、跟它和解，借着禅修缘起观而契入它的本性。 一个人很快就能学到如何选择适合当下情境的禅修主题。所有禅修的主题，像缘起观、慈悲观、无我观、空观、无往观等等，都能够展露实相并带来疗愈。 不论如何，禅观这些主题要有所成，必须拥有相当的专注力（定力）。而要获得这种定力，就要靠日常生活中的正念修习，也就是观察辨识所有当下发生的事。 但是禅修的对象必须是真的深植你内心的实际问题，而非仅仅是哲学思辨的主题。 每一个主题都该像是必须长时间烹煮的食物。我们把它放在锅中，盖上锅盖，然后点火。锅子就是我们自己，而用来烹煮食物所需的热能则是我们的定力。燃料则来自持续不断地修习正念。没有足够的热能，食物就没办法煮熟。但是一旦煮熟了，食物就会彰显它的本性，并帮助和引导我们迈向解脱。 水更清，草更绿 佛陀曾说，生死的问题本身就是正念的问题。一个人究竟是生是死，就看他有没有保持正念。在《南传相应部》经典中，佛陀说了个发生在小村庄的故事： 一位知名的舞蹈家来到小村庄，村民蜂拥到街道上想一睹风采。在这同时，一名罪犯被命令拿着满满一碗的油穿越这个村庄。他必须全神贯注保持碗平稳，因为只要有一滴油从碗里泼到地上，紧跟在他身后的士兵就会抽出剑来砍掉他的头。故事讲到这里的时候，释迦牟尼佛问道：「现在，你认为这囚犯能不能只专注于这只装油的碗，而不心思游移，去偷瞄镇上的那名舞蹈家呢？街 上的村民那么骚动，当中任何一个人可能随时会撞到他，他会不会去看这些人呢？」 还有一次，释迦牟尼佛讲了另一个故事，让我突然领悟个人自己修习正念的无上重要性，也就是保护和照顾好自己，而不要去注意其他人是怎样照顾他们自己的；「不照顾自己的心念而只在意别人这样那样」是一种心的习气，会导致忿恨或不安惯。释迦牟尼佛说： 从前有一对杂技艺人，老师是个穷鳏夫，徒弟是个叫做美达的小女孩。他们在街头表演，好挣钱糊口。他们的表演道具是根长竹竿，老师把竹竿竖在头上并保持平衡，小女孩则顺着竹竿缓缓爬到顶端。老师绕行走路时，小女孩则保持不动。 他们两个都必须全神贯注、保持完美的平衡，以防发生事故。一天，老师教学生：「听着，美达，我以后会看着妳、而妳也要看着我，这样我们就能帮助对方保持专注和平衡，以免发生意外。这样，我们肯定能赚到足够吃的东西。」但是小女孩很聪明，她回答：「亲爱的老师，我想我们最好各自顾好各自的。各自顾好各自就是顾好我们俩儿。这样我能肯定我们不会出事儿，并且挣足吃饭钱。」 佛陀说：「这个孩子说得对。」 在一个家庭里，如果有一个人修习正念，全家都会变得比较正念分明。因为这个活在正念中的家庭成员的存在，会提醒全家人活在正念中。假如一个班级里有个学生活在正念中，那么整个班级都会受影 响。 在和平服务团里，我们必须遵守同样的守则。如果身边的人没尽力，别烦恼，只要挂虑怎么样让自己做得出色即可。自己尽全力，就是提醒身边的人尽最大的努力。无论如何，要能做出贡献，就要持续地修习正念。这是毫无疑问的。 只有借着修习正念，我们才不会迷失自己，而能获致光明的喜悦与平和。只有借着修习正念，我们才能以开放之心和慈悲之眼看待任何人。 刚刚我受一位朋友之邀到楼下喝杯茶，那位朋友名叫克莉丝坦，来自荷兰，经常协助我们工作。在她住的公寓里有一架钢琴。当克莉丝坦倒茶给我时，我看着她那堆工作说：「能不能停一下，为我弹一段钢琴，待会儿再翻译孤儿救助申请书？」克莉丝坦很高兴地暂时放下工作，坐在钢琴前弹了一首她自幼就熟悉的肖邦选曲。 这首作品有数小节很轻柔且富旋律性，但其它段则喧嚣急促。她的狗本来趴在茶桌下，当音乐变得激昂时，牠就开始吠叫并且呜呜哀鸣。我知道牠觉得不舒服，想要音乐停下来。 克莉丝坦的狗一直像小孩般被人宠爱着，而且可能比大多数的小孩对音乐更敏感。牠会如此反应或许也足因为牠的耳朵能接收人类听不到的频率。克莉丝坦试着边安抚这只狗边继续弹琴，但是无济于事。 她结束这段曲子，转而弹奏莫扎特一首轻快和谐的曲子。现在狗儿只是安静地躺着，显得很平和。克莉丝坦弹完曲子后，到我身边坐下说：「通常就算我弹肖邦最轻柔的曲子，这只狗也会跑来抓住我的裤管，试着强迫我离开钢琴。有时候我得把牠赶到门外才能继续弹。但是只要我弹巴哈或莫扎特的曲子，牠都很平静。」 克莉丝坦提到有报告指出，在加拿大，人们试着在夜间为他们的植物弹奏莫扎特的作品，结果，植物长得比平常还快，而且花儿还会往有音乐的方向生长。还有人每天在小麦及黑麦田中弹莫扎特的曲子，结果测量出这些田中的小麦和黑麦长得比别的田快。 当克莉丝坦这么说时，我想到了会议室。人们在里头争辩不休，愤怒和非难之辞你来我往。如果有人把鲜花和植物放在这样的房间里，它们很可能会停止生长。 我又想到一座花园里，它是由活在正念中的出家人所照管。他的花儿总是清新鲜翠，被他从正念中流涌出来的平和与喜悦滋养灌溉着。有一位古人这么说过： 圣人出，川水清，草叶碧。 在每个会议或讨论开始时，我们都该听听音乐或静坐，并修习呼吸。 七、三个绝妙答案 皇帝的三个问题 结束前，让我再讲一次托尔斯泰（Leo Tolstoy）写的一个小故事，这故事是关于皇帝的三个问题。托尔斯泰不知道这位皇帝的名字‥‥ 一天，有个皇帝想到，只要他知道三个问题的答案，行事就不会再有差错了。 .做每件事的最佳时机为何？ .与你共事最重要的人是谁？ .在任何时候，要做的最重要的事是什么？ 皇帝下令遍贴公告，声明如果有人能回答以上问题，就能够得到重赏。许多人读了公告之后马上赶到皇宫，每个人都给了不同的答案。 有人对第一个问题的建议是，皇帝得订出一个时间表，将每年、每月、每天、每个小时该做的事都规画得好好的，然后再照表操课。只有这样，皇帝才能在对的时间做对的事。 另一个人认为，事先就计划好所有事是不可能的，皇帝应该把无谓的休闲娱乐放在一边，且对每一件事保持关注，这样才能知道什么 时候该做什么事。 还有人坚持说，皇帝若光靠自己，不可能有足够的先见和能力去决定什么时候做什么事，因此皇帝必须设立一个智囊团，依照智囊团的忠告行事。 有人则说，有些事必须立即决定，没时间等大家商量；但是如果皇帝想预要先知道会发生什么事，那就应该询问术士和预言师。 第二个问题的答案也是众说纷纭。 有人说皇帝应完全信任他的臣子，有人则认为该信赖神父和法师，还有人认为应该相信医生。也有人对武士充满信心。 众人对第三个问题的答案也没有共识。 有些人说最该追求的事是科学；其它人坚持说是宗教；还有人主张军事技术。 皇帝对每个答案都不满意，一分赏赐都没给。 在沉思好几个晚上之后，皇帝决定去拜访一位住在山上、据说已经开悟的隐士。皇帝希望能找到隐士问他那三个问题，虽然他知道这隐士从不离开山上，而且只见穷人，不愿与权贵之士有任何往来。所以，皇帝装扮成穷农夫，要他的侍从在山下等他，而独自登山去找那位隐士。 到达这位圣者居住的地方后，皇帝发现那隐士正在小屋前的菜园翻土。当他看到有陌生人到来，只是点头致意一下，就继续埋首翻土。这活儿对他来说显然很吃力。他很老了，每次把铲子戳进地面、再把土翻上来，都会上气不接下气。 皇帝走近他，说道：「我到这儿来，是想问你三个问题：做每件事的最佳时机为何？与你共事最重要的人是谁？在任何时候，要做的最重要的事是什么？」 这位隐士注意地听着，但他只拍拍皇帝的肩膀，然后就继续翻土。皇帝说：「你一定累了。来吧，让我帮帮你。」隐士谢过他，把铲子交给皇帝，就坐在地上休息。 翻了两排土之后，皇帝停下来转向隐士，重复他的三个问题。隐士仍然不回答，只是站起来指着铲子说：「你怎么不休息一下？我现在可以接手了。」但皇帝继续翻土；又过了一个小时，转眼也过了两个小时。 最后，太阳开始往山后沉落。皇帝终于放下铲子，对隐士说：「我到这儿来，是要看你能否回答我的三个问题，但是你却不给我任何答案。请告诉我答案，我好回去。」 隐士抬起头问皇帝：「你有没有听到那边有人在跑？」 皇帝转过头，他们两个看到林间有个蓄白色长须的男人，他手按着肚子上流血的伤口狂奔，向着皇帝跑来，却失去神识，跌倒在地， 痛苦地呻吟。皇帝和隐士掀开男人的衣服，看到男人有个被砍得很深的伤口，皇帝很仔细地清洗男子的伤口，并拿自己的衣服去包扎，但血还是在几分钟内就浸透了衣服。皇帝把衣服冲洗干净，再次包扎伤口，这样反复了好几次，血才止住了。 最后，这个受伤的男人恢复意识，向他们要了一杯水喝。皇帝跑下河边，带回一壶干净的水。那时，太阳已经完全沉落，开始变冷。隐士帮忙皇帝将那人扶到小屋里，让他躺在隐士的床上。那个人闭上眼睛，安静地躺着。皇帝在这又是爬坡又是翻土的漫长一天后累坏了，倚着门口就睡着了。 他醒来时，太阳已经升上山头。他一时忘了自己身在何处、又为什么来到这里。他往床那边看去，看到受伤的那个男子也正慌乱地看着他。当那个人与皇帝四日交接，他定定地凝视皇帝，轻轻地低声说道：「请原谅我。」 「你做了什么，要我原谅你？」皇帝问。 「你不认识我，陛下，但是我认识你。我是你的死敌，而且我立誓要复仇，因为在上次那场战役中，你杀了我兄弟，又抢走我的财产。当我知道你独自来这座山找隐士，我决定在你回程的路上突袭你，杀死你。但是，我等了很久都没看见你的踪迹，就离开埋伏的地方，想把你找出来。可是我没找到你，倒是先碰见你的侍从们，他们认出我来，砍伤了我。很幸运地，我逃走了，一直跑到这儿来。如果我没碰见你，现在一定死了；我本来想杀你，你却反过来救了我！我说不出心中的羞愧和感激。如果我活着，我发誓余生都做你的奴仆，而且我 要子子孙孙都这样侍候你。请饶恕我吧。」 皇帝非常高兴这么容易就与宿敌和解。他不但原谅这男人，许诺将他所有的财产还给他，还派遣御医和仆人去医护这男人，直到他康复。命令侍从护送男人回家后，皇帝回去见隐士。他想在回宫前最后一次问隐士那三个问题。他发现隐士正在他们昨天翻的地上洒种子。 隐士站起来看着皇帝。「但是你的问题已经有答案了。」 「怎么说？」皇帝困惑地问。 「昨天，如果你没有同情我年老，帮我翻土的话，你早就在回去的路上被那个人攻击，然后你就会后悔怎么不留下来跟我在一起。 「因此，最重要的时候就是就是你在翻土的时候，最重要的人就是我，最重要的事就是帮我。之后，当那受伤的男人跑到这儿来，最重要的时候就是你照料他伤口的时候，因为要是你没有照料他，他就会死，而你也会失去与他和解的机会，因此，他是最重要的人，而最重要的事就是照料他的伤口。 「记住，最重要的时候永远只有一个，那就是『现在』。『现在』是我们唯一能主导的时间。最重要的人永远就是那个当下和你在一起、在你面前的人，因为谁也不知道将来你是不是还会和别人共处。最重要的事就是让你身旁的人快乐，因为这就是人生所追求的。」 托尔斯泰写的这个故事很像佛经里的一个故事，不逊于任何神圣 的经文。我们会谈到社会服务、为人民服务、为人类服务、为远方的人服务、为世界和平尽力，但是我们经常忘记，我们首先要为自己身边的人而活。 如果你不能为你的妻子或丈夫、孩子、父母服务，你要怎么服务社会？如果你没法让自己的孩子快乐，你怎么能期望让别人快乐？如果我们在和平运动和服务团体的朋友不能互相爱护帮忙，我们能爱护帮助谁？我们是为了其它人类工作，还是只是为了组织的名字工作呢？ 为谁服务？ 为和平而服务。为任何需要的人服务。「服务」的范围无所不包。让我们先回到较一般的范围：我们的家庭、我们的同班同学、我们的朋友、我们的小区。我们必须为了他们而活；因为要是我们不为他们活，我们要写谁而活？ 托尔斯泰是一位圣徒－－我们佛教徒会称他为「菩萨」。但是，皇帝自己能够看到生命的意义和方向吗？ 我们要如何活在当下、在此刻与周遭的人一起活在当下、帮他们减轻痛苦、让他们过得快乐点？到底要怎么做？ 答案是：我们必须修持正念。 托尔斯泰给的原则看起来很简单，但是我们若想要付诸实践，就得藉助正念的方法去寻找这条道路。 我为了方便我们的朋友运用而写下这些。有许多人虽然曾写下这些道理，却没有亲身实践它们，但是我只写下我所实际经验、体会到的事。我希望你和你的朋友在求道－－回归之道－－时，会觉得这本书有点用处。 八、正念的练习 禅修的入门练习 这里有些我经常使用的禅修练习和方法，我会依不同的状况和当下的喜好择一运用。选个你最喜欢且最适合你的。每个方法有效与否，视个人特定的需求而有所不同。虽然这些练习相对来说很简单，但却是要成就所有其它事物的基础。 早晨醒来时，微笑 在天花板或墙上挂一根树枝或其它标示，甚至是「笑」这个字，好让你一张开眼就能看到。这个标示有提醒你的功用。 利用起身前的片刻掌握好呼吸。轻轻地吸进并吐出三口气，同时保持微笑。 随顺你的呼吸。 闲暇时，微笑 不管在任何地方坐着或站着，记得微笑。 看着一个小孩、一片叶子、一幅墙上的画或任何其它相对来说静定的事物，微笑。 安静地吸气及吐气三次。保持微笑，将你专注的所在处视为你的真实自性。 听音乐时，微笑 听一段音乐，听上两、三分钟。 专注在歌词、曲调、旋律及音乐情境上。 观照你的呼吸时，微笑。 发怒时，微笑 当你意识到自己在发怒，立刻微笑 安静地吸气吐气三次，保持微笑。 平躺卧姿放松 背躺在一片平面上，不要用褥垫或枕头支撑。 双臂放松，平放在身体两侧，双脚微微张开，向外舒展。 保持微笑。 轻轻地吸气吐气，专注于你的呼吸。 放松全身肌肉。放松每一块肌肉，就好像它正要沉到地底下，或是像悬挂在微风中的一匹丝绸那般柔顺。 完全地放松，只要专注于自己的呼吸和微笑。 把自己想成一只猫，全身软绵绵地躺在温暖的炉火前。当猫的筋肉松弛下来，任何人的抚触牠都不会抗拒。 继续这样的观想，持续呼吸十五次之久。 坐姿放松 结踟跌坐或半跏跌坐，或是双腿交叉而坐（印第安式坐姿），或是跪坐（日式坐姿），甚至坐到椅子上，两脚着地。 微笑。 吸气及呼气，保持微笑。 放松。 深呼吸 背靠着地躺着。 平稳轻柔地呼吸，把注意力集中在胃的高低起伏。 当你开始吸气时，让腹部鼓起，好把空气带进肺的下半部。 当肺的上半部开始充满空气时，你的胸腔会开始鼓起，腹部则会瘪下去。 不要让自己累着了。 像这样继续练习呼吸十次。 一般来说，呼气会比吸气来得久些。 用脚步测量呼吸 缓慢悠闲地散散步，不管是在花园里、沿着河或在乡村小径上都好。 像平常那样呼吸。用脚步数来测量你呼吸的长度，即呼气和吸气的时间。 像这样继续几分钟后，开始借着多数一步来拉长你的呼气，试着拉长呼气时间。 不要强迫自己拉长吸气。自然就好。用心观察，看看自己是不是会想刻意拉长吸气。像这样继续呼吸十次。 现在，再多数一步来拉长呼气。 注意吸气是否也因为多了一步而拉长。只有在觉得拉长吸气会带来喜乐时才拉长吸气。 像这样呼吸二十次，就恢复平常那样的呼吸。 五分钟后，可以再开始拉长呼吸的练习。 觉得有点疲倦了，就回复平常的呼吸。 反复几次拉长呼吸的训练后，呼气和吸气的时间会渐渐变得相等。 不要练太久，练习吸呼时间相等十到二十次就好，然后就回复到平常的呼吸。 数呼吸 结半踟趺或踟趺坐，或者散散步。 吸气时，要保持觉知：「我正在吸气，一。」 呼气时，也要保持觉知：「我正在呼气，一。」 记得要从腹部呼吸。 开始第二次吸气时，要保持觉知：「我正在吸气，二。」然后慢慢呼气，同样保持觉知：「我正在呼气，二。」 像这样一直数到十，然后再从一开始数起。 只要数错或忘了数，就回到一重新开始。 听音乐时，随顺你的呼吸 听一段音乐。 深长地、轻柔地、平稳地呼吸。 随顺你的呼吸，但做它的主人，同时对音乐的旋律与情境保持觉知。 不要迷失在音乐中，要继续做你的呼吸和你自己的主人。 谈话时，随顺你的呼吸 深长地、轻柔地、平稳地呼吸。 在听朋友说话以及自己的回答时，随顺你的呼吸。 就像听音乐时那样继续练习。 随顺你的呼吸 结半踟跌或踟趺坐，或散散步。 （从腹部）轻缓而平常地吸气，并保持觉知：「我正像平常那样地吸气。」 呼气时，同样也觉知：「我正像平常那样地呼气。」 像这样继续呼吸三次。 在第四次呼吸时，拉长吸气，并保持觉照：「我正深深地吸进一口气。」保持觉照地呼气：「我正深深地呼出一口气。」 继续呼吸三次。 现在，用心地随顺你的呼吸，觉知你的腹部和肺部的每一个动作。 跟着气息的出入。保持觉知：「我正在吸气，并自始至终都跟随着我的吸气。我正在呼气，并自始至终都跟随着我的呼气。」 像这样继续呼吸二十次。再回到平常的呼吸。 五分钟后，重复方才的练习。 记得呼吸时要保持微笑。 一旦你能掌握这个练习，就可以继续下一个练习。 运用呼吸，静定身心以得乐 结半踟趺或跏趺坐。 微笑。 随顺你的呼吸。 当你的身心都安静下来，继续非常轻柔地呼吸，并保持觉知：「我正在吸气，让全身轻安。我正在呼气，让全身轻安。」 像这样持续呼吸二次，在觉照中生起这样的想法：「我正在吸气，让全身轻盈、平和、喜悦。」 继续呼吸三次，并在正念中浮现这样的想法：「我正在吸气，而我的身心都平和喜悦。我正在呼气，而我的身心都平和喜悦。」 在觉照中维持这样的想法五到三十分钟、或是一个小时都可以；这要看你的状况和你有多少时间而定。 练习开始和结束都保持放松且轻柔。 如果你想停止练习了，先用双手轻柔地按摩双眼、脸颊以及腿上的肌肉，再回到平常的坐姿。 稍等片刻再站起来。 觉照身体的姿势 这个方法可以在任何时间、任何地点练习。 先专注于呼吸上，比平常安静且深长地呼吸。 不管你在走路、站，、躺卧或坐着时，都对你身体的姿势保持觉知。 要知道你在哪里走路；知道你站在哪里；知道躺卧在哪儿；知道坐在哪儿。 对你的身体为什么处于这种姿势，保持觉照。 举例来说，意识到你之所以站在山丘上是为了让自己恢复精力、还是为了练习呼吸，或者就只是为了站着。 假如你没任何目的，他要清楚觉察自己并没有任何目的。 泡茶时，保持觉照 准备一壶茶款待客人或泡给自己喝。 在觉照中缓缓地进行每个动作。 不要没有觉知，就让任何一个最细微的动作滑了过去，心中要了了分明。 觉知你的手正握住茶壶把子、提起茶壶。 觉知你将清香暖热的茶汁倒入杯中。 每一个步骤都要在正念中进行。 比平常更轻且更深地呼吸。 如果你的心散乱了，就先把持住自己的呼吸。 洗碗 轻松地洗碗，就好像每个碗都是你静观的对象。 把每个碗都看做是神圣的。 随顺你的呼吸，避免心思散乱。 不要想着快快结束这项工作。 把洗碗当成你生命中最重要的事情。 洗碗就是禅修。 如果你不能在正念中洗碗，你也不可能在静坐中禅修。 洗衣服 不要一次洗太多衣服。 只要挑出三或四件衣服来洗。 用最舒服的姿势站着或坐着，以免背痛。 放松地搓洗衣服。 注意自己双手双臂的每个动作。注意肥皂和水。 当你把衣服搓洗干净了，你的身心应该会感到像衣服一样干净清爽。 记住，只要你的心绪飞散，就保持微笑且掌握住呼吸。 打扫房子 将工作分成几个步骤：清理东西、收整书籍、刷洗厕所、擦净浴室，打扫地板和清除灰尘。 为每样工作安排好相当充裕的时间。 动作要慢，比平常还慢三倍。 对每样工作都全神贯注。 举例来说，整理架上的书时，看著书，注意它是哪本书，觉知自己正要把它放在架上，想要把它放在哪个位置。 觉察自己正伸手去构书，并取下它。 避免任何突然或粗鲁的动作。 维持对呼吸的觉照，特别在你思绪涣散飘移的时候。 慢慢洗个澡 给自己三十到四十五分钟洗个澡。 一秒也不要急。 从一开始准备热水，到最后穿上干净的衣服，每个动作都要保持轻缓。 注意每个动作。把注意力放在身体每个部位上，不要有区别，也不要害怕。 觉知你身上每一道水流。 在你洗完时，你的心应该会像身体那般轻盈平和。 随顺你的呼吸。 想象自己身处于夏日洁净清香的莲花他中。 鹅卵石 静静地坐着和缓缓地呼吸时，把自己想成是一颗穿过清澈河流而沉落的鹅卵石。 下沉时，没有任何目的引导你的动作；朝着河床柔软的沙地那完全的休憩处沉落。继续禅观那鹅卵石，直到你的身心都得到完全的休息，就像那颗在沙地上休憩的鹅卵石。 将这样的平和喜悦持续半个小时，同时观照自己的呼吸。 没有任何关于过去或未来的念头，能够把你从当下的平和喜悦中带离。 宇宙就存在于当下。 没有任何欲望能将你自此刻的喜悦中拉走，即使是成佛或度众生的大愿都不能。觉知到无论是要成佛或者度众生，都必须以当下纯净的安详为基础才能实现。 正念日 从一星期抽出一天来，哪一天都好，只要能配合你的状况。 忘掉你在其它天要做的工作。不要安排任何聚会或接待任何朋友 来访。 只要做些简单的工作，像是打扫房子、做饭、洗衣和情灰尘。 一旦房子整洁干净，东西也都各归其位，就好好地洗个澡。 之后，准备泡茶，喝茶。 你可能会读读经文或写信给好朋友。 再之后，散散步来练习。 在读经或写信时，都要保持觉照，不要让经文或信把你的心给牵引到其它地方去。 读经时，要觉知你正在读什么；写信时，觉察到你正在写什么。 遵循同样的步骤，就像你在听音乐或和朋友聊天时所做的一样。 傍晚时给自己准备清淡的一餐，也许只要一点水果或一杯果汁就好。 睡前再静坐一个小时。 在这一天，要散步两次，每次半小时到四十五分钟。 睡前别再读任何书，而是练习彻底的放松五到廿分钟。 做你呼吸的主人。 闭上眼睛，轻柔地呼吸（但别呼吸得太长），并跟随着你的腹部及胸腔的起伏上下。 在这一天的每个动作，都该比平常至少慢两倍。 静观相互依存（缘起观） 找一张你年幼时的照片。 结半踟趺或踟趺坐。 开始跟随你的呼吸。 呼吸二十次后，开始专注于你眼前的这张一照片。 回忆并且再体验那在拍摄这张照片时组成你的五蕴：你那个年龄时的身体特征（色）、你的感觉（受）、知觉（想）、心智（受）与意识（识）。 继续跟随你的呼吸。 别让记忆把你给吸引走或将你完全席卷。 像这样禅观十五分钟。 保持微笑。再将觉照移转到当下的自己。 觉知你此刻的身体、感觉、知觉、心智与意识。观照组成你的五蕴。 问自己这个问题：「我是谁？」这个问题该被深植在你的心中，就像深埋一颗新生的种子在松软的泥土中，并且浇水滋润。 「我是谁？」这个问题，不该被视为一个得用推论性思维去思索的抽象问题。 「我是谁？」这个问题，不可能借着理性思考来回答，而只能透过观照整个五蕴和合来面对。 别试着寻找一个理性的答案。 禅观十分钟，保持轻缓但深长的呼吸，以避免被哲学性思考拉走。 你自己 独自坐在一间黑暗的房内，或者夜里独坐在河岸边，或任何其它能够独处的地方。 开始掌握住自己的呼吸。 生起这个念头，「我将用手指指着自己，」然后指向相反的方向，而非指向自己的身体。 禅观你自己外于你的色身而存在。 禅观你的色身就在你前面－－在树间、草地上、叶缝中，在河里。 清楚觉察到你就在宇宙中，而宇宙也在你之中；假如宇宙存在，你就存在；假如你存在，宇宙也就存在。 既无生；亦无死。既无来；也无去。 保持微笑。 掌握呼吸。 禅观十到二十分钟。 你的骸骨 用个你觉得舒服的姿势躺在床上、垫子上或草地上。 不要用枕头。 开始掌握你的呼吸。 想象你全身只剩下一副白森森的骸骨，躺在地球表面上。 保持微笑并且继续跟随你的呼吸。 想象你所有的肌肉都腐烂、消散，只有骸骨躺在地球上；想象你的骸骨在葬后已在地下躺了八十年。 仔细观察你的头骨、脊骨、肋骨、体骨、腿骨和臂骨，还有手指骨。 保持微笑，极轻柔地呼吸，心和大脑皆澄澈宁静。 你会了解那副骸骨不是你。 你的色身不是你。 你与其它生命合而为一，不朽地活在树林与草地中，在其它人身 上、在鸟兽间、在空中和在海浪间。 骸骨只是你的一部分。 你无所不在、无时不在。 你不仅是色身，或甚至仅仅是受、想、行与识。 继续禅观二十到三十分钟。 你出生前的本来面目 结踟趺或半踟趺坐，跟随你的呼吸。 集中注意力在你生命的起点－－A。 认识到那也是你死亡的起点。 了知你的生命和死亡同时存在示现：「此」之所以存在，是因为有「彼」。如果「彼」不存在，也就不可能有「此」。（此 有故彼有，此无故彼无。） 了悟到生命与死亡的存在是相互依存附的：其中一个是另一个的基础。 了悟到你同时既是你的生也是你的死，这两者并不敌对，而是同一实相的两面。 然后，集中注意力在这双重示现的结束点－－B，它总被误称为「死亡」。 了悟到它乃是你生命示现的终点，也是你死亡示现的终点。 了悟在A之前和B之后，其实并没任何差别。 寻找你在A之前与B之后的本来面目。 一位逝去的挚爱 用你觉得舒服的姿势坐在椅子上或躺在床上。 开始掌握自己的呼吸。 禅观一位逝去的挚爱，不论他走了几个月或几年。 清楚地认识到他全身的筋肉都已经腐化，只剩下一副骸骨静静地躺在地下。 清楚地认识到你自己的筋肉仍在这儿，仍在你的身上，仍然有色、受、想、行、识这五蕴的聚合。 想想你和这个人过往与如今的互动。保持微笑，并且掌握呼吸。 像这样禅观十五分钟。 空 结踟趺或半跏趺坐。 开始调节你的呼吸。 禅观五蕴（色、受、行、想、识）之和合中的空性。 逐一禅观五蕴中的每一蕴；要观到一切都在变化，都是既无常也无我的。 五蕴的和合就像所有现象的聚合：都依循着相互依存的法则。 它们的聚合与解离，就像是围绕着山顶的云雾的之聚集与消散。 既不要执着也不要否认五蕴。 要了解，「喜欢」或「憎厌」也都是现象，属于五蕴的和合。 清楚觉察五蕴是空的，是无我的；但它们也都是绝妙非凡的存在，就像宇宙中所有的现象或存在任何地方的生命一样奇妙。 试着看出五蕴并没有真的历经生灭，因为它们本身就是终极实相。 借着这个静观，试着看出无需是一个概念、无我是个一概念、空也是一个概念，你就不会被无常、非我和空的概念所囚禁。 你将看到空也是空，而空的终极实相与五蕴的终极实相，其实没有差别。 （修行者必须彻底练好前面五个练习后，才能做这项练习练习时间视个人而定，可能是一小时，也可能是两小时。） 对你最恨的或最嫌恶的人的慈悲 静静地坐着。 呼吸并微笑着。 禅观那个最让你受苦的人的影像。 观想他让你最恨、最轻视或最厌恶的特质。 试着仔细观察这个人的日常生活中什么会让他快乐，什么又会折磨他。 禅观这个人的「想」；试着看透这个人依循什么样的思考模式和推理方式。 观察这个人希望和行动的动机为何。 最后，观照这个人的「识」。 看看他的观点和知见是否开阔自由，是否被任何偏见、狭隘心胸、憎恨或愤怒影响。 看看他是否是自己的主人。 像这样继续下去，直到你感到慈悲在你心底升起，就好像一口充满了清新的水的井，而你的愤怒和怨恨已然消散。 对同样的人反复做这个练习。 因缺乏智慧所造成的痛苦 结跏趺或半踟趺花坐。 开始跟着你的呼吸。 选一个就你所知最痛苦的人或最痛苦的家庭，或者最痛苦的一个社团的情形，做为你禅观的主题。 以个人为禅观的主题时，尽量看出那个人正在经历的一切痛苦。从色身的痛苦（疾病、贫困、客观环境所造成的痛苦）开始，之后，开始静观由「受」所造成的痛苦（内在的矛盾冲突、恐惧、仇恨、嫉妒和内疚）。然后再观由「想」所造成的痛苦（悲观、用阴郁狭隘的心态来思惟他所面临的问题）。看看他的「行」，是否被恐惧、失望、绝望或仇恨所驱使。看看他的「识」，是否因为他的处境、他的烦恼、他周遭的人、他所受的教育、倡导或者缺乏自制而封闭起来。 静观这些痛苦，直到你的心生起像一股清泉般的慈悲，直到你能看出那个人是因为环境和无知而在受苦。你决心尽可能用最安静、最谦逊的方式去帮助那个人脱离当前的困境。 以家庭为禅观的主题时，仍遵循相同的方法。先观察一个人的所有痛苦，然后再观察另外一个人，直到你洞察整个家庭的痛苦。看出他们的痛苦就是你自己的痛苦。看出你不可能谴责这群人中的任何一个人。看出你必须尽可能以最安静、最谦逊的方式帮助他们从当下的困境中解脱。 以社团为禅观的主题时，可以拿一个国家的情形当例子，而这个国家正饱受战争或其它不公不义现象而引起的痛苦。尽量看出每个卷入这场斗争的人都是个牺牲品。看出没有人希望痛苦继续下去，包括所有那些彼此斗争的党派或者看起来相互对立的两造。看出不是某一个人或者少数几个人要为这种情形而被谴责。 看出这种情形之所以存在，乃是因为执着于意识形态，执着于一个缺乏公义的世界经济体系，而这个体系之所以屹立不摇，是因为每 个人的无知和缺乏改变它的决心。看出冲突的双方并不是真的对立，而只是同一实相的两个面向。看出最重要的事就是生命，互相杀戮和彼此压迫并不能解决任何问题。 记住《维摩诘所说经》中的话： 劫中有刀兵，为之起慈悲。 化彼诸众生，令住无诤地。 若有大战阵，立之以等力。 菩萨现威势，降服使和安。 持续禅观，直到一切责备和仇恨都消失，直到慈悲像一股清泉在你心中生起。发誓尽可能以最安静、最谦逊的方式，为觉悟和和解而工作。 无住行 结踟趺或半跏趺坐。跟着你的呼吸。 取一个乡村发展计划或任何你觉得重要的计划，做为你禅修的主题。检查这项计划的目的、将运用的方法，以及与它相关的人。首先思考这项计划的目的。 洞察这项工作志在服务、在减轻他人痛苦、在生慈悲心，而非满足被赞美或被认同的欲望。接着，洞察这个计划所使用的方法是在鼓 励人与人之间的合作。别将这个计划视为一种施舍的举动。最后，考虑与这计划相关的人。 你还在用谁在奉献、谁在获益的角度来看这件事吗？如果你仍然对谁是服务者、谁是获益者有所区别，你是为了你和其它服务者而工作，而不是为了服务而服务。 《金刚经》说：「如是灭度无量无数无边众生，实无众生得灭度者。」决心要以「无住行」的精神工作。 不即 结跏趺或半踟趺坐。跟着你的呼吸。 回忆你人生中重大的成就，逐一检查它们。检查引导你迈向成功的才华、品格、能力与其它有利的条件。 你认为自己是成功的最主要原因，并因而感到自满和傲慢，检查这两种情绪。以缘起观来观照这整件事，看出你以为的成就并非真属于你自己，而是属于非你所能掌控的各种因缘条件的和合。 观照到这点，你就不会再执着这些成就。只有当你能舍弃它们的时候，你才能真的自由，不再被它们所困扰。 回忆你生命中最痛苦的挫败，逐一检查它们。检查你的才华、品 格、能力与其它导致你挫败的不利条件。 检查你心中觉得自己无能成功所涌出的复杂情绪。以缘起观来观照这整件事，看出你之所以挫败并非因为你无能，而是因为缺少有利的因缘条件。看出你根本无能为力去承担这些挫败，看出这些挫败并不是你个人的事。 观照到这点，你就能自其中解脱。只有当你能舍弃它们的时候，你才能真的自由，不再受它们干扰。 不离 结踟趺或半跏趺坐。跟着你的呼吸。 通用缘起观的一种练习：观你自己、观你的骸骨、或观逝去的挚爱。 看出一切事物皆无常，没有永恒的实体。 看出虽然事物无常，没有无永恒的实体，然而它们却绝妙非凡。 当你不再被所有的因缘条件束缚，也就不再被不是因缘条件的东西所束缚。 看看圣人，虽然不被缘起观的教法所限制，却也不背离这个教法； 虽然能弃绝这个教法，犹如主是一堆灰烬，却他能常安住其中而不被它所溺没，犹如水上行舟一般。 一直禅观，直到你了悟，悟道者不会被救度众生的工作所奴役，但也永远不会放弃救渡众生的工作。 附 录佛经选读 正念的基础：《大念处经》 出处：巴利经典长部第二十二经 我是这样听说的：有一次，世尊在拘楼国剑磨瑟达磨城中，与拘楼人在一起。当时，世尊对比丘们说：「比丘们！」比丘们回答：「世尊！」世尊接着说了以下的开示： 比丘们！只有一条道路可以使众生清净、克服愁叹、灭除苦忧、获得正道、体证涅盘，这条道路就是四念处。是哪四个念处呢？ 比丘们！比丘就身体观察身体，精勤、觉知、时时彻知无常，去除对身心世界的贪瞋；就感受观察感受，精勤、觉知、时时彻知无常，去除对身心世界的贪瞋；就心观察心，精勤、觉知、时时彻知无常，去除对身心世界的贪瞋；就诸法观察诸法，精勤、觉知、时时彻知无常，去除对身心世界的贪瞋。 一、观身念处 观呼吸 比丘们！比丘如何就身体观察身体呢？ 比丘们！比丘到森林中，或到树下，或到隐僻无人之处，盘腿而坐，端正身体，把注意力放在嘴巴周围的区域，保持觉知，觉知呼吸 时气息的出入情况。入息长时，他清楚了知：「我入息长」；入息短时，他清楚了知：「我入息短」；出息长时，他清楚了知：「我出息长」；出息短时，他清楚了知：「我出息短」。他如此训练自己：「我当感受全身，而入息」；他如此训练自己：「我当感受全身，而出息」；他如此训练自己：「我当寂止身体的行动，而入息」；他如此训练自己：「我当寂止身体的行动，而出息」。 比丘们！就像技术熟练的木匠或他的徒弟，当他锯木做一次长拉锯的时候，清楚了知：「我做了一次长拉锯」；当做一次短的拉锯时，他清楚了知：「我做了一次短拉锯」。 比丘们！就像这样，比丘入息长时，他清楚了知：「我入息长」；入息短时，他清楚了知：「我入息短」；出息长时，他清楚了知：「我出息长」；出息短时，他清楚了知：「我出息短」。他如此训练自己：「我当感受全身，而入息」；他如此训练自己：「我当感受全身，而出息」；他如此训练自己：「我当寂止身体的行动，而入息」；他如此训练自己：「我当寂止身体的行动，而出息」。 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断灭去的现象，他同时观察身体当中不断生起、灭去的现象。 于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。 比丘们！这就是比丘如何就身体观察身体。 观姿势 又，比丘们！比丘在走路时，他清楚了知：「我正在走路」；在站立时，他清楚了知：「我正站立着」；在坐着时，他清楚了知：「我正坐着」；在躺着时，他清楚了知：「我正躺着」。无论何种姿势，他都清楚了知。 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。 于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 时时彻知无常 又，比丘们！当比丘来回行走时，时时彻知无常；当他看着正前方或侧面时，时时彻知无常；当他弯下身体或伸展身体时，时时彻知无常；当他搭衣持钵时，时时彻知无常；当他在吃、喝、咀嚼或尝味时，时时彻知无常；当他大小便利时，时时彻知无常；当他行走、站立、坐卧、醒觉、说话或沉默时，时时彻知无常。 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观 察身体当中不断灭去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 思惟不净 又，比丘们！比丘仔细思考这身体，自脚底而上，自头发而下，皮肤所覆盖的都是充满种种不净，他这么想：「在这身体中，有头发肤毛、指甲牙齿、及肤肌肉、筋腱骨髓、肾心肝脏、肋膜脾脏、肺肠肠膜、胃脏粪便、胆汁痰脓、血汗脂肪、眼泪淋巴、口水鼻涕、滑液尿水。」 就好像有一只两个口的粮食袋，里面装满各种的豆谷，诸如：稻米、糙米、绿豆、豌豆、芝麻、白米；而且就如同有位能分辨这些豆谷的人，当他打开这以袋子时，他可以看到里面所装的东西，告诉人说：「这是稻米、这是糙米、这是绿豆、这是豌豆、这是芝麻、这是白米。」 比丘们！相同地，比丘仔细思考这身体，自脚底而上，自头发而下，皮肤所覆盖的都是充满种种不净，他这么想：「在这身体中，有头发肤毛、指甲牙齿、皮肤肌肉、筋腱骨髓、肾心肝脏、肋膜脾脏、肺肠肠膜、胃脏粪便、胆汁痰脓、血汗脂肪、眼泪淋巴、口水鼻涕、滑液尿水。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观 察身体当中不断灭去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 思惟四大 又，比丘们！比丘仔细思考这身体，不论置身何处或何种姿势，依身体组成要素的特性，他这么想：「在此身中，有地大、水大、火大及风大。」 比丘们！这就像技术熟练的屠夫，或屠夫的学徒，杀了一条牛并将它分解成块后，他们坐在十字路口。比丘们！相同地，比丘仔细思考这身体，不论置身何处或何种姿势，依身体的组成要素，他这么想：「在此身中，有地大、水大、火大及风大。」 于是他就身体内部观察身体，就身体外部观察身体，同时同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 观墓园九相 （1） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，这尸体 已死一日、二日或三日，变成肿胀、瘀黑且溃烂，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （2） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，这尸体被乌鸦、秃鹰、猎鹰、苍鹭所啄食或被野狗、老虎、豹、胡狼所咬或被其它种种生物所食时，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （3） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，只剩下 骸骨、附着在骨上的一些血肉、及连结骨骸的筋腱，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （4） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，只剩下没有皮肉、只有一块块血迹的骸骨，和连结骨骸的筋腱，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断灭去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （5） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，只剩血 肉不存的骸骨，及连结骨骸的筋腱，他对白己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （6） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，只剩一堆骨节支解的骨头，四散各处：这儿是手骨，那里是脚骨；这儿有踝骨，那里有膝骨；这里有大腿骨，那里有骨盆骨；这是脊椎骨，那是肩胛骨；又有眉骨、颈骨、下颚骨、牙齿及头盖骨，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断灭去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （7） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，只剩下一堆泛白如海螺壳的骨头，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （8） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，经过年余、堆积成堆的骨头，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断灭去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 （9） 又，比丘们！当比丘在墓园里，看到一具被丢弃的尸体，骨头腐 蚀成粉，他对自己的身体这么想：「确实如此，我的身体也是这种性质，也将变成如此，而且无法避免这样的结果。」 于是他就身体内部观察身体，就身体外部观察身体，同时就身体内部、外部观察身体。因此，他观察身体当中不断生起的现象，他观察身体当中不断减去的现象，他同时观察身体当中不断生起、灭去的现象。于是他清楚觉知：「这是身体！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就身体观察身体。 二、观受念处 比丘们！比丘如何就感受观察感受呢？ 比丘们！比丘在经历快乐的感受时，他清楚了知：「我正经历快乐的感受。」 在经历痛苦的感受时，他清楚了知：「我正经历痛苦的感受。」 在经历不苦不乐的感受时，他清楚了知：「我正经历不苦不乐的感受。」 在他执着于快乐的感受时，他清楚了知：「我正执着于快乐的感受。」 没有执着于快乐的感受时，他清楚了知：「我没有执着于快乐的感受。」 在执着于痛苦的感受时，他清楚了知：「我正执着于痛苦的感受。」 没有执着于痛苦的感受时，他清楚了知：「我没有执着于痛苦的感受。」 当执着于不苦不乐的感受时，他清楚了知：「我执着于不苦不乐的感受。」 没有执着于不苦不乐的感受时，他清楚了知：「我没有执着于不苦不乐的感受。」 于是他于内部就感受观察感受，于外部就感受观察感受，同时于内部、外部就感受观察感受。因此，他观察感受当中不断生起的现象，他观察感受当中不断灭去的现象，他同时观察感受当中不断生起、灭去的现象。于是他清楚觉知：「这是感受！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就感受观察感受。 三、观心念处 又，比丘们！比丘如何就心观察心呢？比丘们！当心有贪爱时，比丘清楚了知心有贪爱，当心没有贪爱时，清楚了知心没有贪爱；当 心有嗔恨时，清楚了知心有嗔恨，当心没有嗔恨时，清楚了知心没有嗔恨；当心有愚痴时，清楚了知心有愚痴，当心没有愚痴时，清楚了知心没有愚痴；当心收摄时，清楚了知心收摄，当心涣散时，清楚了知心涣散；当心广大时，清楚了知心广大，当心不广大时，清楚了知心不广大；当心有上时，清楚了知心有上，当心无上时，清楚了知心无上；当心专注时，清楚了知心专注，当心不专注时，清楚了知心不专注；当心解脱时，清楚了知心解脱，当心未解脱时，清楚了知心未解脱。 于是他就内在的心观察心，就外在的心观察心，同时就内在、外在的心观察心。因此，他观察心中不断生起的现象，他观察心中不断灭去的现象，他同时观察心中不断生起、灭去的现象。于是他清楚觉知：「这是心！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就心观察心。 四、观法念处 1.五种障碍（五盖） 比丘们！比丘如何就诸法观察诸法呢？比丘们！比丘就诸法观察诸法，亦即就五盖观察诸法。比丘们！比丘如何就诸法观察诸法，亦即如何就五盖观察诸法呢？比丘们！当比丘生起贪欲时，他清楚了知：「我生起贪欲」；当比丘不起贪欲时，他清楚了知：「我不起贪欲」。他清楚了知，未生的贪欲生起了；他清楚了知，现在生起的贪欲去除了；他清楚了知，现在已去除的贪欲，未来不再生起。 当比丘生起嗔恚时，他清楚了知：「我生起嗔恚」；当比丘不起嗔恚时，他清楚了知：「我不起嗔恚」。他清楚了知，未生的嗔恚生起了；他清楚了知，现在生起的嗔恚去除了；他清楚了知，现在已去除的嗔恚，未来不再生起。 当比丘生起昏沉和睡眠时，他清楚了知：「我生起昏沉和睡眠」；当比丘不起昏沉和睡眠时，他清楚了知：「我不起昏沉和睡眠」。他清楚了知，未生的昏沉和睡眠生起了；他清楚了知，现在生起的昏沉和睡眠去除了；他清楚了知，现在已去除的昏沉和睡眠，未来不再生起。 当比丘生起掉举和后悔时，他清楚了知：「我生起掉举和后悔」；当比丘掉举和后悔不起时，他清楚了知：「我不起掉举和后悔」。他清楚了知，未生的掉举和后悔生起了；他清楚了知，现在生起的掉举和后悔去除了；他清楚了知，现在已去除的掉举和后悔，未来不再生起。 当比丘生起疑惑时，他清楚了知：「我生起疑惑」；当比丘不起疑惑时，他清楚了知：「我不起疑惑」。他清楚了知，未生的疑惑生起了；他清楚了知，现在生起的疑惑去除了；他清楚了知，现在已去除的疑惑，未来不再生起。 于是他就内在的诸法观察诸法，就外在的诸法观察诸法，同时就内在、外在的诸法观察诸法。因此，他观察诸法不断生起的现象，他观察诸法不断减去的现象，他同时观察诸法不断生起、灭去的现象。于是他清楚觉知：「这是诸法！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就诸法观察诸法，亦即就五盖观察诸法。 五取蕴（五蕴） 比丘们！比丘如何就诸法观察诸法呢？比丘们！比丘就诸法观察诸法，亦即就五取蕴观察诸法。比丘们！比丘如何就诸法观察诸法，亦即如何就五取蕴观察诸法呢？比丘们！比丘清楚了知：「这是色，这是色的生起，这是色的减去；这是受，这是受的生起，这是受的减去；这是想，这是想的生起，这是想的灭去；这是行，这是行的生起，这是行的灭去；这是识，这是识的生起，这是识的灭去。」 于是他就内在的诸法观察诸法，就外在的诸法观察诸法，同时就内在、外在的诸法观察诸法。因此，他观察诸法不断生起的现象，他观察诸法不断灭去的现象，他同时观察诸法不断生起、灭去的现象。于是他清楚觉知：「这是诸法！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就诸法观察诸法，亦即就五取蕴观察诸法。 六内处（六根）和六外处（六尘或六境） 比丘们！比丘如何就诸法观察诸法呢？比丘们！比丘就诸法观察诸法，亦即就六内处和六外处观察诸法。比丘们！比丘如何就诸法观察诸法，亦即如何就六内处和六外处观察诸法呢？比丘们！比丘清楚了知眼根，清楚了知色尘，以及清楚了知依此二者所众生的束缚。他清楚了知，未生的束缚生起了；他清楚了知，现在已生的束缚去除了；他清楚了知，现在已去除的束缚，未来不再生起。 比丘清楚了知耳根，清楚了知声尘，以及清楚了知依此二者所众生的束缚。他清楚了知，未生的束缚生起了；他清楚了知，现在已生的束缚去除了；他清楚了知，现在已去除的束缚，未来不再生起。 比丘清楚了知鼻根，清楚了知香尘，以及清楚了知依此二者所众生的束缚。他清楚了知，未生的束缚生起了；他清楚了知，现任已生的束缚去除了；他清楚了知，现在已去除的束缚，未来不再生起。 比丘清楚了知舌根，清楚了知味尘，以及清楚了知依此二者所众生的束缚。他清楚了知，未生的束缚生起了；他清楚了知，现在已生的束缚去除了；他清楚了知，现在已去除的束缚，未来不再生起。 比丘清楚了知身根，清楚了知触尘，以及清楚了知依此二者所众生的束缚。他清楚了知，未生的束缚生起了；他清楚了知，现在已生的束缚去除了；他清楚了知，现在已去除的束缚，未来不再生起。 比丘清楚了知意根，清楚了知法，以及清楚了知依此二者所众生的束缚。他清楚了知，未生的束缚生起了；他清楚了知，现在已生的束缚去除了；他清楚了知，现在已去除的束缚，未来不再生起。 于是他就内在的诸法观察诸法，就外在的诸法观察诸法，同时就内在、外在的诸法观察诸法。因此，他观察诸法不断生起的现象，他观察诸法不断灭去的现象，他同时观察诸法不断生起、灭去的现象。于是他清楚觉知：「这是诸法！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就诸法观察诸法，亦即就六内处和六外处观察诸法。 七菩提分（七觉支） 又，比丘们！比丘如何就诸法观察诸法呢？比丘们！比丘就诸法观察诸法，亦即就七菩提分观察诸法。比丘们！比丘如何就诸法观察 诸法，亦即如何就七菩提分观察诸法呢？比丘们！比丘有念菩提分时，他清楚了知：「我有念菩提分」；当比丘没有念菩提分时，清楚了知：「我没有念菩提分」。他清楚了知，未生的念菩提分生起了；他清楚了知，现在已生的念菩提分，增长圆满了。 比丘有择法菩提分时，他清楚了知：「我有择法菩提分」；当比丘没有择法菩提分时，清楚了知：「我没有择法菩提分」。他清楚了知，未生的择法菩提分生起了；他清楚了知，现在已生的择法菩提分，增长圆满了。 比丘有精进菩提分时，他清楚了知：「我有精进菩提分」；当比丘没有精进菩提分时，清楚了知：「我没有精进菩提分」。他清楚了知，未生的精进菩提分生起了；他清楚了知，现在已生的精进菩提分，增长圆满了。 比丘有喜菩提分时，他清楚了知：「我有喜菩提分」；当比了没有喜菩提分时，清楚了知：「我没有喜菩提分」。他清楚了知，未生的喜菩提分生起了；他清楚了知，现在已生的喜菩提分，增长圆满了。 比丘有轻安菩提分时，他清楚了知：「我有轻安菩提分」；当比丘没有轻安菩提分时，清楚了知：「我没有轻安菩提分」。他清楚了知，未生的轻安菩提分生起了；他清楚了知，现在已生的轻安菩提分，增长圆满了。 比丘有定菩提分时，他清楚了知：「我有定菩提分」；当比丘没有定菩提分时，清楚了知：「我没有定菩提分」。他清楚了知，未生的定 菩提分生起了；他清楚了知，现在已生的定菩提分，增长圆满了。 比丘有行舍菩提分时，他清楚了知：「我有行舍菩提分」；当比丘没有行舍菩提分时，清楚了知：「我没有行舍菩提分」。他清楚了知，未生的行舍菩提分生起了；他清楚了知，现在已生的行舍菩提分，增长圆满了。 于是他就内在的诸法观察诸法，就外在的诸法观察诸法，同时就内在、外在的诸法观察诸法。因此，他观察诸法不断生起的现象，他观察诸法不断灭去的现象，他同时观察诸法不断生起、灭去的现象。于是他清楚觉知：「这是法！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就诸法观察诸法，亦即就七菩提分观察诸法。 四圣谛 又，比丘们！比丘如何就诸法观察诸法呢？比丘们！比丘就诸法观察诸法，亦即就四圣谛观察诸法。比丘们！比丘如何就诸法观察诸法，亦即如何就四圣谛观察诸法呢？比丘们！比丘如实地清楚了知：「这是苦」；他如实地清楚了知：「这是苦之集」；他如实地清楚了知：「这是苦之灭」；他如实地清楚了知：「这是导致苦灭之道」。 又，比丘们！什么是苦圣谛呢？生是苦，老是苦，病是苦，死是苦，愁、叹、苦、忧、恼是苦，怨憎会是苦，爱别离是苦，求不得是苦。总括地说，五取蕴就是苦。 又，比丘们！什么是生？如果有所谓的生，对一切众生而言，在 各类的众生中，他们的受生、形成、出生、显现诸蕴、获得内外处，比丘们！这就叫作生。 又，比丘们！什么是老？如果有所谓的老，对一切众生而言，在各类的众生中，他们的衰弱、老朽、牙齿脱落、头发灰白、及肤松皱、寿命将尽、机能退化，比丘们！这就是老。 又，比丘们！什么是死？如果有所谓的死，对一切众生而言，在各类的众生中，他们的崩溃、散灭、命终、死亡、寿命结束、五蕴离析、身体弃舍、生命灭绝，比丘们！这就是死。 又，比丘们！什么是愁？凡是有人不论何时，受到损失或不幸之事的影响，生起这些痛苦的心态：忧愁、哀愁、愁苦、深忧及深愁这些痛苦的心态，比丘们！这就是愁。 又，比丘们！什么是叹？凡是有人不论何时，受到损失或不幸之事的影响，生起哭号、哭泣、叹息、以及哀号、哀叹的状态，比丘们！这就是叹。 又，比丘们！什么是苦？比丘们！由于身体的接触而生起身体上任何的苦楚、不适、不愉快的感受，比丘们！这就是苦。 又，比丘们！什么是忧？比丘们！心理上任何的苦楚、不适或由心理接触而生起任何痛苦、不愉快的感受，比丘们！这就是忧。 又，比丘们！什么是恼？凡是有人不论何时，受到损失或不幸之 事的影响，众生恼乱、苦恼、忧恼、燥恼这些心态，比丘们！这就是恼。 又，比丘们！什么是怨僧会苦？凡是有人不论何时、何处遇到不愉快、不喜欢的色、声、香、味、触、法，或时时处处遇到不幸、伤害、困难、不安，如果交往、相遇、接触、结合，比丘们！这就叫怨憎会苦。 又，比丘们！什么是爱别离苦？凡是有人不论何时、何处与所感兴趣、所喜欢、所爱的色、声、香、味、触、法的尘境分离，对那些期望他幸运、富裕、舒适或安全的人，如父母、兄弟姊妹、朋友同事、亲戚等，与他们分离，不能相见、亲近、结合，比丘们，这就叫爱别离苦。 又，比丘们！什么是求不得苦？比丘们！对众生而言，他们是受生支配的众生，生起这样的欲求：「但愿我们不受生的支配！但愿我们不再轮回转生！」但这并不是只靠欲求就可得到的，这就是求不得苦。 比丘们！什么是求不得苦？比丘们！对众生而言，他们是受老支配的众生，生起这样的欲求：「但愿我们不受老的支配！但愿我们不受老的支配！」但这并不是只靠欲求就可得到的，这就是求不得苦。 比丘们！什么是求不得苦？比丘们！受病支配的众生，生起这样的欲求：「但愿我们不受病的支配！但愿我们没有病苦！」但这并不是只靠欲求就可得到的，这就是求不得苦。 比丘们！什么是求不得苦？比丘们！受死支配的众生，生起这样的欲求：「但愿我们不受死的支配！但愿我们永远不死！」但这并不是只靠欲求就可得到的，这就是求不得苦。 比丘们！什么是求不得苦？比丘们！受愁、叹、苦、忧、恼的支配的众生，生起这样的欲求：「但愿我们不受愁、叹、苦、忧、恼的支配！但愿我们不再愁、叹、苦、忧、恼！」但这并不是只靠欲求就可得到的，这就是求不得苦。 比丘们！什么是「总括地说五取蕴就是苦」？色取蕴是苦、受取蕴是苦、想取蕴是苦、行取蕴是苦、识取蕴是苦。比丘们！这就是「总括地说五取蕴就是苦」。比丘们！这就是苦圣谛。 又，比丘们！什么是苦集圣谛呢？它就是贪爱，就是造成不断轮回，为喜乐、欲求所束缚，以及任何情况都不忘寻求快乐的欲望，也就是欲爱、有爱及无有爱。比丘们！而这贪爱从哪里生起，又从何处建立？在身心世界，只要有诱人的、令人喜悦的事物，就有贪爱的生起和建立。 而在身心世界，什么是诱人的、令人喜悦的事物呢？在身心世界中，眼根是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，耳根是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，鼻根是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，舌根是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，身根是诱人的、令人喜悦的，于是贪爱就 在那里生起，贪爱就在该处建立；在身心世界中，意根是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，色尘是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，声尘是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，香尘是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，味尘是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，触尘是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，法尘是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，眼识是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，耳识是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，鼻识是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，舌识是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，身识是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，意识是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，眼触是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，耳触是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，鼻 触是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，舌触是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，身触是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，意触是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，从眼触所生的受是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，从耳触所生的受是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，从鼻触所生的受是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，从舌触所生的受是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，从身触所生的受是诱人的、令人喜悦的，于是贪爱就在那里生起，贫爱就在该处建立；在身心世界中，从意触所生的受是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，对色尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对声尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对香尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对味尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对触尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对法尘生起的 想是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，对色尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对声尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对香尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对味尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对触尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对法尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，对色尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对声尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对香尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对味尘生起的贪爱是诱人的、令人喜悦的，于是贫爱就在那里生起，贪爱就在该处建立；在身心世界中，对触尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对法尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，对色尘生起的思惟是诱人的、令人喜悦的，于是 贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对声尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对香尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对味尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对触尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对法尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。 在身心世界中，对色尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对声尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对香尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对味尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对触尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立；在身心世界中，对法尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里生起，贪爱就在该处建立。比丘们！这就是苦集圣谛。 苦灭圣谛的解释： 又，比丘们！什么是苦灭圣谛呢？它是贪爱的完全远离、灭尽、舍离、弃舍、解脱、无染。但比丘们何处根除贪爱，何处息灭贪爱呢？在身心世界中，有诱人的、令人喜悦的地方，就是可以根除和息灭贪爱的地方。 但在身心世界中，什么是诱人的、令人喜悦的事物呢？在身心世界中，眼根是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，耳根是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，鼻根是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，舌根是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，身根是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，意根是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。在身心世界中，色尘是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，声尘是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，香尘是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，味尘是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，触尘是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，法尘是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。 在身心世界中，眼识是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，耳识是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，鼻识是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，舌识是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，身识是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，意识是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。 在身心世界中，眼触是诱人的、令人喜悦的，于是贪爱就在那里 根除和息灭；在身心世界中，耳触是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，鼻触是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，舌触是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，身触是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，意触是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。 在身心世界中，从眼触所生的受是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，从耳触所生的受是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，从鼻触所生的受是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，从舌触所生的受是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，从身触所生的受是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，从意触所生的受是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。 在身心世界中，对色尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对声尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对香尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对味尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对触尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对法尘生起的想是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。 在身心世界中，对色尘生起的行是诱人的、令人喜悦的，于是贪 爱就在那里根除和息灭；在身心世界中，对声尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对香尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对味尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对触尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对法尘生起的行是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。 在身心世界中，对色尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对声尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对香尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对味尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对触尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对法尘生起的贪爱是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。 在身心世界中，对色尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对声尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对香尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对味尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对触尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对法尘生起的思惟是诱人的、令人喜悦的，于是贪爱就在那里根除和息 灭。 在身心世界中，对色尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对声尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对香尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对味尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对触尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭；在身心世界中，对法尘生起的细察是诱人的、令人喜悦的，于是贪爱就在那里根除和息灭。比丘们！这就是苦灭圣谛。 道圣谛的解释： 又，比丘们！什么是导致苦灭的道圣谛呢？那就是八圣道，即正见、正思惟、正语、正业、正命、正精进、正念、正定。 又，比丘们！什么是正见呢？比丘们！正见就是知苦、知苦之集、知苦之灭、知导致苦灭之道的知见。比丘们！这就是正见。 又，比丘们！什么是正思惟呢？比丘们！正思惟就是离欲、不染世乐的思惟，也是没有嗔恨、没有暴力的想法。比丘们！这就是正思惟。 又，比丘们！什么是正语呢？比丘们！正语就是不妄语、不两舌、不恶口与不绮语。比丘们！这就是正语。 又，比丘们！什么是正业呢？比丘们！正业就是不杀、不偷及不邪淫。比丘们！这就是正业。 又，比丘们！什么是正命呢？比丘们！正命就是圣弟子不以错误的方式营取生活，而以正确的方式经营生活。比丘们！这就是正命。 又，比丘们！什么是正精进呢？比丘们！比丘下定决心，精进努力、振奋心志、全力以赴地投入防止未生的恶行、不善之心念的生起；比丘下定决心，精进努力、振奋心志、全力以赴地投入去除已生起的恶行和不善的心念；比丘下定决心，精进努力、振奋心志、全力以赴地投入开展未生的善行及善的心念，使之能生起；比丘下定决心，精进努力、振奋心志、全力以赴地投入保持已生的善念，不使它退失，使之增长、成熟、圆满地开展；比丘们！这就是正精进。 又，比丘们！什么是正念呢？比丘们！比丘精勤、觉知、时时彻知无常，就身体观察身体，去除对身心世界的贪爱和嗔恨；比丘精勤、觉知、时时彻知无常，就感受观察感受，去除对身心世界的贪爱和嗔恨；比丘精勤、觉知、时时彻知无常，就心观察心，去除对身心世界的贪爱和嗔恨；比丘精勤、觉知、时时彻知无常，就诸法观察诸法，去除对身心世界的贪爱和嗔恨；比丘们！这就是正念。 又，比丘们！什么是正定呢？比丘们！比丘舍离贪爱、不善之心念，众生离欲之心，伴随着寻和伺并充满喜乐，他进入初禅；寻、伺消失，获得内心平静和专心一致，众生离欲和无寻、无伺之心，充满喜乐，他进入二禅；喜消失后，他住于平等心，对感受完全觉知、时时彻知无常，并且在身体经验到圣者所说的：「由觉知和平等心所众 生的乐」他进入三禅；在根除苦乐以及先前的喜忧也消失之后，他因此进入超越苦、乐的四禅，充满平等心和觉知。比丘们！这就是正定。比丘们！这就是导致苦灭之道圣谛。 于是他就内在的诸法观察诸法，就外在的诸法观察诸法，同时就内在、外在的诸法观察诸法。因此，他观察诸法不断生起的现象，他观察诸法不断减去的现象，他同时观察诸法不断生起、灭去的现象。于是他清楚觉知：「这是诸法！」修成了只有了知和只有觉照的境界，超越执着，不再贪着身心世界的任何事物。比丘们！这就是比丘如何就诸法观察诸法，亦即就四圣谛观察诸法。 比丘们！任何人，依这个方式正确地修四念处七年，就可以期望有两种果报中的一种：现生得最上智慧，或者如果五蕴仍然存在，则得不还果。 比丘们！不用说七年，如果有任何人，依这个方式正确地修四念处六年，就可以期望得到两种果位中的一种：现生得大智慧，或者如果五蕴仍然存在，则得不还果。 比丘们！不用说六年，……比丘们！不用说五年，……比丘们！不用说四年，……比丘们！不用说三年，……比丘们！不用说二年，…… 比丘们！不用说一年，如果有任何人，依这个方式正确地修四念处七个月，就可以期望得到两种果位中的一种：现生得最上智慧，或者如果五蕴仍然存在，则得不还果。 比丘们！不用说七个月，……比丘们！不用说六个月，……比丘们！不用说五个月，……比丘们！不用说四个月，……比丘们！不用说三个月，……比丘们！不用说二个月，……比丘们！不用说一个月，……比丘们！不用说半个月，如果有人依这个方式正确地修四念处七天，就可以期望得到两种果位中的一种：现生得最上智慧，或者如果五蕴仍然存在，则得不还果。 这就是为什么说：「比丘们！只有一条道路可以使众生清净、克服愁叹、灭除苦忧、获得正道、体证涅盘，这条道路就是四念处。」 世尊如此说法后，比丘们皆大欢喜，赞叹世尊所说的法。 正念的呼吸：《安般守意经》 出处：巴利经典中部第118经 英译中：广净法师 诸比丘，持续不断修习安般念，能获大果，能致大誉。持续不断修习安般念，可令四念处修习圆满。持续不断修习安般念，可令七觉支修习圆满，持续不断修习安般念，可令智慧解脱完成。 诸比丘，如何修习安般念，如何持续不断修习，能获大果，能致大誉？ 诸比丘，如此，比丘或往林中，或往树下，或往寂静处，结踟趺坐，端身正直，系念在前。息入知正入息，息出知正出息。 （吸气，行者知道他正在吸气；呼气，行者知道他正在呼气。） 一、观 身 吸入的气很长，行者知道「我正在吸入长息」；呼出的气很长，行者知道「我正在呼出长息」。 吸入的气很短，行者知道「我正在吸入短息」；呼出的气很短，行者知道「我正在呼出短息」。 「我正在吸气，同时注意到我的全身。我正在呼气，同时注意 到我的全身。」行者努力练习。 「吸气我令全身宁静祥和，呼气我令全身宁静祥和。」行者努力练习。 二、观 受 「吸气我感到喜悦，呼气我感到喜悦。」行者努力练习。 「吸气我感到快乐，呼气我感到快乐。」行者努力练习。 「我正在吸气，同时了知我心理的活动。我正在呼气，同时了知我心理活动。」行者努力练习。 「我正在吸气，同时使我心理的活动宁静祥和。我正在呼气，同时使我心理的活动宁静祥和。」行者努力练习。 三、观 心 「我正在吸气，同时觉察到我的心。我正在呼气，同时觉察到我的心。」行者努力练习。 「我正在吸气，同时使我的心愉悦祥和。我正在呼气，同时使我的心愉悦祥和。」行者努力练习。 「我正在吸气，同时集中我的心念。我正在呼气，同时集中我的心念。」行者努力练习。 「我正在吸气，同时疏解我的心结。我正在呼气，同时疏解我的心结。」行者努力练习。 四、观 法 「我正在吸气，同时观察诸法无常的本性。我正在呼气，同时观察诸法无常的本性。」行者努力练习。 「我正在吸气，同时观察诸法离染。我正在呼气，同时观察诸法离染。」行者努力练习。 「我正在吸气，同时观察解脱。我正在呼气，同时观察解脱。」行者努力练习。 「我正在吸气，同时观察出离。我正在呼气，同时观察出离。」行者努力练习。 诸比丘，如是修习安般念，如是持续不断修习，能获大果，能致大誉。 五、圆满四念处 诸比丘，如何修习安般，如何持续不断修习，能令四念处修习圆满？ （1） 诸比丘，当行者专注于呼吸，吸气行者了知「我正在吸气」；呼气，行者了知「我正在呼气」；呼出长息了知「我正呼出长息」；吸入长息，了知「我正吸入长息」；呼出短息，了知「我正呼出短息」；吸入短息，了知「我正吸入短息」。 当出入息时，了知一切身，了知「我正令全身宁静祥和」。行者坚持安住观身在身，全然了知他的状况，超越世间所有之执着、忧患。 行者如此专注安般修息四念处的第一念处－－身念处。 （2） 诸比丘，当行者专注于呼吸，吸气行者了知「我感到喜悦」；呼气，行者了知「我感到喜悦」；吸气，了知「我感到快乐」；呼气，了知「我感到快乐」；呼气，了知「我心理的活动」；吸气，了知「我心理的活动」；吸气，同时令其心理的活动宁静祥和；呼气，同时令其心理的活动宁静祥和；行者坚持安住观受在受，全然了知他的状况，超越世间所有之执着、忧患。 行者如此专注安般修息四念处的第二念处－－受念处。 （3） 诸比丘，当行者专注于呼吸，吸气呼气了知其心；使其心愉悦祥和集中心念；令心解脱。行者坚持安住观心在心，全然了知他的状况，超越世间所有之执着、忧患。诸比丘不修习安般念则不能得到任何禅定智慧。 「我正在吸气，同时观察出离。我正在呼气，同时观察出离。」行者努力练习。 （4） 诸比丘，当行者专注于呼吸，照见诸法无常、离染、解脱、出离 的本性。行者坚持安住观身在身，全然了知他的状况，超越世间所有之执着、忧患。 诸比丘，如此修习安般念，如此持续不断修习，能令四念处修习圆满。 诸比丘，如何修习四念处，如何持续不断修习，能圆满安住于七觉支？ 诸比丘，若行者恒坚持观身在身，观受在受，观心在心，观法在法，无分散意，可令念觉支坚固，念觉支坚固故，念觉支能趋于圆满。 若行者住于念（专注，禅定状态）无分散意，能审视心中生起的诸法－－心的对象，能令第二觉支－－择法觉支生起趋于圆满。 若行者持续、坚持不移观照、审视诸法，无分散意，能令第三觉支－－精进觉支生起趋于圆满。 若行者精进坚固，沉着住于修习之流，则离欲喜起，能令第四觉支－－喜觉支生起趋于圆满。 若行者住于喜的状态，无分散意，能令身心轻安，身心经安则令第五觉支－－轻安觉支生起趋于圆满。 若行者身心轻安，则心易达于定境，能令第六觉支－－定觉支生起趋于圆满。 若行者住于甚深禅定中，能除诸分别，令第七觉支－－舍觉支生起趋于圆满。 诸比丘，如何格修习七觉支，如何持续不断修习，能令正智解脱完成？ 行者住寂静处，遵循七觉支之道，观诸法离欲，可令念觉支修习，如是乃至择法、精进、喜、轻安、定、舍觉支修习，如此修习七觉支能令正智解脱完成。 佛世尊说此经己，诸比丘皆大欢喜。 观心念：《学处集要》 梵文英译：Edward Conze 菩萨四处寻找自己的心念。但，是什么心念呢？ 既不是渴爱，也不是嗔恨或无明。 那么，是过去？是未来？或是现在吗？ 但，过去已不复存在；未来犹未来临；现在又不确定。 迦叶， 这是因为心念无法从身内、身外，或身之内外间去捉摸。 这是因为心念无形无色、不可见、不滞止、无法掌握、无法维持，又飘移不定。 诸佛从未见过心念，现在见不到，未来也不会见到。 诸佛从未见过的，又怎会有观察的过程？除非是出自虚妄的想象认知？ 心念就如魔术之幻；诞生于虚妄的想象力，幻现出千变万状。 心念就如川河之流，剎那也不停息；才刚迸现，就碎散消逝。 心念就如灯中之焰，随因缘递变。 心念就如雷电，瞬间爆裂即杳无踪迹…… 菩萨四处寻找心念，于心内、心外都见不到，于五蕴、四大或六内处也找不到。 因为见不到，他转而寻找心念的动向，并自问：心念究竟来自何处？ 他想到：「心念的对象在哪里，心念就在那里生起。」 那么，心念是一回事，心念的对象是另一回事吗？ 不，心念的对象是什么，心念就是什么。 倘若心念是一回事，心念的对象是另一回事，那么就有二重心念了。 所以，心的对象本身就是心念。 那么，心念能观察心念吗？ 不，心念无法观察心念。 正如剑刃无法自砍，心念也见不着自身。 此外，不管怎么从四面八方震荡它、困住它，心念依旧源源不绝、毫不迟疑、敏捷如猴、飘忽如风。 它范围迢远、无形无体、瞬息万变，六根（眼耳鼻舌身意）与六境（色声香味触法）相触即被牵引，且随境流转不休。 换个角度说，心念的稳定、一心、不动、不恼、专注一境与不散乱，即为正念。 不住无为：《维摩诘所说经》 出处：大正藏第14册维摩诘所说经菩萨行品第11 中译：鸠摩罗什 何谓菩萨不住无为。 谓修学空。不以空为证。 修学无相无作。不以无相无作为证。 修学无起。不以无起为证。 观于无常而不厌善本。 观世间苦而不恶生死。 观于无我而诲人不倦。 观于寂灭而不永寂灭。 观于远离。而身心修善。 观无所归。而归趣善法。 观于无生。而以生法。荷负一切。 观于无漏而不断诸漏。 观无诸行。而以行法教化众生。 观于空无。而不舍大悲。 观正法位。而不随小乘。 观诸法虚妄无牢无人无主无相。本愿未满。而不虚福德禅定智慧。 修如此法。是名菩萨不住无为。 又具福德故不住无为。具智慧故不尽有为。 大慈悲故不住无为。满本愿故不尽有为。 若般之心：《心经》 出处：大正藏第8册 中译：玄奘 观自在菩萨，行深般若波罗蜜多时，照见五蕴皆空，度一切苦厄。 舍利子，色不异空，空不异色，色即是空，空即是色，受想行识，亦复如是。 舍利子，是诸法空相，不生不灭，不垢不净，不增不减，是故空中无色，无受想行识，无眼耳鼻舌身意，无色身想味触法，无眼界，乃至无意识界，无无明，亦无无明尽，乃至无老死，亦无老死尽，无苦集灭道，无智亦无得，以无所得故。 菩提萨睡，依般若波罗蜜多故，心无罣碍，无罣碍故，无有恐怖，远离颠倒梦想，究竟涅盘。三世诸佛，依般若波罗蜜多故，得阿耨多罗三藐三菩提。 故知般若波罗蜜多，是大神咒，是大明咒，是无上咒，是无等等咒，能除一切苦，真实不虚，故说般若波罗蜜多咒，即说咒曰，揭谛，揭谛，波罗揭谛，波罗僧揭谛，菩提萨婆诃。","categories":[],"tags":[{"name":"Chan","slug":"Chan","permalink":"http://yoursite.com/tags/Chan/"}]},{"title":"python学习笔记","slug":"python学习笔记","date":"2017-04-02T07:47:11.000Z","updated":"2017-04-06T06:13:59.888Z","comments":true,"path":"2017/04/02/python学习笔记/","link":"","permalink":"http://yoursite.com/2017/04/02/python学习笔记/","excerpt":"欢迎来到信平的小屋，QQ :759949947, Email: xpws2006@163.com此文是博主业余时间学习python做的笔记,但求吃透每个知识点，特此勉励自己。","text":"欢迎来到信平的小屋，QQ :759949947, Email: xpws2006@163.com此文是博主业余时间学习python做的笔记,但求吃透每个知识点，特此勉励自己。 Scipy什么是Scipy?SciPy函数库在NumPy库的基础上增加了众多的数学、科学以及工程计算中常用的库函数。例如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵等等。 Scipy中的数据结构 ndarray： N维数组 Series：变长字典 DataFrame: 数据框 NumPy什么是NumPy ?NumPy系统是Python的一种开源的数值计算扩展。这种工具可用来存储和处理大型矩阵，比Python自身的嵌套列表（nested list structure)结构要高效的多。 输出矩阵np.ones()函数定义矩阵。使用np.ones( (3,4))函数生成一个3 * 4列的矩阵。返回的类型是ndarray，是个多维数组。1234import numpy as npxArray = np.ones( (3,4))print( xArray) 输出结果：123[[ 1. 1. 1. 1.] [ 1. 1. 1. 1.] [ 1. 1. 1. 1.]] Matplotlib什么是 Matplotlib ？matplotlib 是python最著名的绘图库，它提供了一整套和matlab相似的命令API，十分适合交互式地行制图。而且也可以方便地将它作为绘图控件，嵌入GUI应用程序中。它的文档相当完备，并且Gallery页面中有上百幅缩略图，打开之后都有源程序。因此如果你需要绘制某种类型的图，只需要在这个页面中浏览/复制/粘贴一下，基本上都能搞定。 特征： 基于Numpy 二维会图库，简单快速的生成曲线图，直方图和散点图等形式的图 常用的pyplot是一个简单提供类似MATLAB接口的模块 Pandas什么是Pandas ?Python Data Analysis Library 或 pandas 是连接 SciPy 和 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具 特征： 基于SciPy和NumPy 高效的Series和DataFrame数据结构 强大的可扩展数据操作与分析的Python库 高效处理大数据集的切片等功能 提供优化库功能读写多种文件格式，如CSV,HDFS Pandas主要用来用来处理数据，数据处理分为以下几个步骤： 数据收集 =&gt; 数据整理 =&gt; 数据分析 Matplotlib常用绘图折线图 Line plots折线图Line plots时关联一组x和y值的直线。12345678import numpy as npimport pylab as pl x = [1, 2, 3, 4, 5]# Make an array of x valuesy = [1, 4, 9, 16, 25]# Make an array of y values for each x valuepl.plot(x, y)# use pylab to plot x and ypl.show()# show the plot on the screen 运行脚本，显示如下图表： 散点图 Scatter plots把pl.plot(x, y)改成pl.plot(x, y, ‘o’)即可，下图的蓝色版本 1 线条颜色 Changing the line color改变线条为红色：把pl.plot(x, y, ‘o’)改成pl.plot(x, y, ’or’) 颜色代码如下图所示： 2 线条样式 Changing the line style设置线条为虚线的核心代码为: pl.plot(x,y, ‘–’) 线条虚实代码如下图所示： 3 marker样式 Changing the marker style设置为蓝色星型markers的核心代码为： pl.plot(x,y, ‘b*’ ) 星形代码如下图所示： 图和轴标题以及轴坐标限度 Plot and axis titles and limits演示例子如下：123456789101112131415import numpy as npimport pylab as plx = [1, 2, 3, 4, 5]# Make an array of x valuesy = [1, 4, 9, 16, 25]# Make an array of y values for each x valuepl.plot(x, y)# use pylab to plot x and ypl.title('Plot of y vs. x')# give plot a titlepl.xlabel('x axis')# make axis labelspl.ylabel('y axis') pl.xlim(0.0, 7.0)# set axis limitspl.ylim(0.0, 30.)pl.show()# show the plot on the screen 在一个坐标系上绘制多个图 Plotting more than one plot on the same set of axes演示例子如下：123456789101112131415161718import numpy as npimport pylab as pl x1 = [1, 2, 3, 4, 5]# Make x, y arrays for each graphy1 = [1, 4, 9, 16, 25]x2 = [1, 2, 4, 6, 8]y2 = [2, 4, 8, 12, 16]pl.plot(x1, y1, 'r')# use pylab to plot x and ypl.plot(x2, y2, 'g')pl.title('Plot of y vs. x')# give plot a titlepl.xlabel('x axis')# make axis labelspl.ylabel('y axis')pl.xlim(0.0, 9.0)# set axis limitspl.ylim(0.0, 30.)pl.show()# show the plot on the screen 运行脚本后，显示如下图表： 图例 Figure legendspl.legend((plot1, plot2), (‘label1, label2’), ‘best’, numpoints=1) 其中第三个参数表示图例放置的位置: ‘best’ ， ‘upper right’, ‘upper left’ , ‘center’, ‘lower left’ , ‘lower right’ . 如果在当前figure里plot的时候已经指定了label，如plt.plot(x,z,label=”cos(x2)“)，直接调用plt.legend()就可以了。12345678910111213141516171819202122232425262728import numpy as npimport pylab as plx1 = [1, 2, 3, 4, 5]# Make x, y arrays for each graphy1 = [1, 4, 9, 16, 25]x2 = [1, 2, 4, 6, 8]y2 = [2, 4, 8, 12, 16]plot1 = pl.plot(x1, y1, 'r')# use pylab to plot x and y : Give your plots namesplot2 = pl.plot(x2, y2, 'go')pl.title('Plot of y vs. x')# give plot a titlepl.xlabel('x axis')# make axis labelspl.ylabel('y axis')pl.xlim(0.0, 9.0)# set axis limitspl.ylim(0.0, 30.)pl.legend((plot1, plot2), ('label1, label2') )# make legendpl.show()# show the plot on the screen 直方图 Histograms12345678910111213141516import numpy as npimport pylab as pl# make an array of random numbers with a gaussian distribution with# mean = 5.0# rms = 3.0# number of points = 1000data = np.random.normal(5.0, 3.0, 1000)# make a histogram of the data arraypl.hist(data)# make plot labelspl.xlabel('data')pl.show() 运行脚本后，显示如下图表：如果不想要黑色轮廓可以改为: pl.hist(data, histtype=’stepfilled’),运行效果如下图所示： 自定义直方图bin宽度 Setting the width of the histogram bins manually增加这两行 12bins = np.arange(-5., 16., 1.) #浮点数版本的rangepl.hist(data, bins, histtype='stepfilled') 同一画板上绘制多幅子图 Plotting more than one axis per canvas如果需要同时绘制多幅图表的话，可以是给figure传递一个整数参数指定图标的序号，如果所指定序号的绘图对象已经存在的话，将不创建新的对象，而只是让它成为当前绘图对象。12fig1 = pl.figure(1)pl.subplot(211) subplot(211)把绘图区域等分为2行*1列共两个区域, 然后在区域1(上区域)中创建一个轴对象. pl.subplot(212)在区域2(下区域)创建一个轴对象。 参考资料：绘图: matplotlib核心剖析http://www.cnblogs.com/vamei/archive/2013/01/30/2879700.html","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"用hexo搭建自己的博客","slug":"用hexo搭建自己的博客","date":"2017-04-01T14:14:46.000Z","updated":"2017-04-06T07:09:27.728Z","comments":true,"path":"2017/04/01/用hexo搭建自己的博客/","link":"","permalink":"http://yoursite.com/2017/04/01/用hexo搭建自己的博客/","excerpt":"Hexo是一个快速、简洁且高效的博客框架，支持markDown，有丰富的插件和主题。官方地址是： https://hexo.io/zh-cn/","text":"Hexo是一个快速、简洁且高效的博客框架，支持markDown，有丰富的插件和主题。官方地址是： https://hexo.io/zh-cn/ 安装环境安装git去git的官网下载安装软件，按照提示一步步使用默认安装就可以了，非常简单。 https://git-scm.com/ 安装Node.js去Node.js的官网，按照操作系统的选择不同的安装版本，按照提示一步步安装即可。 http://nodejs.org/ 安装Hexo鼠标右键选择Git Bash Here,然后在git命令行工具里输入以下命令： npm install -g hexo-cli 如果没有错误提示，那么在git的交互环境输入以下命令查看git版本： hexo version 到此，hexo就安装成功了，第一步结束，环境配置完毕可以开始创建自己的博客了 本地部署Hexostep1: 新建一个存放博客目录的文件夹，例如：blogstep2: 进入到blog文件夹 hexo init blog 然后 npm install,博客安装完毕step3: 使用以下命令启动博客 hexo s step4:打开浏览器 http://127.0.0.1:4000/ 即可访问 部署到Githubstep1: 在github上注册一个账号 https://github.com/ 在github上新建一个项目，博主的项目名字叫做cxinping.github.io,这个是固定的，不能改，一个github用户只能创建一个个人静态网页和一个组织静态网页。step2: 配置blog目录下的_config.yml文件，修改deploy参数，其中repository换成刚刚新建项目的git地址，这里使用git形式。1234deploy: type: git repository: https://github.com/cxinping/cxinping.github.io.git branch: master 备注：在hexo3.x版本下，这里的type应该填git，不是github；另外冒号后面都有一个英文的空格，不然会报错的。 step3: 修改完config.yml文件后，需要使用以下命令保存配置。 npm install hexo-deployer-git –save step4: 在blog目录下，用上面提到的gitbash执行以下命令即可 hexo g hexo d step5:在浏览器中输入 https://cxinping.github.io 即可看到部署的博客。 部署时保证README.md文件不被渲染确保README.md文件不被渲染很容易，只要在博客根目录下的配置文件_config.yml中配置一下”skip_render”选项就行了，将不需要渲染的文件名称加入的其选项下就行了。 skip_render: README.md 使用MarkdownMarkdown的文档介绍http://www.appinn.com/markdown/ #引入代码高亮参考： https://liuzhichao.com/2016/hexo-use-prettify-to-highlight-code.html","categories":[],"tags":[{"name":"心有猛虎","slug":"心有猛虎","permalink":"http://yoursite.com/tags/心有猛虎/"}]},{"title":"《NumPy 学习指南》笔记","slug":"《NumPy-学习指南》信平笔记","date":"2017-04-01T12:12:40.000Z","updated":"2017-04-02T07:48:10.121Z","comments":true,"path":"2017/04/01/《NumPy-学习指南》信平笔记/","link":"","permalink":"http://yoursite.com/2017/04/01/《NumPy-学习指南》信平笔记/","excerpt":"欢迎来到信平的小屋，QQ :759949947, Email: xpws2006@163.com此文是博主业余时间学习《NumPy 学习指南》做的笔记,但求吃透每个知识点，特此勉励自己。","text":"欢迎来到信平的小屋，QQ :759949947, Email: xpws2006@163.com此文是博主业余时间学习《NumPy 学习指南》做的笔记,但求吃透每个知识点，特此勉励自己。 第1章 NumPy 快速入门什么是NumPy？NumPy，即Numeric Python的缩写，是一个优秀的开源科学计算库，并已经成为Python科学计算生态系统的重要组成部分。NumPy为我们提供了丰富的数学函数、强大的多维数组对象以及优异的运算性能。尽管Python作为流行的编程语言非常灵活易用，但它本身并非为科学计算量身定做，在开发效率和执行效率上均不适合直接用于数据分析，尤其是大数据的分析和处理。幸运的是，NumPy为Python插上了翅膀，在保留Python语言优势的同时大大增强了科学计算和数据处理的能力。更重要的是，NumPy与SciPy、Matplotlib、SciKits等其他众多Python科学计算库很好地结合在一起，共同构建了一个完整的科学计算生态系统。毫不夸张地讲，NumPy是使用Python进行数据分析的一个必备工具。 为什么使用NumPy?NumPy中数组的存储效率和输入输出性能均远远优于Python中等价的基本数据结构（如嵌套的list容器）。其能够提升的性能是与数组中元素的数目成比例的。对于大型数组的运算，使用NumPy的确很有优势。对于TB级的大文件，NumPy使用内存映射文件来处理，以达到最优的数据读写性能。不过，NumPy数组的通用性不及Python提供的list容器，这是其不足之处。因此在科学计算之外的领域，NumPy的优势也就不那么明显了。 数组对象1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import sysfrom datetime import datetimeimport numpy as npdef numpysum(n): a = np.arange(n) ** 2 b = np.arange(n) ** 3 c = a + b return cdef pythonsum(n): a = range(n) b = range(n) c = [] for i in range(len(a)): a[i] = i ** 2 b[i] = i ** 3 c.append(a[i] + b[i]) return csize = 3 start = datetime.now()c = pythonsum(size)delta = datetime.now() - startprint \"The last 2 elements of the sum\", c[-2:]print \"PythonSum elapsed time in microseconds\", delta.microsecondsstart = datetime.now()c = numpysum(size)delta = datetime.now() - startprint \"The last 2 elements of the sum\", c[-2:]print \"NumPySum elapsed time in microseconds\", delta.microseconds","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"","slug":"刘克亚励志英语","date":"2017-03-31T10:12:40.000Z","updated":"2017-04-05T09:24:34.667Z","comments":true,"path":"2017/03/31/刘克亚励志英语/","link":"","permalink":"http://yoursite.com/2017/03/31/刘克亚励志英语/","excerpt":"刘克亚励志英语欢迎来到信平的小屋，QQ :759949947, Email: xpws2006@163.com本文是学习英语的磨耳朵材料，力求听透每个音节，学的向刘老师一样好，特此勉励自己。","text":"刘克亚励志英语欢迎来到信平的小屋，QQ :759949947, Email: xpws2006@163.com本文是学习英语的磨耳朵材料，力求听透每个音节，学的向刘老师一样好，特此勉励自己。 清晨励志演讲完整版Your browser does not support the audio element. As you slowly open your eyes, look around, notice where the light comes into your room, listen carefully, see if there are any new sounds you can recognize; feel with your body and spirit, see if you can sense the freshness in the air. Yes, yes, yes, it’s a new day, it’s a different day, and it’s a bright day! And most importantly, it is a new beginning for your life, a beginning where you are going to make new decisions, take new actions, make new friends and take your life to a totally unprecedented level. In your mind’s eye, you can see clearly the things you want to have, the places you intend to go, the relationships you desire to develop, and the positions you aspire to reach. You can hear your laughers of joy and happiness on the day when everything happens as you dream! You can see the smiles on the people around you when the magic moment strikes. You can feel your face is getting red, your heart is beating fast, and your blood is rushing all over your body, to every single corner of your being! You know all this is real as long as you are confident, passionate, and committed. And you are confident, you are passionate, you are committed! You will no longer fear making new sounds, showing new facial expressions, using your body in new ways, approaching new people, and asking new questions. You will live every single day of your life with absolute passion, and you will show your passion through the words you speak and the actions you take. You will focus all your time and effort on the most important goals of your life. You will never succumb to challenges and hardships; you will never w*er in you·r pursuit of excellence. After all, you are the best, and you deserve the best! As your coach and friend, I can assure you the door to all the best things in the world will open to you,but the key to that door is in your hand. You must do your part, you must faithfully follow the plans you make and take the actions you plan,you must never quit, you must never fear! I know you must do it, you can do it, you will do it,and you will succeed!Now stand firm and tall, make a fist, get excited and yell it out。I must do it! I can do it! I will do it! I will succeed! 中文： 当你慢慢睁开你的双眼，环顾四周，注意到阳光进入你的房间。请仔细聆听，看看有没有你能够识别的声音;用你的身体和灵魂去感受，看看你能否感触到空气的清新。 是的，是的，是的，这是全新的一天，这是与众不同的一天，这是阳光明媚的一天!最重要的是，这是你生命的新的开端：你将作出新的决定，采取新的行动，结交新的朋友，把你的生命带到一个完全史无前例的水平。 用你心灵的窗户，你能够清晰地看到：你想要拥有的东西，你打算去的地方，你渴望发展的关系，以及你希冀达到的地位。你可以听见在你梦想成真的那一天你快乐和幸福的笑声!你能够看见在这神奇的时刻到来时你周围的人们脸上的微笑。你会感觉到你的双颊正变得通红，心跳在加速，血液正流遍你的全身，涌向身体的每一个角落! 你知道，只要你信心十足，充满激情，并且勇于拼搏，一切将会变成现实。你是信心十足的，你是充满激情的，你是勇于献身的!你将不再害怕发出新的声音，表现新的面部表情，展示新的身姿，接近新的人们，以及询问新的问题。你将以绝对的激情去过好生活中的每一天;你将通过你的言行来展示你的激情。你将全神贯注于你生命中最重要的目标。你将永远也不会向挑战和磨难屈服;你将永远也不会在追求卓越时畏缩。毕竟，你是最优秀的，并且受之无愧。 作为你的教练和朋友，我可以向你保证：通往世界上最美好事物的大门将会向你敞开，但是这扇门的钥匙掌握在你的手中。你必须做你分内之事，你必须忠实地遵循你的计划，并且按计划行动。你必须永不言弃，必须无所谓惧!我知道你必须去做，你能够去做，你将会去做，你一定会成功!现在站稳了，立直了，握紧拳头，兴奋起来，并且大声呐喊：我必须去做，我能够去做，我将会去做，我一定会成功! 晚间励志演讲完整版Your browser does not support the audio element. As you lie down in your bed, close your eyes and relax. Take a deep breath and hold it for a count of 3 (1,2,3), then slowly exhale. Your body is in a comfortable position and your eyes are closed. Take another deep breath, even deeper than before, and hold the air for a count of 5 (1,2,3,4,5). Again, slowly let your breath out. Notice how relaxed and loose your body feels. Now inhale again, from the center of your body, and hold your breath for a count of 8 (1,2,3,4,5,6,7,8). Picture yourself standing in an observation elevator. Through the glasses, you can see the doors to all the buildings and the children playing on the ground between the buildings and the elevator. Slowly you begin to sense the elevator is moving, it’s going up. As it goes up, you begin to notice the windows and the people behind the windows. The buildings are tall, and it takes a little while to get to the top. Just as the elevator reaches over the buildings, a beautiful lake emerges in your sight, the water is clear and peaceful, ducks are playing on the lake and ripples are spreading. Beyond the lake is a grass covered mountain slope with sheep easily enjoying their meals. The elevator keeps going up, when it reaches over the top of the mountain, a beautiful harbor appears on the horizon. In the harbor sailboats are easily cruising on the blue and peaceful water. Around the harbor, large areas of trees and flowers and magnificent houses spread themselves regularly along the coast. You are struck by the beauty of the environment and the harmony between nature and design. You begin to imagine yourself living in that harbor, having all the things you ever desire, enjoying all the friendships you ever dream, and achieving all the accomplishments you ever aspire. You know all these beautiful things will be yours to keep, to cherish and to enjoy, because you have found a clear purpose for your life, you have developed a sound strategy, and you have taken all the actions necessary: You got up as promised everyday; you listened to the recordings and practiced as planned; you thought often and critically; you always wrote down your thoughts and inspirations; you pushed yourself beyond your comfort zone when it comes to making new friends and seizing every learning opportunity. Now, with 100% certainty you are moving in the right direction and at the right pace, you can be at ease going into sleep. Let your eyes relax, then your head, your neck, your shoulder, your chest, your waist, your thighs, your knees, your feet, your toes and your whole body. Go, slowly, go, sleep! Have a sweet dream! 中文： 当你躺在床上，闭上眼睛，放松。深吸一口气等我数到3 ,1，2，3〕，，然后慢慢呼出。你的身体很舒服，你的双眼闭着。再深呼一口气，比以前一口更深，屏住呼吸等我数到5〔1，2，3，4，5 〕，然后慢慢呼出。注意你很放松，你的身体软软的。再深吸一口气，感觉从你身体的重心处，屏住呼吸等我数到8 〔1，2，3，4，5 〕,然后慢慢的呼出。 想象自己站在一个观光电梯里，透过玻璃，你可以看到附近建筑物的门以及在空地上玩耍的孩子。慢慢地你意识到电梯在向上移动。随着电梯渐渐上行，你开始注意到建筑物的窗子背后的人。建筑物很高，过了好大一会，电梯渐渐靠近顶点。就当电梯超过建筑物的一瞬间，一潭漂亮的湖水出现在你的视野里。湖水清澈而平静，湖面上有鸭子在嬉戏，涟漪从它们身边慢慢地向外散去。湖水的外面是一个山坡，山坡上郁郁葱葱，山羊在上面懒懒地一边吃草一边玩耍。电梯继续上升，就在掠过山顶的一瞬间，一座漂亮的海港赫然出现在地平线上。海港中，帆船在湛蓝的海面上漫不经心地飘来飘去。海港周围，大片的鲜花和绿地以及设计精美的房屋沿海岸线规则有序地排列着。你被这优美的环境以及人与自然之间如此完美的融合深深地打动了，你开始想象自己就住在这个海港里，拥有你一切想得到的财物，享受你所梦想中的友谊，实现你所渴望中的成就。 你知道所有这一切都将会是你的，你可以尽情的享受和珍存。因为你现在有了明确的人生方向，制定出了完善的人生策略，采取一切必要的行动：你每天按计划起床；按计划听录音进行模仿；你经常思考问题，并随时将你的体会和灵感记录下来；你总是不断地激发自己、超越自己，让自己结交新的朋友，抓住每一个学习的机会。 现在，你可以完全肯定自己正在沿着正确的步调前进，你可以放心的进入梦乡了。尽量放松你的眼部，渐渐地，你的头部，你的颈部，你的肩部，你的胸部，你的腰部，你的大腿，你的膝关节，你的双脚，你的脚趾，你的全身……慢慢地，慢慢地，慢慢地，睡吧！做一个美梦！ 克亚营销导图克亚营销导图是你打造营销系统的核心工具。 image below","categories":[],"tags":[{"name":"英语","slug":"英语","permalink":"http://yoursite.com/tags/英语/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-03-30T07:14:08.463Z","updated":"2017-03-30T07:18:33.480Z","comments":true,"path":"2017/03/30/hello-world/","link":"","permalink":"http://yoursite.com/2017/03/30/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}